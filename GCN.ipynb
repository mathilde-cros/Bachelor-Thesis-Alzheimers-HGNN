{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ReLU, Linear, Sequential, ModuleList\n",
    "from torch_geometric.nn import EdgeConv, GCNConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.mlp = torch.nn.Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels, 1))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, dropout):\n",
    "        super(Graph, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GraphConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GraphConv(hidden_channels, hidden_channels))\n",
    "        self.mlp = torch.nn.Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels, 1))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, dropout):\n",
    "        super(Edge, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(EdgeConv(nn=Sequential(Linear(in_channels, hidden_channels)), aggr='mean'))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(EdgeConv(nn=Sequential(Linear(hidden_channels, hidden_channels)), aggr='mean'))\n",
    "        self.mlp = torch.nn.Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels, 1))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the datasets created in the Graph_Building.ipynb and Matrix_Profiling.ipynb notebooks\n",
    "# %store -r dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Raw_to_Graph(197):\n",
      "====================\n",
      "Number of graphs: 197\n",
      "Weighted: False\n",
      "Threshold: 0.4\n",
      "Correlation Method: pearson\n",
      "Number of features: 7\n",
      "Number of classes: 4\n",
      "\n",
      "Data(edge_index=[2, 2008], weight=[2008], x=[116, 7], y='0', num_nodes=116)\n",
      "=============================================================\n",
      "Number of nodes: 116\n",
      "Number of edges: 2008\n",
      "Average node degree: 17.31\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# Here this isn't working for some reason otherwise...\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.convert_matrix import from_numpy_array\n",
    "\n",
    "# Creating a dictionary of lists of paths to the correlation matrices for each method. Each list in the dictionary represents a different method.\n",
    "methods = ['pearson', 'spearman', 'kendall', 'partial']\n",
    "full_corr_path_lists = {}\n",
    "for method in methods:\n",
    "    method_dir = f'ADNI_full/corr_matrices/corr_matrix_{method}/'\n",
    "    full_corr_path_lists[method] = []\n",
    "    for file in os.listdir(method_dir):\n",
    "        full_corr_path_lists[method].append(file)\n",
    "# Generating the diagnostic file from the diagnostic_label.csv file\n",
    "diagnostic_label = np.loadtxt('ADNI_full/diagnostic_label.csv', dtype=str, delimiter=',')\n",
    "\n",
    "# Combining the 'EMCI', 'LMCI' and 'MCI' diagnostics into a single 'MCI' label for simplicity, then one-hot encoding the diagnostics\n",
    "for patient in range(len(diagnostic_label)):\n",
    "    if diagnostic_label[patient] == 'CN':\n",
    "        diagnostic_label[patient] = 0\n",
    "    elif diagnostic_label[patient] == 'SMC':\n",
    "        diagnostic_label[patient] = 1\n",
    "    elif diagnostic_label[patient] == 'EMCI' or diagnostic_label[patient] == 'LMCI' or diagnostic_label[patient] == 'MCI':\n",
    "        diagnostic_label[patient] = 2\n",
    "    elif diagnostic_label[patient] == 'AD':\n",
    "        diagnostic_label[patient] = 3\n",
    "    else:\n",
    "        print('Error: Diagnostic label not recognised')\n",
    "        break\n",
    "# Loading the age feature of patients to use as a node feature\n",
    "ages = np.loadtxt('ADNI_full/age.csv', delimiter=',')\n",
    "min_age = np.min(ages)\n",
    "max_age = np.max(ages)\n",
    "# Prepocessing the sex feature of patients to use as a node feature. Here, 0 represents male patients and 1 represents female patients\n",
    "sex = np.loadtxt('ADNI_full/sex.csv', dtype=str, delimiter=',')\n",
    "for patient in range(len(sex)):\n",
    "    if sex[patient] == 'M':\n",
    "        sex[patient] = 0\n",
    "    else:\n",
    "        sex[patient] = 1\n",
    "\n",
    "# Defining functions to simplify the code in the class Raw_to_Graph.\n",
    "# To convert a dictionnary into a numpy array\n",
    "def dict_to_array(dict):\n",
    "    array = np.array(list(dict.values()))\n",
    "    return array\n",
    "\n",
    "# To normalize an array\n",
    "def normalize_array(array):\n",
    "    norm_array = (array - np.mean(array)) / np.std(array)\n",
    "    return norm_array\n",
    "# Defining a class to preprocess raw data into a format suitable for training Graph Neural Networks (GNNs).\n",
    "## With the possibility of assigning weight to edges.\n",
    "\n",
    "class Raw_to_Graph(InMemoryDataset):\n",
    "    def __init__(self, root, threshold, method, weight, transform=None, pre_transform=None):\n",
    "        self.threshold = threshold\n",
    "        self.method = method\n",
    "        self.weight = weight\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    # This function is used to process the raw data into a format suitable for GNNs, by constructing graphs out of the connectivity matrices.\n",
    "    def process(self):\n",
    "        graphs=[]\n",
    "        corr_matrices = full_corr_path_lists[self.method]\n",
    "        for patient_idx, patient_matrix in enumerate(corr_matrices):\n",
    "            path = f'ADNI_full/corr_matrices/corr_matrix_{self.method}/{patient_matrix}'\n",
    "            corr_matrix = pd.read_csv(path, header=None).values\n",
    "            # Here ROIs stands for Regions of Interest\n",
    "            nbr_ROIs = corr_matrix.shape[0]\n",
    "            edge_matrix = np.zeros((nbr_ROIs,nbr_ROIs))\n",
    "            for j in range(nbr_ROIs):\n",
    "                for k in range(nbr_ROIs):\n",
    "                    # Here we are using the absolute value of each element of the correlation matrix, as the corr coeff is in the range [-1,1].\n",
    "                    if np.abs(corr_matrix[ j , k ]) < self.threshold:\n",
    "                        edge_matrix[ j , k ] = 0\n",
    "                    else:\n",
    "                        if self.weight:\n",
    "                            # Here we assign the absolute value of the correlation coefficient as the edge weight.\n",
    "                            edge_matrix[ j , k ] = corr_matrix[ j , k]\n",
    "                        else:\n",
    "                            # Here we assign 1 as the edge weight, i.e. regardless of the the absolute value of the correlation coefficient.\n",
    "                            edge_matrix[ j , k ] = 1\n",
    "\n",
    "            # Create a NetworkX graph from the edge matrix\n",
    "            NetworkX_graph = from_numpy_array(edge_matrix)\n",
    "\n",
    "            # Compute the degree, betweenness centrality, clustering coefficient, local efficiency for each node of the graph and the global efficiency of the graph\n",
    "            degree_dict = dict(NetworkX_graph.degree())\n",
    "            between_central_dict = nx.betweenness_centrality(NetworkX_graph)\n",
    "            cluster_coeff_dict = nx.clustering(NetworkX_graph)\n",
    "            global_eff = nx.global_efficiency(NetworkX_graph)\n",
    "            local_eff_dict = {}\n",
    "            for node in NetworkX_graph.nodes():\n",
    "                subgraph_neighb = NetworkX_graph.subgraph(NetworkX_graph.neighbors(node))\n",
    "                if subgraph_neighb.number_of_nodes() > 1:\n",
    "                    efficiency = nx.global_efficiency(subgraph_neighb)\n",
    "                else:\n",
    "                    efficiency = 0.0\n",
    "                local_eff_dict[node] = efficiency\n",
    "\n",
    "            # Convert the degree, betweenness centrality, local efficiency, clustering coefficient and ratio of local to global efficiency dictionaries to NumPy arrays then normalize them\n",
    "            degree_array = dict_to_array(degree_dict)\n",
    "            degree_array_norm = normalize_array(degree_array)\n",
    "\n",
    "            between_central_array = dict_to_array(between_central_dict)\n",
    "            between_central_array_norm = normalize_array(between_central_array)\n",
    "\n",
    "            local_efficiency_array = dict_to_array(local_eff_dict)\n",
    "            local_eff_array_norm = normalize_array(local_efficiency_array)\n",
    "\n",
    "            ratio_local_global_array = dict_to_array(local_eff_dict) / global_eff\n",
    "            ratio_local_global_array_norm = normalize_array(ratio_local_global_array)\n",
    "\n",
    "            cluster_coeff_array = dict_to_array(cluster_coeff_dict)\n",
    "            cluster_coeff_array_norm = normalize_array(cluster_coeff_array)\n",
    "            \n",
    "            # Extracting the age and sex features of the patient\n",
    "            patient_age = ages[patient_idx]\n",
    "            age_norm = (patient_age - min_age) / (max_age - min_age)\n",
    "            patient_sex = int(sex[patient_idx])\n",
    "            # Making the age and sex arrays the same size as the other arrays\n",
    "            age_array = np.full((nbr_ROIs,), age_norm)\n",
    "            sex_array = np.full((nbr_ROIs,), patient_sex)\n",
    "\n",
    "            # Concatenate the degree, participation coefficient, betweenness centrality, local efficiency, and ratio of local to global efficiency arrays to form a single feature vector\n",
    "            x_conc = torch.tensor(np.concatenate((degree_array_norm, between_central_array_norm, local_eff_array_norm, cluster_coeff_array_norm, ratio_local_global_array_norm, age_array, sex_array)), dtype=torch.float)\n",
    "            # Determining the number of features concatenated to reshape with the correct dimensions\n",
    "            x = torch.reshape(x_conc, (7, nbr_ROIs)).T\n",
    "\n",
    "            # Create a Pytorch Geometric Data object from the NetworkX \n",
    "            graph_data = from_networkx(NetworkX_graph)\n",
    "            ## The feature matrix of the graph is the degree, betweenness centrality, local efficiency, clustering coefficient and ratio of local to global efficiency of each node\n",
    "            graph_data.x = x\n",
    "            ## The target/output variable that we want to predict is the diagnostic label of the patient\n",
    "            graph_data.y = diagnostic_label[patient_idx]\n",
    "            graphs.append(graph_data)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "def dataset_features_and_stats(dataset):\n",
    "    print()\n",
    "    print(f'Dataset: {dataset}:')\n",
    "    print('====================')\n",
    "    print(f'Number of graphs: {len(dataset)}')\n",
    "    print(f'Weighted: {dataset.weight}')\n",
    "    print(f'Threshold: {dataset.threshold}')\n",
    "    print(f'Correlation Method: {dataset.method}')\n",
    "    print(f'Number of features: {dataset.num_features}')\n",
    "    print(f'Number of classes: {len(np.unique(diagnostic_label))}')\n",
    "\n",
    "    # Getting the first graph object in the dataset.\n",
    "    data = dataset[0]\n",
    "\n",
    "    print()\n",
    "    print(data)\n",
    "    print('=============================================================')\n",
    "\n",
    "    # Some statistics about the first graph.\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "    print(f'Has self-loops: {data.has_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "threshold = 0.4\n",
    "weight = False\n",
    "method = 'pearson'\n",
    "\n",
    "root = f'Raw_to_graph/ADNI_T_{threshold}_W_{weight}_M_{method}'\n",
    "dataset = Raw_to_Graph(root=root, threshold=threshold, method=method, weight=weight)\n",
    "dataset_features_and_stats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 137\n",
      "Number of validation graphs: 20\n",
      "Number of test graphs: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Creating the train, validation and test sets\n",
    "X = dataset\n",
    "y = dataset.data.y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.125, random_state=42)\n",
    "\n",
    "print(f'Number of training graphs: {len(X_train)}')\n",
    "print(f'Number of validation graphs: {len(X_valid)}')\n",
    "print(f'Number of test graphs: {len(X_test)}')\n",
    "\n",
    "train_loader = DataLoader(X_train, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(X_valid, batch_size=len(X_valid), shuffle=True)\n",
    "test_loader = DataLoader(X_test, batch_size=len(X_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge(\n",
      "  (convs): ModuleList(\n",
      "    (0): EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=14, out_features=8, bias=True)\n",
      "    ))\n",
      "    (1-2): 2 x EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Defining the model, optimizer and loss function\n",
    "Conv_type = 'Edge'\n",
    "\n",
    "if Conv_type == 'GCN':\n",
    "    model = GCN(in_channels=7, hidden_channels=32, num_layers=5, dropout=0.7)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "elif Conv_type == 'Graph':\n",
    "    model = Graph(in_channels=7, hidden_channels=8, num_layers=5, dropout=0.5)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "elif Conv_type == 'Edge':\n",
    "    model = Edge(in_channels=14, hidden_channels=8, num_layers=3, dropout=0.5)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Printing the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing=False, n_epochs=100):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        for data in train_loader:\n",
    "            # Converting each element of data.y to a float\n",
    "            numeric_torch_y = torch.tensor([float(y) for y in data.y], dtype=torch.float)\n",
    "            # Ensure target tensor has the same shape as model output\n",
    "            target = numeric_torch_y.view(-1, 1).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.num_graphs\n",
    "            train_accuracy += torch.sum(torch.abs(out - target) < 0.5).item()\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracy /= len(train_loader.dataset)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                # Converting each element of data.y to a float\n",
    "                numeric_torch_y = torch.tensor([float(y) for y in data.y], dtype=torch.float)\n",
    "                # Ensure target tensor has the same shape as model output\n",
    "                target = numeric_torch_y.view(-1, 1).float()\n",
    "\n",
    "                out = model(data.x, data.edge_index, data.batch)\n",
    "                loss = criterion(out, target)\n",
    "                valid_loss += loss.item() * data.num_graphs\n",
    "                valid_accuracy += torch.sum(torch.abs(out - target) < 0.5).item()\n",
    "            valid_loss /= len(valid_loader.dataset)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_accuracy /= len(valid_loader.dataset)\n",
    "            valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "            if testing:\n",
    "                test_loss = 0\n",
    "                test_accuracy = 0\n",
    "                for data in test_loader:\n",
    "                    # Converting each element of data.y to a float\n",
    "                    numeric_torch_y = torch.tensor([float(y) for y in data.y], dtype=torch.float)\n",
    "                    # Ensure target tensor has the same shape as model output\n",
    "                    target = numeric_torch_y.view(-1, 1).float()\n",
    "\n",
    "                    out = model(data.x, data.edge_index, data.batch)\n",
    "                    loss = criterion(out, target)\n",
    "                    test_loss += loss.item() * data.num_graphs\n",
    "                    test_accuracy += torch.sum(torch.abs(out - target) < 0.5).item()\n",
    "                test_loss /= len(test_loader.dataset)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracy /= len(test_loader.dataset)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "\n",
    "        if testing:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "            print(f'Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {valid_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "            print(f'Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {valid_accuracy:.4f}')\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=f'Train Loss')\n",
    "    plt.plot(valid_losses, label=f'Validation Loss')\n",
    "    if testing:\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=f'Train Accuracy')\n",
    "    plt.plot(valid_accuracies, label=f'Validation Accuracy')\n",
    "    if testing:\n",
    "        plt.plot(test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if testing:\n",
    "        return train_losses, valid_losses, test_losses\n",
    "    else:\n",
    "        return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (31086x16 and 8x8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Running the training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, n_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m target \u001b[38;5;241m=\u001b[39m numeric_torch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, target)\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mEdge.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 13\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/nn/conv/edge_conv.py:60\u001b[0m, in \u001b[0;36mEdgeConv.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     58\u001b[0m     x: PairTensor \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: PairTensor)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:463\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 463\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    465\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/nn/conv/edge_conv.py:63\u001b[0m, in \u001b[0;36mEdgeConv.message\u001b[0;34m(self, x_i, x_j)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_i: Tensor, x_j: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (31086x16 and 8x8)"
     ]
    }
   ],
   "source": [
    "# Running the training\n",
    "train_losses, valid_losses = train(model, optimizer, criterion, train_loader, valid_loader, test_loader, n_epochs=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alzheimers-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
