{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import torch\n",
    "# import networkx as nx\n",
    "# from torch_geometric.data import InMemoryDataset\n",
    "# from torch_geometric.utils import from_numpy_array, from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of lists of paths to the correlation matrices for each method. Each list in the dictionary represents a different method.\n",
    "methods = ['pearson', 'spearman', 'kendall', 'partial']\n",
    "full_corr_path_lists = {}\n",
    "for method in methods:\n",
    "    method_dir = f'ADNI_full/corr_matrices/corr_matrix_{method}/'\n",
    "    full_corr_path_lists[method] = []\n",
    "    for file in os.listdir(method_dir):\n",
    "        full_corr_path_lists[method].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the diagnostic file from the diagnostic_label.csv file\n",
    "labels_full = np.loadtxt('ADNI_full/diagnostic_label.csv', dtype=str, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTINUER A PARTIR DE LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove \"SMC\" values\n",
    "idx = f.filter_SMC_patient_info()\n",
    "corr_matrices = [corr_matrices_full[i] for i in idx]\n",
    "labels = [labels_full[i] for i in idx]\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 'CN':\n",
    "        labels[i] = 0\n",
    "\n",
    "    elif labels[i] == 'EMCI' or labels[i] == 'MCI' or labels[i] == 'LMCI':\n",
    "        labels[i] = 1\n",
    "\n",
    "    elif labels[i] == 'AD':\n",
    "        labels[i] = 2\n",
    "\n",
    "    else:\n",
    "        print('Error: incorrect label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(labels) == len(corr_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNI_dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, threshold=0.4):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Converts raw data into GNN-readable format by constructing\n",
    "        graphs out of connectivity matrices.\n",
    "\n",
    "        \"\"\"\n",
    "        graphs=[]\n",
    "        for i in range(len(corr_matrices)):\n",
    "            corr_matrix = corr_matrices[i]\n",
    "            n_rois = corr_matrix.shape[0]\n",
    "            edge_matrix = np.zeros((n_rois,n_rois))\n",
    "            for j in range(n_rois):\n",
    "                for k in range(n_rois):\n",
    "                    if np.abs(corr_matrix[ j , k ]) < self.threshold:\n",
    "                        edge_matrix[ j , k ] = 0\n",
    "                    else:\n",
    "                        edge_matrix[ j , k ] = corr_matrix[ j , k]\n",
    "\n",
    "            corr_matrix_nx = from_numpy_array(edge_matrix)\n",
    "\n",
    "            deg_dict = dict(corr_matrix_nx.degree())\n",
    "            bc_dict = nx.betweenness_centrality(corr_matrix_nx)\n",
    "            cc_dict = nx.clustering(corr_matrix_nx)\n",
    "            # Compute the global efficiency of the graph\n",
    "            ge = nx.global_efficiency(corr_matrix_nx)\n",
    "\n",
    "            le_dict = {}\n",
    "\n",
    "            # loop over all nodes in the graph\n",
    "            for node in corr_matrix_nx.nodes():\n",
    "                # find the subgraph of neighbors of the current node\n",
    "                subgraph = corr_matrix_nx.subgraph(corr_matrix_nx.neighbors(node))\n",
    "\n",
    "                # calculate the efficiency of the subgraph\n",
    "                if subgraph.number_of_nodes() > 1:\n",
    "                    efficiency = nx.global_efficiency(subgraph)\n",
    "                else:\n",
    "                    efficiency = 0.0\n",
    "\n",
    "                # store the efficiency in the dictionary\n",
    "                le_dict[node] = efficiency\n",
    "\n",
    "\n",
    "            # Compute the participation coefficient and ratio of local to global efficiency of each node\n",
    "            ratio_le_ge = np.array(list(le_dict.values())) / ge\n",
    "\n",
    "            # Convert the degree, participation coefficient, betweenness centrality, local efficiency, and ratio of local to global efficiency dictionaries to NumPy arrays\n",
    "            deg_array = np.array(list(deg_dict.values()))\n",
    "            bc_array = np.array(list(bc_dict.values()))\n",
    "            le_array = np.array(list(le_dict.values()))\n",
    "\n",
    "\n",
    "\n",
    "            cc_array = np.array(list(cc_dict.values()))\n",
    "            ratio_le_ge_array = ratio_le_ge\n",
    "\n",
    "            # Normalize the degree, participation coefficient, betweenness centrality, local efficiency, and ratio of local to global efficiency arrays to have zero mean and unit variance\n",
    "            deg_array_norm = (deg_array - np.mean(deg_array)) / np.std(deg_array)\n",
    "            bc_array_norm = (bc_array - np.mean(bc_array)) / np.std(bc_array)\n",
    "            le_array_norm = (le_array - np.mean(le_array)) / np.std(le_array)\n",
    "            ratio_le_ge_array_norm = (ratio_le_ge_array - np.mean(ratio_le_ge_array)) / np.std(ratio_le_ge_array)\n",
    "            cc_array_norm = (cc_array - np.mean(cc_array)) / np.std(cc_array)\n",
    "\n",
    "            # Concatenate the degree, participation coefficient, betweenness centrality, local efficiency, and ratio of local to global efficiency arrays to form a single feature vector\n",
    "            x_conc = torch.tensor(np.concatenate((deg_array_norm, bc_array_norm, le_array_norm, cc_array_norm, ratio_le_ge_array_norm)), dtype=torch.float)\n",
    "            x = torch.reshape(x_conc , (5 , n_rois)).T\n",
    "\n",
    "\n",
    "            corr_matrix_data = from_networkx(corr_matrix_nx)\n",
    "            corr_matrix_data.x = x\n",
    "            corr_matrix_data.y = labels[i]\n",
    "            #pcorr_matrix_data.pos = coordinates\n",
    "\n",
    "            # Add to running list of all dataset items\n",
    "            graphs.append(corr_matrix_data)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ADNI_dataset('ADNI_0.5')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J'ai peut-Ãªtre besoin d'importer des trucs de la mais pour l'instant je ne sais pas quoi\n",
    "from nilearn import datasets, plotting, image\n",
    "import nibabel as nib\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.interfaces.fmriprep import load_confounds\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU, BatchNorm1d\n",
    "from torch_geometric.nn import EdgeConv, GCNConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import wandb\n",
    "import random\n",
    "import functions as f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alzheimers-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
