{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82f483a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ReLU, Linear, Sequential, ModuleList\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458d71b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME']=\"HGAT.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805697be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Hypergraph Attention Neural Network\n",
    "class HGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, heads, nbr_classes):\n",
    "        super(HGAT, self).__init__()\n",
    "        self.nbr_classes = nbr_classes\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(f.HypergraphConv(in_channels, hidden_channels, use_attention=True, heads=heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(f.HypergraphConv(hidden_channels * heads, hidden_channels, use_attention=True, heads=heads))\n",
    "        self.mlp = Sequential(Linear(hidden_channels * heads, hidden_channels), ReLU(), Linear(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # xs = []\n",
    "        for conv in self.convs:\n",
    "            print(edge_index)\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "                # xs.append(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # x = torch.cat(xs, dim=1)\n",
    "        # x = self.double_mlp(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c63a67b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Testing the class Raw_to_Hypergraph with one example and saving it\n",
    "threshold = 0.5\n",
    "age = False\n",
    "sex = False\n",
    "method = 'fourier_cluster'\n",
    "weight = False\n",
    "\n",
    "hg_data_path = f'Hypergraphs/{method}/thresh_{threshold}'\n",
    "\n",
    "root = f'Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MPTrue'\n",
    "dataset = f.Raw_to_Hypergraph(root=root, hg_data_path=hg_data_path, method=method, weight=weight, threshold=threshold, age=age, sex=sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "734f1135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 137\n",
      "Number of validation graphs: 20\n",
      "Number of test graphs: 40\n",
      "Number of classes: 4\n",
      "tensor([1., 2., 0., 2., 0., 0., 2., 0., 1., 0., 2., 3., 2., 1., 3., 2., 2., 2.,\n",
      "        2., 2., 2., 3., 3., 1., 2., 1., 2., 1., 3., 0., 3., 0., 0., 0., 3., 3.,\n",
      "        2., 0., 3., 2., 0., 3., 1., 1., 1., 0., 2., 3., 0., 3., 3., 2., 2., 2.,\n",
      "        1., 0., 1., 0., 0., 3., 1., 3., 0., 2., 1., 0., 2., 2., 2., 2., 3., 2.,\n",
      "        2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 0., 1., 0., 2., 2., 2.,\n",
      "        2., 2., 0., 2., 2., 0., 3., 2., 2., 1., 2., 0., 2., 2., 2., 2., 0., 2.,\n",
      "        2., 2., 0., 0., 2., 3., 2., 2., 2., 2., 3., 0., 2., 1., 2., 3., 3., 0.,\n",
      "        3., 2., 2., 1., 2., 0., 2., 2., 0., 1., 0.])\n",
      "torch.Size([137])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Creating the train, validation and test sets\n",
    "train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset)\n",
    "print(y_train)\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "065f29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratify = True\n",
    "# Training the model\n",
    "def train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, test_loader=False, testing=False, n_epochs=100):\n",
    "    test_loader = test_loader\n",
    "    testing = testing\n",
    "    n_epochs = n_epochs\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    max_valid_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    weight = 'NA'\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = \"Alzheimers_GNN\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config = {\n",
    "        \"architecture\": \"HGAT\",\n",
    "        \"method\": method,\n",
    "        \"strat + w loss\": stratify,\n",
    "        \"weights\": weight,\n",
    "        \"corr_m node feature\": True,\n",
    "        \"weight_decay\": w_decay,\n",
    "        \"threshold\": threshold,\n",
    "        \"matrix profiling\": True,\n",
    "        \"learning_rate\": parameters[0],\n",
    "        \"hidden_channels\": parameters[1],\n",
    "        \"num_layers\": parameters[2],\n",
    "        \"dropout\": parameters[3],\n",
    "        \"epochs\": n_epochs},)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if testing:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy, \"Test Accuracy\": test_accuracy})\n",
    "        else:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy})\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "        print(f'Train Loss: {train_losses[-1]:.4f}, Validation Loss: {valid_losses[-1]:.4f}')\n",
    "        print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {valid_accuracies[-1]:.4f}')\n",
    "        print(f'Max Validation Accuracy: {max_valid_accuracy:.4f}')\n",
    "\n",
    "    if testing:\n",
    "        print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=f'Train Loss')\n",
    "    plt.plot(valid_losses, label=f'Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=f'Train Accuracy')\n",
    "    plt.plot(valid_accuracies, label=f'Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    lr = parameters[0]\n",
    "    hidden_channels = parameters[1]\n",
    "    num_layers = parameters[2]\n",
    "    dropout = parameters[3]\n",
    "    heads = parameters[4]\n",
    "    filename = f'HGAT_Models_MP/threshold_{threshold}/method_{method}/lr{lr}_hc{hidden_channels}_nl{num_layers}_d{dropout}_epochs{n_epochs}_wdecay{w_decay}_w{weight}.png'\n",
    "    plt.savefig(filename)\n",
    "    if testing:\n",
    "        plt.title(f'Test Accuracy: {test_accuracy}')\n",
    "    plt.show()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    if testing:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy\n",
    "    else:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1a567",
   "metadata": {},
   "source": [
    "Why unvalid loader for maximal_clique hypergraphs?\n",
    "\n",
    "None hyperedge attr when using attention ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f69e0bb4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "<unknown>:564: DeprecationWarning: invalid escape sequence '\\m'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 137\n",
      "Number of validation graphs: 20\n",
      "Number of test graphs: 40\n",
      "Number of classes: 4\n",
      "HGAT(\n",
      "  (convs): ModuleList(\n",
      "    (0): HypergraphConv(572, 64)\n",
      "    (1): HypergraphConv(128, 64)\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmathilde-cros\u001b[0m (\u001b[33malzheimers-cl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mathilde/Documents/Bachelor-Thesis-Alzheimers-HGNN/wandb/run-20240530_011726-w9g08m43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN/runs/w9g08m43' target=\"_blank\">gallant-paper-9683</a></strong> to <a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN' target=\"_blank\">https://wandb.ai/alzheimers-cl/Alzheimers_GNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN/runs/w9g08m43' target=\"_blank\">https://wandb.ai/alzheimers-cl/Alzheimers_GNN/runs/w9g08m43</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,   56,  ..., 1853, 1854, 1855],\n",
      "        [   0,    0,    0,  ..., 1794, 1795, 1796]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Running the training\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, test_loader, testing, n_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Valid Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_valid_accuracy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_accuracy})\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_accuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_valid_accuracy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Valid Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_valid_accuracy})\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Bachelor-Thesis-Alzheimers-HGNN/functions.py:747\u001b[0m, in \u001b[0;36mepochs_training\u001b[0;34m(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\u001b[0m\n\u001b[1;32m    745\u001b[0m target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    746\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 747\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, target)\n\u001b[1;32m    749\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mHGAT.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(edge_index)\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Bachelor-Thesis-Alzheimers-HGNN/functions.py:689\u001b[0m, in \u001b[0;36mHypergraphConv.forward\u001b[0;34m(self, x, hyperedge_index, hyperedge_weight, hyperedge_attr)\u001b[0m\n\u001b[1;32m    687\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_attention:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m hyperedge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n\u001b[1;32m    691\u001b[0m     hyperedge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(hyperedge_attr)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example model\n",
    "\n",
    "threshold = 0.5\n",
    "age = False\n",
    "sex = False\n",
    "method = 'fourier_cluster'\n",
    "weight = False\n",
    "\n",
    "hg_data_path = f'Hypergraphs/{method}/thresh_{threshold}'\n",
    "root = f'Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MPTrue'\n",
    "dataset = f.Raw_to_Hypergraph(root=root, hg_data_path=hg_data_path, method=method, weight=weight, threshold=threshold, age=age, sex=sex)\n",
    "\n",
    "train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset)\n",
    "\n",
    "# Defining the model, optimizer and loss function\n",
    "lr=0.0001\n",
    "hidden_channels=64\n",
    "num_layers=2\n",
    "dropout=0.1\n",
    "heads=2\n",
    "w_decay = 0\n",
    "parameters = [lr, hidden_channels, num_layers, dropout, heads]\n",
    "in_channels = dataset.num_features\n",
    "\n",
    "model = HGAT(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], heads=parameters[4], nbr_classes=nbr_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Printing the model architecture\n",
    "print(model)\n",
    "\n",
    "# Running the training\n",
    "train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, n_epochs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bb180",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## A CHANGER POUR QU'IL Y AIT LE HEADS PARAMETER\n",
    "\n",
    "# # Doing some parameter gridsearch to find the best hyperparameters\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# # Building the graphs\n",
    "# threshold = 0.5\n",
    "# age = False\n",
    "# sex = False\n",
    "# method = 'fourier_cluster'\n",
    "# weight = False\n",
    "\n",
    "# hg_data_path = f'Hypergraphs/{method}/thresh_{threshold}'\n",
    "# root = f'Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MPTrue'\n",
    "# dataset = f.Raw_to_Hypergraph(root=root, hg_data_path=hg_data_path, method=method, weight=weight, threshold=threshold, age=age, sex=sex)\n",
    "\n",
    "# # Creating the train, validation and test sets\n",
    "# train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset, stratify)\n",
    "\n",
    "# # param_grid = {\n",
    "# #     'learning_rate': [0.001, 0.0001],\n",
    "# #     'hidden_channels': [128, 64, 32],\n",
    "# #     'num_layers': [3, 2, 1],\n",
    "# #     'dropout_rate': [0.2, 0.1, 0.0],\n",
    "# #     'weight_decay': [0.001, 0.0001]\n",
    "# # }\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.0001, 0.001],\n",
    "#     'hidden_channels': [64, 128],\n",
    "#     'num_layers': [1, 2, 3],\n",
    "#     'dropout_rate': [0.0, 0.1, 0.2],\n",
    "#     'weight_decay': [0.0001, 0.001]\n",
    "# }\n",
    "\n",
    "# # Create combinations of hyperparameters\n",
    "# param_combinations = ParameterGrid(param_grid)\n",
    "# n_epochs = 800\n",
    "# in_channels = dataset.num_features\n",
    "# # Train using each combination\n",
    "# for params in param_combinations:\n",
    "#     filename = f'HGAT_Models_MP/threshold_{threshold}/method_{method}/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}_wdecay{params[\"weight_decay\"]}_w{weight}.png'\n",
    "#     if os.path.exists(filename):\n",
    "#         pass\n",
    "#     else:\n",
    "#         parameters = [params['learning_rate'], params['hidden_channels'], params['num_layers'], params['dropout_rate']]\n",
    "#         model = HGAT(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "#         if stratify:\n",
    "#             diag_lab = [0 , 1 , 2, 3]\n",
    "#             class_freq = []\n",
    "#             for i in diag_lab:\n",
    "#                 class_freq.append(np.count_nonzero(torch.Tensor(y_train) == i))\n",
    "#             class_freq = torch.FloatTensor(class_freq)\n",
    "#             class_weights = 1 / class_freq\n",
    "#             class_weights /= class_weights.sum()\n",
    "#             criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "#         else:\n",
    "#             criterion = torch.nn.CrossEntropyLoss() \n",
    "#         if 'weight_decay' not in params.keys():\n",
    "#             w_decay = 0\n",
    "#         else:\n",
    "#             w_decay = params['weight_decay']\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "#         train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, test_loader, testing=True, n_epochs=800)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "alzheimers-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
