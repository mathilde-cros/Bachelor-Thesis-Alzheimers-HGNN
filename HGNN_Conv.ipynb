{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ReLU, Linear, Sequential, ModuleList\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME']=\"HGNN_Conv.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Hypergraph Networks\n",
    "class HGConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, nbr_classes):\n",
    "        super(HGConv, self).__init__()\n",
    "        self.nbr_classes = nbr_classes\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(f.HypergraphConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(f.HypergraphConv(hidden_channels, hidden_channels))\n",
    "        # self.double_mlp = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels, hidden_channels))\n",
    "        self.mlp = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # xs = []\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "                # xs.append(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # x = torch.cat(xs, dim=1)\n",
    "        # x = self.double_mlp(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the class Raw_to_Hypergraph with one example and saving it\n",
    "threshold = 0.4\n",
    "weight = False\n",
    "age = False\n",
    "sex = False\n",
    "matrixprofile = False\n",
    "if matrixprofile:\n",
    "    in_channels = 461 + int(age) + int(sex)\n",
    "else:\n",
    "    in_channels = 5 + int(age) + int(sex)\n",
    "method = 'pearson'\n",
    "\n",
    "root = f'Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MP{matrixprofile}'\n",
    "dataset = f.Raw_to_Hypergraph(root=root, threshold=threshold, method=method, weight=weight, sex=sex, age=age, matrixprofile=matrixprofile)\n",
    "f.dataset_features_and_stats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train, validation and test sets\n",
    "train_loader, valid_loader, test_loader, nbr_classes = f.create_train_test_valid(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train(model, optimizer, criterion, train_loader, valid_loader, parameters, test_loader=False, testing=False, n_epochs=100):\n",
    "    test_loader = test_loader\n",
    "    testing = testing\n",
    "    n_epochs = n_epochs\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = \"Alzheimers_GNN\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config = {\n",
    "        \"architecture\": \"HGConv\",\n",
    "        \"weights\": weight,\n",
    "        \"matrix profiling\": matrixprofile,\n",
    "        \"learning_rate\": parameters[0],\n",
    "        \"hidden_channels\": parameters[1],\n",
    "        \"num_layers\": parameters[2],\n",
    "        \"dropout\": parameters[3],\n",
    "        \"epochs\": n_epochs},)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if testing:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, test_losses, test_accuracies = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, train_losses, train_accuracies, valid_losses, valid_accuracies, test_losses, test_accuracies)\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "            print(f'Train Loss: {train_losses[-1]:.4f}, Validation Loss: {valid_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
    "            print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {valid_accuracies[-1]:.4f}, Test Accuracy: {test_accuracies[-1]:.4f}')\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Test Loss\": test_losses[-1], \"Test Accuracy\": test_accuracies[-1]})\n",
    "        else:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies= f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, train_losses, train_accuracies, valid_losses, valid_accuracies)\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "            print(f'Train Loss: {train_losses[-1]:.4f}, Validation Loss: {valid_losses[-1]:.4f}')\n",
    "            print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {valid_accuracies[-1]:.4f}')\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1]})\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=f'Train Loss')\n",
    "    plt.plot(valid_losses, label=f'Validation Loss')\n",
    "    if testing:\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=f'Train Accuracy')\n",
    "    plt.plot(valid_accuracies, label=f'Validation Accuracy')\n",
    "    if testing:\n",
    "        plt.plot(test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    lr = parameters[0]\n",
    "    hidden_channels = parameters[1]\n",
    "    num_layers = parameters[2]\n",
    "    dropout = parameters[3]\n",
    "    if matrixprofile:\n",
    "        filename = f'HGConv_Models_MP/lr{lr}_hc{hidden_channels}_nl{num_layers}_d{dropout}_epochs{n_epochs}.png'\n",
    "    else:\n",
    "        filename = f'HGConv_Models/lr{lr}_hc{hidden_channels}_nl{num_layers}_d{dropout}_epochs{n_epochs}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    if testing:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, test_losses, test_accuracies\n",
    "    else:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model, optimizer and loss function\n",
    "lr=0.00001\n",
    "hidden_channels=32\n",
    "num_layers=3\n",
    "dropout=0.2\n",
    "parameters = [lr, hidden_channels, num_layers, dropout]\n",
    "\n",
    "model = HGConv(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Printing the model architecture\n",
    "print(model)\n",
    "\n",
    "# Running the training\n",
    "train_losses, train_accuracies, valid_losses, valid_accuracies = train(model, optimizer, criterion, train_loader, valid_loader, parameters, n_epochs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Doing some parameter gridsearch to find the best hyperparameters\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# MP = True\n",
    "\n",
    "# # param_grid = {\n",
    "# #     'learning_rate': [0.01, 0.001, 0.0001, 0.00001],\n",
    "# #     'hidden_channels': [128, 64, 32],\n",
    "# #     'num_layers': [3, 2, 1],\n",
    "# #     'dropout_rate': [0.3, 0.2, 0.1, 0.0]\n",
    "# # }\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.00001, 0.0001, 0.001, 0.01],\n",
    "#     'hidden_channels': [32, 64, 128],\n",
    "#     'num_layers': [1, 2, 3],\n",
    "#     'dropout_rate': [0.0, 0.1, 0.2, 0.3]\n",
    "# }\n",
    "\n",
    "# # Create combinations of hyperparameters\n",
    "# param_combinations = ParameterGrid(param_grid)\n",
    "# n_epochs = 800\n",
    "# # Train using each combination\n",
    "# for params in param_combinations:\n",
    "#     if MP:\n",
    "#         filename = f'HGConv_Models_MP/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}.png'\n",
    "#     else:\n",
    "#         filename = f'HGConv_Models/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}.png'\n",
    "#     if os.path.exists(filename):\n",
    "#         pass\n",
    "#     else:\n",
    "#         parameters = [params['learning_rate'], params['hidden_channels'], params['num_layers'], params['dropout_rate']]\n",
    "#         model = HGConv(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0])\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "#         train_losses, train_accuracies, valid_losses, valid_accuracies = train(model, optimizer, criterion, train_loader, valid_loader, parameters, n_epochs=800)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
