{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import wandb\n",
    "\n",
    "import models as m\n",
    "import functions as f\n",
    "from functions import dict_to_array, normalize_array\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.convert_matrix import from_numpy_array\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the fake dataset. \n",
    "# We decided that patients without Alzheimer's (class 0) would have a stronger connectivity of their brain regions in the top left corner of the correlation matrix. \n",
    "# This is a simplification of the real data, but it will allow us to test the model without the complications from the data.\n",
    "\n",
    "def generate_correlation_matrix(dimensions, stronger_top_left=True):\n",
    "    # Generate a random matrix\n",
    "    matrix = np.zeros((dimensions, dimensions))\n",
    "    \n",
    "    # Introduce more 1's in the top-left corner for class 1 matrices\n",
    "    for row in range(dimensions):\n",
    "        for column in range(dimensions):\n",
    "            if row <= column and random.random() < 0.25:\n",
    "                matrix[row, column] = 1\n",
    "    for i in range(dimensions//2):\n",
    "        for j in range(dimensions//2):\n",
    "            if i <= j:\n",
    "                if stronger_top_left and random.random() < 0.75:  # 75% chance of setting a value to 1\n",
    "                    matrix[i, j] = 1\n",
    "\n",
    "    # Make the matrix symmetric\n",
    "    corr_matrix = np.maximum(matrix, matrix.T)\n",
    "    \n",
    "    # Make the diagonal elements equal to 1\n",
    "    np.fill_diagonal(corr_matrix, 1)\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "def generate_dataset(num_samples, dimensions, stronger_top_left=True, class_ratio=0.5):\n",
    "    num_class0 = int(num_samples * class_ratio)\n",
    "    num_class1 = num_samples - num_class0\n",
    "    \n",
    "    class0_matrices = [generate_correlation_matrix(dimensions, stronger_top_left) for _ in range(num_class0)]\n",
    "    class1_matrices = [generate_correlation_matrix(dimensions, stronger_top_left=False) for _ in range(num_class1)]\n",
    "    \n",
    "    labels = [0] * num_class0 + [1] * num_class1\n",
    "    \n",
    "    # Shuffle the data\n",
    "    combined = list(zip(class0_matrices + class1_matrices, labels))\n",
    "    random.shuffle(combined)\n",
    "    corr_matrices, labels = zip(*combined)\n",
    "    \n",
    "    return corr_matrices, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 1. 0. ... 0. 1. 1.]]\n",
      "0\n",
      "\n",
      "[[1. 1. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "0\n",
      "\n",
      "[[1. 0. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "1\n",
      "\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "0\n",
      "\n",
      "[[1. 1. 0. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 1.]]\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining the properties of our dataset\n",
    "num_samples = 1000\n",
    "dimensions = 116\n",
    "class_ratio = 0.5  # Ratio of samples for class 1\n",
    "corr_matrices, labels = generate_dataset(num_samples, dimensions, stronger_top_left=True, class_ratio=class_ratio)\n",
    "\n",
    "# matrices contain the generated correlation matrices\n",
    "# labels contain the corresponding class labels\n",
    "\n",
    "for i in range(5):\n",
    "    print(corr_matrices[i])\n",
    "    print(labels[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running different models on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "stratify = True\n",
    "def train_fake(model, filename, method_wandb, optimizer, criterion, w_decay, parameters, train_loader, valid_loader, test_loader=False, testing=False, n_epochs=80):\n",
    "    test_loader = test_loader\n",
    "    testing = testing\n",
    "    n_epochs = n_epochs\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    max_valid_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = \"Fake_Alzheimers\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config = {\n",
    "        \"model type\": method_wandb,\n",
    "        \"strat + w loss\": stratify,\n",
    "        \"weight_decay\": w_decay,\n",
    "        \"learning_rate\": parameters[0],\n",
    "        \"hidden_channels\": parameters[1],\n",
    "        \"num_layers\": parameters[2],\n",
    "        \"dropout\": parameters[3],\n",
    "        \"epochs\": n_epochs},)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if testing:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy, \"Test Accuracy\": test_accuracy})\n",
    "        else:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy})\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "        print(f'Train Loss: {train_losses[-1]:.4f}, Validation Loss: {valid_losses[-1]:.4f}')\n",
    "        print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {valid_accuracies[-1]:.4f}')\n",
    "        print(f'Max Validation Accuracy: {max_valid_accuracy:.4f}')\n",
    "\n",
    "    if testing:\n",
    "        print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=f'Train Loss')\n",
    "    plt.plot(valid_losses, label=f'Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=f'Train Accuracy')\n",
    "    plt.plot(valid_accuracies, label=f'Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    wandb.finish()\n",
    "\n",
    "    if testing:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy\n",
    "    else:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class to preprocess raw data into a format suitable for training Graph Neural Networks (GNNs).\n",
    "## With the possibility of assigning weight to edges, adding the age feature, sex feature, and matrixe profiling.\n",
    "\n",
    "class Fake2C_Raw_to_graph(InMemoryDataset):\n",
    "    def __init__(self, root, corr_matrices, labels, transform=None, pre_transform=None):\n",
    "        self.corr_matrices = corr_matrices\n",
    "        self.labels = labels\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    # This function is used to process the raw data into a format suitable for GNNs, by constructing graphs out of the connectivity matrices.\n",
    "    def process(self):\n",
    "        graphs=[]\n",
    "        for patient_idx, patient_matrix in enumerate(corr_matrices):\n",
    "            # Here ROIs stands for Regions of Interest\n",
    "            nbr_ROIs = patient_matrix.shape[0]\n",
    "            edge_matrix = np.zeros((nbr_ROIs,nbr_ROIs))\n",
    "            for j in range(nbr_ROIs):\n",
    "                for k in range(nbr_ROIs):\n",
    "                    edge_matrix[j,k] = patient_matrix[j,k]\n",
    "\n",
    "            # Create a NetworkX graph from the edge matrix\n",
    "            NetworkX_graph = from_numpy_array(edge_matrix)\n",
    "\n",
    "            # Compute the degree, betweenness centrality, clustering coefficient, local efficiency for each node of the graph and the global efficiency of the graph\n",
    "            degree_dict = dict(NetworkX_graph.degree())\n",
    "            between_central_dict = nx.betweenness_centrality(NetworkX_graph)\n",
    "            cluster_coeff_dict = nx.clustering(NetworkX_graph)\n",
    "            global_eff = nx.global_efficiency(NetworkX_graph)\n",
    "            local_eff_dict = {}\n",
    "            for node in NetworkX_graph.nodes():\n",
    "                subgraph_neighb = NetworkX_graph.subgraph(NetworkX_graph.neighbors(node))\n",
    "                if subgraph_neighb.number_of_nodes() > 1:\n",
    "                    efficiency = nx.global_efficiency(subgraph_neighb)\n",
    "                else:\n",
    "                    efficiency = 0.0\n",
    "                local_eff_dict[node] = efficiency\n",
    "\n",
    "            # Convert the degree, betweenness centrality, local efficiency, clustering coefficient and ratio of local to global efficiency dictionaries to NumPy arrays then normalize them\n",
    "            degree_array = dict_to_array(degree_dict)\n",
    "            degree_array_norm = normalize_array(degree_array)\n",
    "\n",
    "            between_central_array = dict_to_array(between_central_dict)\n",
    "            between_central_array_norm = normalize_array(between_central_array)\n",
    "\n",
    "            local_efficiency_array = dict_to_array(local_eff_dict)\n",
    "            local_eff_array_norm = normalize_array(local_efficiency_array)\n",
    "\n",
    "            ratio_local_global_array = dict_to_array(local_eff_dict) / global_eff\n",
    "            ratio_local_global_array_norm = normalize_array(ratio_local_global_array)\n",
    "\n",
    "            cluster_coeff_array = dict_to_array(cluster_coeff_dict)\n",
    "            cluster_coeff_array_norm = normalize_array(cluster_coeff_array)\n",
    "\n",
    "            # Initializing an array for the graph features\n",
    "            x_array = np.stack([degree_array_norm, between_central_array_norm, local_eff_array_norm, cluster_coeff_array_norm, ratio_local_global_array_norm], axis=-1)\n",
    "            x_array = x_array.astype(np.float32)\n",
    "\n",
    "            # Concatenate the degree, participation coefficient, betweenness centrality, local efficiency, and ratio of local to global efficiency arrays to form a single feature vector\n",
    "            x = torch.tensor(x_array, dtype=torch.float)\n",
    "\n",
    "            # Create a Pytorch Geometric Data object from the NetworkX\n",
    "            graph_data = from_networkx(NetworkX_graph)\n",
    "            ## The feature matrix of the graph is the degree, betweenness centrality, local efficiency, clustering coefficient and ratio of local to global efficiency of each node\n",
    "            graph_data.x = x\n",
    "            ## The target/output variable that we want to predict is the diagnostic label of the patient\n",
    "            graph_data.y = float(labels[patient_idx])\n",
    "            graphs.append(graph_data)\n",
    "            print('done with patient', patient_idx)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME']=\"Fake2C_GCN.ipynb\"\n",
    "method = 'GCN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(edge_index=[2, 5246], weight=[5246], x=[116, 5], y=[1], num_nodes=116)\n",
      "=============================================================\n",
      "Number of nodes: 116\n",
      "Number of edges: 5246\n",
      "Average node degree: 45.22\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "root = f'Fake2C_Raw_to_graph/model{method}'\n",
    "dataset = Fake2C_Raw_to_graph(root, corr_matrices, labels)\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 700\n",
      "Number of validation graphs: 100\n",
      "Number of test graphs: 200\n",
      "Number of classes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: run alzheimers-cl/Fake_Alzheimers/2qixmudb was previously created and deleted; try a new run name (<Response [409]>)\n",
      "Thread SenderThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/apis/normalize.py\", line 41, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/internal_api.py\", line 2216, in upsert_run\n",
      "    response = self.gql(\n",
      "               ^^^^^^^^^\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/internal_api.py\", line 341, in gql\n",
      "    ret = self._retry_gql(\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/retry.py\", line 131, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/internal_api.py\", line 369, in execute\n",
      "    return self.client.execute(*args, **kwargs)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/gql_request.py\", line 59, in execute\n",
      "    request.raise_for_status()\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://api.wandb.ai/graphql\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n",
      "    self._run()\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._process(record)\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/internal.py\", line 328, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 389, in send\n",
      "    send_handler(record)\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 411, in send_request\n",
      "    send_handler(record)\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 639, in send_request_defer\n",
      "    self.debounce(final=True)\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 540, in debounce\n",
      "    self._maybe_update_config(always=final)\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 517, in _maybe_update_config\n",
      "    self._debounce_config()\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 546, in _debounce_config\n",
      "    self._api.upsert_run(\n",
      "  File \"/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/apis/normalize.py\", line 51, in wrapper\n",
      "    raise CommError(message, error)\n",
      "wandb.errors.CommError: run alzheimers-cl/Fake_Alzheimers/2qixmudb was previously created and deleted; try a new run name (Error 409: Conflict)\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 854 getcaller\n"
     ]
    },
    {
     "ename": "MailboxError",
     "evalue": "transport failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMailboxError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m     w_decay \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mparameters[\u001b[38;5;241m0\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mw_decay)\n\u001b[0;32m---> 43\u001b[0m train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_wandb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m, in \u001b[0;36mtrain_fake\u001b[0;34m(model, filename, method_wandb, optimizer, criterion, w_decay, parameters, train_loader, valid_loader, test_loader, testing, n_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# start a new wandb run to track this script\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# set the wandb project where this run will be logged\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFake_Alzheimers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# track hyperparameters and run metadata\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrat + w loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m testing:\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1195\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m         logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1176\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1174\u001b[0m except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:756\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicating run to backend with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m second timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m run_init_handle \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_run(run)\n\u001b[0;32m--> 756\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_init_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcancel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    762\u001b[0m     run_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mrun_result\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:281\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keepalive \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slot\u001b[38;5;241m.\u001b[39m_get_and_clear(timeout\u001b[38;5;241m=\u001b[39mwait_timeout)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n",
      "\u001b[0;31mMailboxError\u001b[0m: transport failed"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.0001],\n",
    "    'hidden_channels': [64],\n",
    "    'num_layers': [1],\n",
    "    'dropout_rate': [0.0],\n",
    "    'weight_decay': [0.0001]\n",
    "}\n",
    "\n",
    "# Creating the train, validation and test sets\n",
    "train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset, stratify)\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "param_combinations = ParameterGrid(param_grid)\n",
    "n_epochs = 800\n",
    "in_channels = 5\n",
    "nbr_classes = 2\n",
    "stratify = True\n",
    "method_wandb = 'GCN'\n",
    "# Train using each combination\n",
    "for params in param_combinations:\n",
    "    filename = f'Fake2C_Models/GCN/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}_wdecay{params[\"weight_decay\"]}.png'\n",
    "    if os.path.exists(filename):\n",
    "        pass\n",
    "    else:\n",
    "        parameters = [params['learning_rate'], params['hidden_channels'], params['num_layers'], params['dropout_rate']]\n",
    "        model = m.GCN(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "        if stratify:\n",
    "            diag_lab = [0 , 1]\n",
    "            class_freq = []\n",
    "            for i in diag_lab:\n",
    "                class_freq.append(np.count_nonzero(torch.Tensor(y_train) == i))\n",
    "            class_freq = torch.FloatTensor(class_freq)\n",
    "            class_weights = 1 / class_freq\n",
    "            class_weights /= class_weights.sum()\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = torch.nn.CrossEntropyLoss() \n",
    "        if 'weight_decay' not in params.keys():\n",
    "            w_decay = 0\n",
    "        else:\n",
    "            w_decay = params['weight_decay']\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "        train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = train_fake(model, filename, method_wandb, optimizer, criterion, w_decay, parameters, train_loader, valid_loader, test_loader, testing=True, n_epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME']=\"Fake2C_GAT.ipynb\"\n",
    "method = 'GAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with patient 0\n",
      "done with patient 1\n",
      "done with patient 2\n",
      "done with patient 3\n",
      "done with patient 4\n",
      "done with patient 5\n",
      "done with patient 6\n",
      "done with patient 7\n",
      "done with patient 8\n",
      "done with patient 9\n",
      "done with patient 10\n",
      "done with patient 11\n",
      "done with patient 12\n",
      "done with patient 13\n",
      "done with patient 14\n",
      "done with patient 15\n",
      "done with patient 16\n",
      "done with patient 17\n",
      "done with patient 18\n",
      "done with patient 19\n",
      "done with patient 20\n",
      "done with patient 21\n",
      "done with patient 22\n",
      "done with patient 23\n",
      "done with patient 24\n",
      "done with patient 25\n",
      "done with patient 26\n",
      "done with patient 27\n",
      "done with patient 28\n",
      "done with patient 29\n",
      "done with patient 30\n",
      "done with patient 31\n",
      "done with patient 32\n",
      "done with patient 33\n",
      "done with patient 34\n",
      "done with patient 35\n",
      "done with patient 36\n",
      "done with patient 37\n",
      "done with patient 38\n",
      "done with patient 39\n",
      "done with patient 40\n",
      "done with patient 41\n",
      "done with patient 42\n",
      "done with patient 43\n",
      "done with patient 44\n",
      "done with patient 45\n",
      "done with patient 46\n",
      "done with patient 47\n",
      "done with patient 48\n",
      "done with patient 49\n",
      "done with patient 50\n",
      "done with patient 51\n",
      "done with patient 52\n",
      "done with patient 53\n",
      "done with patient 54\n",
      "done with patient 55\n",
      "done with patient 56\n",
      "done with patient 57\n",
      "done with patient 58\n",
      "done with patient 59\n",
      "done with patient 60\n",
      "done with patient 61\n",
      "done with patient 62\n",
      "done with patient 63\n",
      "done with patient 64\n",
      "done with patient 65\n",
      "done with patient 66\n",
      "done with patient 67\n",
      "done with patient 68\n",
      "done with patient 69\n",
      "done with patient 70\n",
      "done with patient 71\n",
      "done with patient 72\n",
      "done with patient 73\n",
      "done with patient 74\n",
      "done with patient 75\n",
      "done with patient 76\n",
      "done with patient 77\n",
      "done with patient 78\n",
      "done with patient 79\n",
      "done with patient 80\n",
      "done with patient 81\n",
      "done with patient 82\n",
      "done with patient 83\n",
      "done with patient 84\n",
      "done with patient 85\n",
      "done with patient 86\n",
      "done with patient 87\n",
      "done with patient 88\n",
      "done with patient 89\n",
      "done with patient 90\n",
      "done with patient 91\n",
      "done with patient 92\n",
      "done with patient 93\n",
      "done with patient 94\n",
      "done with patient 95\n",
      "done with patient 96\n",
      "done with patient 97\n",
      "done with patient 98\n",
      "done with patient 99\n",
      "done with patient 100\n",
      "done with patient 101\n",
      "done with patient 102\n",
      "done with patient 103\n",
      "done with patient 104\n",
      "done with patient 105\n",
      "done with patient 106\n",
      "done with patient 107\n",
      "done with patient 108\n",
      "done with patient 109\n",
      "done with patient 110\n",
      "done with patient 111\n",
      "done with patient 112\n",
      "done with patient 113\n",
      "done with patient 114\n",
      "done with patient 115\n",
      "done with patient 116\n",
      "done with patient 117\n",
      "done with patient 118\n",
      "done with patient 119\n",
      "done with patient 120\n",
      "done with patient 121\n",
      "done with patient 122\n",
      "done with patient 123\n",
      "done with patient 124\n",
      "done with patient 125\n",
      "done with patient 126\n",
      "done with patient 127\n",
      "done with patient 128\n",
      "done with patient 129\n",
      "done with patient 130\n",
      "done with patient 131\n",
      "done with patient 132\n",
      "done with patient 133\n",
      "done with patient 134\n",
      "done with patient 135\n",
      "done with patient 136\n",
      "done with patient 137\n",
      "done with patient 138\n",
      "done with patient 139\n",
      "done with patient 140\n",
      "done with patient 141\n",
      "done with patient 142\n",
      "done with patient 143\n",
      "done with patient 144\n",
      "done with patient 145\n",
      "done with patient 146\n",
      "done with patient 147\n",
      "done with patient 148\n",
      "done with patient 149\n",
      "done with patient 150\n",
      "done with patient 151\n",
      "done with patient 152\n",
      "done with patient 153\n",
      "done with patient 154\n",
      "done with patient 155\n",
      "done with patient 156\n",
      "done with patient 157\n",
      "done with patient 158\n",
      "done with patient 159\n",
      "done with patient 160\n",
      "done with patient 161\n",
      "done with patient 162\n",
      "done with patient 163\n",
      "done with patient 164\n",
      "done with patient 165\n",
      "done with patient 166\n",
      "done with patient 167\n",
      "done with patient 168\n",
      "done with patient 169\n",
      "done with patient 170\n",
      "done with patient 171\n",
      "done with patient 172\n",
      "done with patient 173\n",
      "done with patient 174\n",
      "done with patient 175\n",
      "done with patient 176\n",
      "done with patient 177\n",
      "done with patient 178\n",
      "done with patient 179\n",
      "done with patient 180\n",
      "done with patient 181\n",
      "done with patient 182\n",
      "done with patient 183\n",
      "done with patient 184\n",
      "done with patient 185\n",
      "done with patient 186\n",
      "done with patient 187\n",
      "done with patient 188\n",
      "done with patient 189\n",
      "done with patient 190\n",
      "done with patient 191\n",
      "done with patient 192\n",
      "done with patient 193\n",
      "done with patient 194\n",
      "done with patient 195\n",
      "done with patient 196\n",
      "done with patient 197\n",
      "done with patient 198\n",
      "done with patient 199\n",
      "done with patient 200\n",
      "done with patient 201\n",
      "done with patient 202\n",
      "done with patient 203\n",
      "done with patient 204\n",
      "done with patient 205\n",
      "done with patient 206\n",
      "done with patient 207\n",
      "done with patient 208\n",
      "done with patient 209\n",
      "done with patient 210\n",
      "done with patient 211\n",
      "done with patient 212\n",
      "done with patient 213\n",
      "done with patient 214\n",
      "done with patient 215\n",
      "done with patient 216\n",
      "done with patient 217\n",
      "done with patient 218\n",
      "done with patient 219\n",
      "done with patient 220\n",
      "done with patient 221\n",
      "done with patient 222\n",
      "done with patient 223\n",
      "done with patient 224\n",
      "done with patient 225\n",
      "done with patient 226\n",
      "done with patient 227\n",
      "done with patient 228\n",
      "done with patient 229\n",
      "done with patient 230\n",
      "done with patient 231\n",
      "done with patient 232\n",
      "done with patient 233\n",
      "done with patient 234\n",
      "done with patient 235\n",
      "done with patient 236\n",
      "done with patient 237\n",
      "done with patient 238\n",
      "done with patient 239\n",
      "done with patient 240\n",
      "done with patient 241\n",
      "done with patient 242\n",
      "done with patient 243\n",
      "done with patient 244\n",
      "done with patient 245\n",
      "done with patient 246\n",
      "done with patient 247\n",
      "done with patient 248\n",
      "done with patient 249\n",
      "done with patient 250\n",
      "done with patient 251\n",
      "done with patient 252\n",
      "done with patient 253\n",
      "done with patient 254\n",
      "done with patient 255\n",
      "done with patient 256\n",
      "done with patient 257\n",
      "done with patient 258\n",
      "done with patient 259\n",
      "done with patient 260\n",
      "done with patient 261\n",
      "done with patient 262\n",
      "done with patient 263\n",
      "done with patient 264\n",
      "done with patient 265\n",
      "done with patient 266\n",
      "done with patient 267\n",
      "done with patient 268\n",
      "done with patient 269\n",
      "done with patient 270\n",
      "done with patient 271\n",
      "done with patient 272\n",
      "done with patient 273\n",
      "done with patient 274\n",
      "done with patient 275\n",
      "done with patient 276\n",
      "done with patient 277\n",
      "done with patient 278\n",
      "done with patient 279\n",
      "done with patient 280\n",
      "done with patient 281\n",
      "done with patient 282\n",
      "done with patient 283\n",
      "done with patient 284\n",
      "done with patient 285\n",
      "done with patient 286\n",
      "done with patient 287\n",
      "done with patient 288\n",
      "done with patient 289\n",
      "done with patient 290\n",
      "done with patient 291\n",
      "done with patient 292\n",
      "done with patient 293\n",
      "done with patient 294\n",
      "done with patient 295\n",
      "done with patient 296\n",
      "done with patient 297\n",
      "done with patient 298\n",
      "done with patient 299\n",
      "done with patient 300\n",
      "done with patient 301\n",
      "done with patient 302\n",
      "done with patient 303\n",
      "done with patient 304\n",
      "done with patient 305\n",
      "done with patient 306\n",
      "done with patient 307\n",
      "done with patient 308\n",
      "done with patient 309\n",
      "done with patient 310\n",
      "done with patient 311\n",
      "done with patient 312\n",
      "done with patient 313\n",
      "done with patient 314\n",
      "done with patient 315\n",
      "done with patient 316\n",
      "done with patient 317\n",
      "done with patient 318\n",
      "done with patient 319\n",
      "done with patient 320\n",
      "done with patient 321\n",
      "done with patient 322\n",
      "done with patient 323\n",
      "done with patient 324\n",
      "done with patient 325\n",
      "done with patient 326\n",
      "done with patient 327\n",
      "done with patient 328\n",
      "done with patient 329\n",
      "done with patient 330\n",
      "done with patient 331\n",
      "done with patient 332\n",
      "done with patient 333\n",
      "done with patient 334\n",
      "done with patient 335\n",
      "done with patient 336\n",
      "done with patient 337\n",
      "done with patient 338\n",
      "done with patient 339\n",
      "done with patient 340\n",
      "done with patient 341\n",
      "done with patient 342\n",
      "done with patient 343\n",
      "done with patient 344\n",
      "done with patient 345\n",
      "done with patient 346\n",
      "done with patient 347\n",
      "done with patient 348\n",
      "done with patient 349\n",
      "done with patient 350\n",
      "done with patient 351\n",
      "done with patient 352\n",
      "done with patient 353\n",
      "done with patient 354\n",
      "done with patient 355\n",
      "done with patient 356\n",
      "done with patient 357\n",
      "done with patient 358\n",
      "done with patient 359\n",
      "done with patient 360\n",
      "done with patient 361\n",
      "done with patient 362\n",
      "done with patient 363\n",
      "done with patient 364\n",
      "done with patient 365\n",
      "done with patient 366\n",
      "done with patient 367\n",
      "done with patient 368\n",
      "done with patient 369\n",
      "done with patient 370\n",
      "done with patient 371\n",
      "done with patient 372\n",
      "done with patient 373\n",
      "done with patient 374\n",
      "done with patient 375\n",
      "done with patient 376\n",
      "done with patient 377\n",
      "done with patient 378\n",
      "done with patient 379\n",
      "done with patient 380\n",
      "done with patient 381\n",
      "done with patient 382\n",
      "done with patient 383\n",
      "done with patient 384\n",
      "done with patient 385\n",
      "done with patient 386\n",
      "done with patient 387\n",
      "done with patient 388\n",
      "done with patient 389\n",
      "done with patient 390\n",
      "done with patient 391\n",
      "done with patient 392\n",
      "done with patient 393\n",
      "done with patient 394\n",
      "done with patient 395\n",
      "done with patient 396\n",
      "done with patient 397\n",
      "done with patient 398\n",
      "done with patient 399\n",
      "done with patient 400\n",
      "done with patient 401\n",
      "done with patient 402\n",
      "done with patient 403\n",
      "done with patient 404\n",
      "done with patient 405\n",
      "done with patient 406\n",
      "done with patient 407\n",
      "done with patient 408\n",
      "done with patient 409\n",
      "done with patient 410\n",
      "done with patient 411\n",
      "done with patient 412\n",
      "done with patient 413\n",
      "done with patient 414\n",
      "done with patient 415\n",
      "done with patient 416\n",
      "done with patient 417\n",
      "done with patient 418\n",
      "done with patient 419\n",
      "done with patient 420\n",
      "done with patient 421\n",
      "done with patient 422\n",
      "done with patient 423\n",
      "done with patient 424\n",
      "done with patient 425\n",
      "done with patient 426\n",
      "done with patient 427\n",
      "done with patient 428\n",
      "done with patient 429\n",
      "done with patient 430\n",
      "done with patient 431\n",
      "done with patient 432\n",
      "done with patient 433\n",
      "done with patient 434\n",
      "done with patient 435\n",
      "done with patient 436\n",
      "done with patient 437\n",
      "done with patient 438\n",
      "done with patient 439\n",
      "done with patient 440\n",
      "done with patient 441\n",
      "done with patient 442\n",
      "done with patient 443\n",
      "done with patient 444\n",
      "done with patient 445\n",
      "done with patient 446\n",
      "done with patient 447\n",
      "done with patient 448\n",
      "done with patient 449\n",
      "done with patient 450\n",
      "done with patient 451\n",
      "done with patient 452\n",
      "done with patient 453\n",
      "done with patient 454\n",
      "done with patient 455\n",
      "done with patient 456\n",
      "done with patient 457\n",
      "done with patient 458\n",
      "done with patient 459\n",
      "done with patient 460\n",
      "done with patient 461\n",
      "done with patient 462\n",
      "done with patient 463\n",
      "done with patient 464\n",
      "done with patient 465\n",
      "done with patient 466\n",
      "done with patient 467\n",
      "done with patient 468\n",
      "done with patient 469\n",
      "done with patient 470\n",
      "done with patient 471\n",
      "done with patient 472\n",
      "done with patient 473\n",
      "done with patient 474\n",
      "done with patient 475\n",
      "done with patient 476\n",
      "done with patient 477\n",
      "done with patient 478\n",
      "done with patient 479\n",
      "done with patient 480\n",
      "done with patient 481\n",
      "done with patient 482\n",
      "done with patient 483\n",
      "done with patient 484\n",
      "done with patient 485\n",
      "done with patient 486\n",
      "done with patient 487\n",
      "done with patient 488\n",
      "done with patient 489\n",
      "done with patient 490\n",
      "done with patient 491\n",
      "done with patient 492\n",
      "done with patient 493\n",
      "done with patient 494\n",
      "done with patient 495\n",
      "done with patient 496\n",
      "done with patient 497\n",
      "done with patient 498\n",
      "done with patient 499\n",
      "done with patient 500\n",
      "done with patient 501\n",
      "done with patient 502\n",
      "done with patient 503\n",
      "done with patient 504\n",
      "done with patient 505\n",
      "done with patient 506\n",
      "done with patient 507\n",
      "done with patient 508\n",
      "done with patient 509\n",
      "done with patient 510\n",
      "done with patient 511\n",
      "done with patient 512\n",
      "done with patient 513\n",
      "done with patient 514\n",
      "done with patient 515\n",
      "done with patient 516\n",
      "done with patient 517\n",
      "done with patient 518\n",
      "done with patient 519\n",
      "done with patient 520\n",
      "done with patient 521\n",
      "done with patient 522\n",
      "done with patient 523\n",
      "done with patient 524\n",
      "done with patient 525\n",
      "done with patient 526\n",
      "done with patient 527\n",
      "done with patient 528\n",
      "done with patient 529\n",
      "done with patient 530\n",
      "done with patient 531\n",
      "done with patient 532\n",
      "done with patient 533\n",
      "done with patient 534\n",
      "done with patient 535\n",
      "done with patient 536\n",
      "done with patient 537\n",
      "done with patient 538\n",
      "done with patient 539\n",
      "done with patient 540\n",
      "done with patient 541\n",
      "done with patient 542\n",
      "done with patient 543\n",
      "done with patient 544\n",
      "done with patient 545\n",
      "done with patient 546\n",
      "done with patient 547\n",
      "done with patient 548\n",
      "done with patient 549\n",
      "done with patient 550\n",
      "done with patient 551\n",
      "done with patient 552\n",
      "done with patient 553\n",
      "done with patient 554\n",
      "done with patient 555\n",
      "done with patient 556\n",
      "done with patient 557\n",
      "done with patient 558\n",
      "done with patient 559\n",
      "done with patient 560\n",
      "done with patient 561\n",
      "done with patient 562\n",
      "done with patient 563\n",
      "done with patient 564\n",
      "done with patient 565\n",
      "done with patient 566\n",
      "done with patient 567\n",
      "done with patient 568\n",
      "done with patient 569\n",
      "done with patient 570\n",
      "done with patient 571\n",
      "done with patient 572\n",
      "done with patient 573\n",
      "done with patient 574\n",
      "done with patient 575\n",
      "done with patient 576\n",
      "done with patient 577\n",
      "done with patient 578\n",
      "done with patient 579\n",
      "done with patient 580\n",
      "done with patient 581\n",
      "done with patient 582\n",
      "done with patient 583\n",
      "done with patient 584\n",
      "done with patient 585\n",
      "done with patient 586\n",
      "done with patient 587\n",
      "done with patient 588\n",
      "done with patient 589\n",
      "done with patient 590\n",
      "done with patient 591\n",
      "done with patient 592\n",
      "done with patient 593\n",
      "done with patient 594\n",
      "done with patient 595\n",
      "done with patient 596\n",
      "done with patient 597\n",
      "done with patient 598\n",
      "done with patient 599\n",
      "done with patient 600\n",
      "done with patient 601\n",
      "done with patient 602\n",
      "done with patient 603\n",
      "done with patient 604\n",
      "done with patient 605\n",
      "done with patient 606\n",
      "done with patient 607\n",
      "done with patient 608\n",
      "done with patient 609\n",
      "done with patient 610\n",
      "done with patient 611\n",
      "done with patient 612\n",
      "done with patient 613\n",
      "done with patient 614\n",
      "done with patient 615\n",
      "done with patient 616\n",
      "done with patient 617\n",
      "done with patient 618\n",
      "done with patient 619\n",
      "done with patient 620\n",
      "done with patient 621\n",
      "done with patient 622\n",
      "done with patient 623\n",
      "done with patient 624\n",
      "done with patient 625\n",
      "done with patient 626\n",
      "done with patient 627\n",
      "done with patient 628\n",
      "done with patient 629\n",
      "done with patient 630\n",
      "done with patient 631\n",
      "done with patient 632\n",
      "done with patient 633\n",
      "done with patient 634\n",
      "done with patient 635\n",
      "done with patient 636\n",
      "done with patient 637\n",
      "done with patient 638\n",
      "done with patient 639\n",
      "done with patient 640\n",
      "done with patient 641\n",
      "done with patient 642\n",
      "done with patient 643\n",
      "done with patient 644\n",
      "done with patient 645\n",
      "done with patient 646\n",
      "done with patient 647\n",
      "done with patient 648\n",
      "done with patient 649\n",
      "done with patient 650\n",
      "done with patient 651\n",
      "done with patient 652\n",
      "done with patient 653\n",
      "done with patient 654\n",
      "done with patient 655\n",
      "done with patient 656\n",
      "done with patient 657\n",
      "done with patient 658\n",
      "done with patient 659\n",
      "done with patient 660\n",
      "done with patient 661\n",
      "done with patient 662\n",
      "done with patient 663\n",
      "done with patient 664\n",
      "done with patient 665\n",
      "done with patient 666\n",
      "done with patient 667\n",
      "done with patient 668\n",
      "done with patient 669\n",
      "done with patient 670\n",
      "done with patient 671\n",
      "done with patient 672\n",
      "done with patient 673\n",
      "done with patient 674\n",
      "done with patient 675\n",
      "done with patient 676\n",
      "done with patient 677\n",
      "done with patient 678\n",
      "done with patient 679\n",
      "done with patient 680\n",
      "done with patient 681\n",
      "done with patient 682\n",
      "done with patient 683\n",
      "done with patient 684\n",
      "done with patient 685\n",
      "done with patient 686\n",
      "done with patient 687\n",
      "done with patient 688\n",
      "done with patient 689\n",
      "done with patient 690\n",
      "done with patient 691\n",
      "done with patient 692\n",
      "done with patient 693\n",
      "done with patient 694\n",
      "done with patient 695\n",
      "done with patient 696\n",
      "done with patient 697\n",
      "done with patient 698\n",
      "done with patient 699\n",
      "done with patient 700\n",
      "done with patient 701\n",
      "done with patient 702\n",
      "done with patient 703\n",
      "done with patient 704\n",
      "done with patient 705\n",
      "done with patient 706\n",
      "done with patient 707\n",
      "done with patient 708\n",
      "done with patient 709\n",
      "done with patient 710\n",
      "done with patient 711\n",
      "done with patient 712\n",
      "done with patient 713\n",
      "done with patient 714\n",
      "done with patient 715\n",
      "done with patient 716\n",
      "done with patient 717\n",
      "done with patient 718\n",
      "done with patient 719\n",
      "done with patient 720\n",
      "done with patient 721\n",
      "done with patient 722\n",
      "done with patient 723\n",
      "done with patient 724\n",
      "done with patient 725\n",
      "done with patient 726\n",
      "done with patient 727\n",
      "done with patient 728\n",
      "done with patient 729\n",
      "done with patient 730\n",
      "done with patient 731\n",
      "done with patient 732\n",
      "done with patient 733\n",
      "done with patient 734\n",
      "done with patient 735\n",
      "done with patient 736\n",
      "done with patient 737\n",
      "done with patient 738\n",
      "done with patient 739\n",
      "done with patient 740\n",
      "done with patient 741\n",
      "done with patient 742\n",
      "done with patient 743\n",
      "done with patient 744\n",
      "done with patient 745\n",
      "done with patient 746\n",
      "done with patient 747\n",
      "done with patient 748\n",
      "done with patient 749\n",
      "done with patient 750\n",
      "done with patient 751\n",
      "done with patient 752\n",
      "done with patient 753\n",
      "done with patient 754\n",
      "done with patient 755\n",
      "done with patient 756\n",
      "done with patient 757\n",
      "done with patient 758\n",
      "done with patient 759\n",
      "done with patient 760\n",
      "done with patient 761\n",
      "done with patient 762\n",
      "done with patient 763\n",
      "done with patient 764\n",
      "done with patient 765\n",
      "done with patient 766\n",
      "done with patient 767\n",
      "done with patient 768\n",
      "done with patient 769\n",
      "done with patient 770\n",
      "done with patient 771\n",
      "done with patient 772\n",
      "done with patient 773\n",
      "done with patient 774\n",
      "done with patient 775\n",
      "done with patient 776\n",
      "done with patient 777\n",
      "done with patient 778\n",
      "done with patient 779\n",
      "done with patient 780\n",
      "done with patient 781\n",
      "done with patient 782\n",
      "done with patient 783\n",
      "done with patient 784\n",
      "done with patient 785\n",
      "done with patient 786\n",
      "done with patient 787\n",
      "done with patient 788\n",
      "done with patient 789\n",
      "done with patient 790\n",
      "done with patient 791\n",
      "done with patient 792\n",
      "done with patient 793\n",
      "done with patient 794\n",
      "done with patient 795\n",
      "done with patient 796\n",
      "done with patient 797\n",
      "done with patient 798\n",
      "done with patient 799\n",
      "done with patient 800\n",
      "done with patient 801\n",
      "done with patient 802\n",
      "done with patient 803\n",
      "done with patient 804\n",
      "done with patient 805\n",
      "done with patient 806\n",
      "done with patient 807\n",
      "done with patient 808\n",
      "done with patient 809\n",
      "done with patient 810\n",
      "done with patient 811\n",
      "done with patient 812\n",
      "done with patient 813\n",
      "done with patient 814\n",
      "done with patient 815\n",
      "done with patient 816\n",
      "done with patient 817\n",
      "done with patient 818\n",
      "done with patient 819\n",
      "done with patient 820\n",
      "done with patient 821\n",
      "done with patient 822\n",
      "done with patient 823\n",
      "done with patient 824\n",
      "done with patient 825\n",
      "done with patient 826\n",
      "done with patient 827\n",
      "done with patient 828\n",
      "done with patient 829\n",
      "done with patient 830\n",
      "done with patient 831\n",
      "done with patient 832\n",
      "done with patient 833\n",
      "done with patient 834\n",
      "done with patient 835\n",
      "done with patient 836\n",
      "done with patient 837\n",
      "done with patient 838\n",
      "done with patient 839\n",
      "done with patient 840\n",
      "done with patient 841\n",
      "done with patient 842\n",
      "done with patient 843\n",
      "done with patient 844\n",
      "done with patient 845\n",
      "done with patient 846\n",
      "done with patient 847\n",
      "done with patient 848\n",
      "done with patient 849\n",
      "done with patient 850\n",
      "done with patient 851\n",
      "done with patient 852\n",
      "done with patient 853\n",
      "done with patient 854\n",
      "done with patient 855\n",
      "done with patient 856\n",
      "done with patient 857\n",
      "done with patient 858\n",
      "done with patient 859\n",
      "done with patient 860\n",
      "done with patient 861\n",
      "done with patient 862\n",
      "done with patient 863\n",
      "done with patient 864\n",
      "done with patient 865\n",
      "done with patient 866\n",
      "done with patient 867\n",
      "done with patient 868\n",
      "done with patient 869\n",
      "done with patient 870\n",
      "done with patient 871\n",
      "done with patient 872\n",
      "done with patient 873\n",
      "done with patient 874\n",
      "done with patient 875\n",
      "done with patient 876\n",
      "done with patient 877\n",
      "done with patient 878\n",
      "done with patient 879\n",
      "done with patient 880\n",
      "done with patient 881\n",
      "done with patient 882\n",
      "done with patient 883\n",
      "done with patient 884\n",
      "done with patient 885\n",
      "done with patient 886\n",
      "done with patient 887\n",
      "done with patient 888\n",
      "done with patient 889\n",
      "done with patient 890\n",
      "done with patient 891\n",
      "done with patient 892\n",
      "done with patient 893\n",
      "done with patient 894\n",
      "done with patient 895\n",
      "done with patient 896\n",
      "done with patient 897\n",
      "done with patient 898\n",
      "done with patient 899\n",
      "done with patient 900\n",
      "done with patient 901\n",
      "done with patient 902\n",
      "done with patient 903\n",
      "done with patient 904\n",
      "done with patient 905\n",
      "done with patient 906\n",
      "done with patient 907\n",
      "done with patient 908\n",
      "done with patient 909\n",
      "done with patient 910\n",
      "done with patient 911\n",
      "done with patient 912\n",
      "done with patient 913\n",
      "done with patient 914\n",
      "done with patient 915\n",
      "done with patient 916\n",
      "done with patient 917\n",
      "done with patient 918\n",
      "done with patient 919\n",
      "done with patient 920\n",
      "done with patient 921\n",
      "done with patient 922\n",
      "done with patient 923\n",
      "done with patient 924\n",
      "done with patient 925\n",
      "done with patient 926\n",
      "done with patient 927\n",
      "done with patient 928\n",
      "done with patient 929\n",
      "done with patient 930\n",
      "done with patient 931\n",
      "done with patient 932\n",
      "done with patient 933\n",
      "done with patient 934\n",
      "done with patient 935\n",
      "done with patient 936\n",
      "done with patient 937\n",
      "done with patient 938\n",
      "done with patient 939\n",
      "done with patient 940\n",
      "done with patient 941\n",
      "done with patient 942\n",
      "done with patient 943\n",
      "done with patient 944\n",
      "done with patient 945\n",
      "done with patient 946\n",
      "done with patient 947\n",
      "done with patient 948\n",
      "done with patient 949\n",
      "done with patient 950\n",
      "done with patient 951\n",
      "done with patient 952\n",
      "done with patient 953\n",
      "done with patient 954\n",
      "done with patient 955\n",
      "done with patient 956\n",
      "done with patient 957\n",
      "done with patient 958\n",
      "done with patient 959\n",
      "done with patient 960\n",
      "done with patient 961\n",
      "done with patient 962\n",
      "done with patient 963\n",
      "done with patient 964\n",
      "done with patient 965\n",
      "done with patient 966\n",
      "done with patient 967\n",
      "done with patient 968\n",
      "done with patient 969\n",
      "done with patient 970\n",
      "done with patient 971\n",
      "done with patient 972\n",
      "done with patient 973\n",
      "done with patient 974\n",
      "done with patient 975\n",
      "done with patient 976\n",
      "done with patient 977\n",
      "done with patient 978\n",
      "done with patient 979\n",
      "done with patient 980\n",
      "done with patient 981\n",
      "done with patient 982\n",
      "done with patient 983\n",
      "done with patient 984\n",
      "done with patient 985\n",
      "done with patient 986\n",
      "done with patient 987\n",
      "done with patient 988\n",
      "done with patient 989\n",
      "done with patient 990\n",
      "done with patient 991\n",
      "done with patient 992\n",
      "done with patient 993\n",
      "done with patient 994\n",
      "done with patient 995\n",
      "done with patient 996\n",
      "done with patient 997\n",
      "done with patient 998\n",
      "done with patient 999\n",
      "\n",
      "Dataset: Fake2C_Raw_to_graph(1000):\n",
      "====================\n",
      "Number of graphs: 1000\n",
      "Weighted: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Fake2C_Raw_to_graph' object has no attribute 'threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFake2C_Raw_to_graph/model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Fake2C_Raw_to_graph(root, corr_matrices, labels)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_features_and_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bachelor-Thesis-Alzheimers-HGNN/functions.py:142\u001b[0m, in \u001b[0;36mdataset_features_and_stats\u001b[0;34m(dataset, diagnostic_label)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of graphs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeighted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThreshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrelation Method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:318\u001b[0m, in \u001b[0;36mInMemoryDataset.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    315\u001b[0m         data_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()]\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(data_list)[key]\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Fake2C_Raw_to_graph' object has no attribute 'threshold'"
     ]
    }
   ],
   "source": [
    "root = f'Fake2C_Raw_to_graph/model{method}'\n",
    "dataset = Fake2C_Raw_to_graph(root, corr_matrices, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 700\n",
      "Number of validation graphs: 100\n",
      "Number of test graphs: 200\n",
      "Number of classes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qlsvqemi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sky-4</strong> at: <a href='https://wandb.ai/alzheimers-cl/Fake_Alzheimers/runs/qlsvqemi' target=\"_blank\">https://wandb.ai/alzheimers-cl/Fake_Alzheimers/runs/qlsvqemi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240315_012244-qlsvqemi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qlsvqemi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mathilde/Documents/Bachelor-Thesis-Alzheimers-HGNN/wandb/run-20240315_013732-pirtwl14</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alzheimers-cl/Fake_Alzheimers/runs/pirtwl14' target=\"_blank\">frosty-sun-5</a></strong> to <a href='https://wandb.ai/alzheimers-cl/Fake_Alzheimers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alzheimers-cl/Fake_Alzheimers' target=\"_blank\">https://wandb.ai/alzheimers-cl/Fake_Alzheimers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alzheimers-cl/Fake_Alzheimers/runs/pirtwl14' target=\"_blank\">https://wandb.ai/alzheimers-cl/Fake_Alzheimers/runs/pirtwl14</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m     w_decay \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     52\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mparameters[\u001b[38;5;241m0\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mw_decay)\n\u001b[0;32m---> 53\u001b[0m train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_wandb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 32\u001b[0m, in \u001b[0;36mtrain_fake\u001b[0;34m(model, filename, optimizer, criterion, w_decay, train_loader, valid_loader, parameters, method, test_loader, testing, n_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m testing:\n\u001b[0;32m---> 32\u001b[0m         train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_accuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_valid_accuracy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Valid Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_valid_accuracy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_accuracy})\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Bachelor-Thesis-Alzheimers-HGNN/functions.py:661\u001b[0m, in \u001b[0;36mepochs_training\u001b[0;34m(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m testing:\n\u001b[1;32m    660\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 661\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.0001],\n",
    "    'hidden_channels': [64],\n",
    "    'num_layers': [1],\n",
    "    'dropout_rate': [0.0],\n",
    "    'weight_decay': [0.0001],\n",
    "    'heads': [3, 4]\n",
    "}\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "param_combinations = ParameterGrid(param_grid)\n",
    "n_epochs = 800\n",
    "in_channels = 5\n",
    "nbr_classes = 2\n",
    "stratify = True\n",
    "method_wandb = 'GAT'\n",
    "\n",
    "# Creating the train, validation and test sets\n",
    "train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset, stratify)\n",
    "\n",
    "# Train using each combination\n",
    "for params in param_combinations:\n",
    "    filename = f'Fake2C_Models/GAT/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}_heads{params[\"heads\"]}_wdecay{params[\"weight_decay\"]}.png'\n",
    "    if os.path.exists(filename):\n",
    "        pass\n",
    "    else:\n",
    "        parameters = [params['learning_rate'], params['hidden_channels'], params['num_layers'], params['dropout_rate'], params['heads']]\n",
    "        model = m.GAT(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], heads=parameters[4], nbr_classes=nbr_classes)\n",
    "        if stratify:\n",
    "            diag_lab = [0 , 1]\n",
    "            class_freq = []\n",
    "            for i in diag_lab:\n",
    "                class_freq.append(np.count_nonzero(torch.Tensor(y_train) == i))\n",
    "            class_freq = torch.FloatTensor(class_freq)\n",
    "            class_weights = 1 / class_freq\n",
    "            class_weights /= class_weights.sum()\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = torch.nn.CrossEntropyLoss() \n",
    "        if 'weight_decay' not in params.keys():\n",
    "            w_decay = 0\n",
    "        else:\n",
    "            w_decay = params['weight_decay']\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "        train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = train_fake(model, filename, method_wandb, optimizer, criterion, w_decay, train_loader, valid_loader, parameters, test_loader, testing=True, n_epochs=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypergraph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fake2C_Raw_to_Hypergraph(InMemoryDataset):\n",
    "    def __init__(self, root, hg_data_path, labels, transform=None, pre_transform=None):\n",
    "        self.method = method\n",
    "        self.weight = weight\n",
    "        self.threshold = threshold\n",
    "        self.age = age\n",
    "        self.sex = sex\n",
    "        self.hg_data_path = hg_data_path\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    # This function is used to process the raw data into a format suitable for GNNs, by constructing graphs out of the connectivity matrices.\n",
    "    def process(self):\n",
    "        # Loading the prebuilt hypergraphs and the correlation matrices\n",
    "        hg_dict_list = f.load_hg_dict(self.hg_data_path)\n",
    "\n",
    "        graphs=[]\n",
    "        for patient_idx, patient_matrix in enumerate(corr_matrices):\n",
    "            # Create a NetworkX graph from the hypergraph matrix\n",
    "            patient_hg = hg_dict_list[patient_idx]\n",
    "            hypergraph = hnx.Hypergraph(patient_hg)\n",
    "\n",
    "            # Adding the matrix profiling features to the feature array\n",
    "            path = f'ADNI_full/matrix_profiles/matrix_profile_pearson/{patient_matrix}'\n",
    "            if patient_matrix.endswith('.DS_Store'):\n",
    "                continue  # Skip hidden system files like .DS_Store\n",
    "            with open(path, \"rb\") as fl:\n",
    "                patient_dict = pkl.load(fl)\n",
    "            # combine dimensions\n",
    "            features = np.array(patient_dict['mp']).reshape(len(patient_dict['mp']),-1)\n",
    "            features = features.astype(np.float32)\n",
    "\n",
    "            # Concatenate the degree, participation coefficient, betweenness centrality, local efficiency, and ratio of local to global efficiency arrays to form a single feature vector\n",
    "            x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "            # Create a Pytorch Geometric Data object\n",
    "            edge_index0 = []\n",
    "            edge_index1 = []\n",
    "            i = 0\n",
    "            for hyperedge, nodes in hypergraph.incidence_dict.items():\n",
    "                edge_index0 = np.concatenate((edge_index0, nodes), axis=0)\n",
    "                for j in range(len(nodes)):\n",
    "                    edge_index1.append(i)\n",
    "                i += 1\n",
    "            edge_index = np.stack([[int(x) for x in edge_index0], edge_index1], axis=0)\n",
    "            y = torch.tensor(float(diagnostic_label[patient_idx]))\n",
    "            hg_data = Data(x=x, edge_index=torch.tensor(edge_index, dtype=torch.long), y=y)\n",
    "            graphs.append(hg_data)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to save the fake hypergraphs\n",
    "def save_fake_hypergraph(hg_dict, directory, method, id):\n",
    "    dir = f'{directory}/{method}'\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    with open(f'{dir}/{method}/{id}.pkl', 'wb') as f:\n",
    "        pkl.dump(hg_dict, f)\n",
    "    return\n",
    "\n",
    "def save_all_fake_hypergraphs(method_list, corr_matrices, labels):\n",
    "    for i, patient_matrix in enumerate(corr_matrices):\n",
    "        print(f'Processing patient {i}')\n",
    "        for method in method_list:\n",
    "            if method == 'maximal_clique':\n",
    "                root = f'Fake2C_Raw_to_graph/model{method}'\n",
    "                dataset = Fake2C_Raw_to_graph(root, corr_matrices, labels)\n",
    "                graph = f.r2g_to_nx(dataset[i])\n",
    "                _, hg_dict = m.graph_to_hypergraph_max_cliques(graph)\n",
    "            elif method == 'knn':\n",
    "                k_neighbors = 3\n",
    "                _, hg_dict = m.generate_hypergraph_from_knn(patient_matrix, k_neighbors)\n",
    "            save_fake_hypergraph(hg_dict, 'Fake_hypergraphs', method, i)\n",
    "            print(f'Patient {i} processed and saved for the {method}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME']=\"Fake2C_HGKNN.ipynb\"\n",
    "method = 'HGKNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the graphs\n",
    "hg_data_path = f'Fake_hypergraphs/{method}/'\n",
    "root = f'Fake2C_Raw_to_hypergraph/model_{method}'\n",
    "dataset = Fake2C_Raw_to_Hypergraph(root, hg_data_path, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HGConv with KNN method\n",
    "\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.001, 0.0001],\n",
    "#     'hidden_channels': [128, 64],\n",
    "#     'num_layers': [3, 2, 1],\n",
    "#     'dropout_rate': [0.2, 0.1, 0.0],\n",
    "#     'weight_decay': [0.001, 0.0001]\n",
    "# }\n",
    "param_grid = {\n",
    "    'learning_rate': [0.0001, 0.001],\n",
    "    'hidden_channels': [64, 128],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'dropout_rate': [0.0, 0.1, 0.2],\n",
    "    'weight_decay': [0.0001, 0.001]\n",
    "}\n",
    "\n",
    "# Creating the train, validation and test sets\n",
    "stratify = True\n",
    "train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset, stratify)\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "param_combinations = ParameterGrid(param_grid)\n",
    "n_epochs = 80\n",
    "in_channels = 5\n",
    "nbr_classes = 2\n",
    "method_wandb = 'HGKNN'\n",
    "\n",
    "# Train using each combination\n",
    "for params in param_combinations:\n",
    "    filename = f'Fake2C_Models/HGKNN/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}_wdecay{params[\"weight_decay\"]}_w{weight}.png'\n",
    "    if os.path.exists(filename):\n",
    "        pass\n",
    "    else:\n",
    "        parameters = [params['learning_rate'], params['hidden_channels'], params['num_layers'], params['dropout_rate']]\n",
    "        model = m.HGConv(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "        if stratify:\n",
    "            diag_lab = [0 , 1]\n",
    "            class_freq = []\n",
    "            for i in diag_lab:\n",
    "                class_freq.append(np.count_nonzero(torch.Tensor(y_train) == i))\n",
    "            class_freq = torch.FloatTensor(class_freq)\n",
    "            class_weights = 1 / class_freq\n",
    "            class_weights /= class_weights.sum()\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = torch.nn.CrossEntropyLoss() \n",
    "        if 'weight_decay' not in params.keys():\n",
    "            w_decay = 0\n",
    "        else:\n",
    "            w_decay = params['weight_decay']\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "        train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = train_fake(model, filename, method_wandb, optimizer, criterion, w_decay, parameters, train_loader, valid_loader, test_loader, testing=True, n_epochs=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HGConv with maximal clique method\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME']=\"Fake2C_HGMC.ipynb\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alzheimers-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
