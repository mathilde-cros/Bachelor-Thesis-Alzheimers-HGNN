{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "tJKHEbXIlreI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJKHEbXIlreI",
        "outputId": "3b3615bb-02ba-4bcf-8754-7d04f6e34d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bachelor_Thesis_Alzheimers_HGNN'...\n",
            "remote: Enumerating objects: 20191, done.\u001b[K\n",
            "remote: Counting objects: 100% (2243/2243), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2123/2123), done.\u001b[K\n",
            "remote: Total 20191 (delta 98), reused 2229 (delta 88), pack-reused 17948\u001b[K\n",
            "Receiving objects: 100% (20191/20191), 5.16 GiB | 15.98 MiB/s, done.\n",
            "Resolving deltas: 100% (3589/3589), done.\n",
            "Updating files: 100% (4926/4926), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mathilde-cros/Bachelor_Thesis_Alzheimers_HGNN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "C-BIstWpl480",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-BIstWpl480",
        "outputId": "d8ebb4b5-ee87-4c8b-87cc-1e4bd9b266be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch_scatter\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=507268 sha256=50bf4d8559613ff7df29a93827148770cd85cdb6a1a9d085badd69d277287e99\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch_scatter\n",
            "Installing collected packages: torch_scatter\n",
            "Successfully installed torch_scatter-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install torch_scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1DfCJfT6n80R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DfCJfT6n80R",
        "outputId": "783b8d4f-a9c4-49b4-8788-007b1266169c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.3.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.3.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cf49622b",
      "metadata": {
        "id": "cf49622b"
      },
      "outputs": [],
      "source": [
        "## FUTURE WORK (NOT PART OF THESIS)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch_geometric\n",
        "import torch_scatter\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_scatter import scatter_add, scatter,  scatter_mean\n",
        "from torch_geometric.typing import Adj, Size, OptTensor\n",
        "from typing import Optional\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import pdb\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.utils import add_self_loops, degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aUDbK2_poGJH",
      "metadata": {
        "id": "aUDbK2_poGJH"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_NOTEBOOK_NAME']=\"AllDeepSets.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "95e70cf4",
      "metadata": {
        "id": "95e70cf4"
      },
      "outputs": [],
      "source": [
        "def glorot(tensor):\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
        "        tensor.data.uniform_(-stdv, stdv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "875d4cd1",
      "metadata": {
        "id": "875d4cd1"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout=.5, Normalization='bn', InputNorm=False):\n",
        "        super(MLP, self).__init__()\n",
        "        self.lins = nn.ModuleList()\n",
        "        self.normalizations = nn.ModuleList()\n",
        "        self.InputNorm = InputNorm\n",
        "\n",
        "        assert Normalization in ['bn', 'ln', 'None']\n",
        "        if Normalization == 'bn':\n",
        "            if num_layers == 1:\n",
        "                # just linear layer i.e. logistic regression\n",
        "                if InputNorm:\n",
        "                    self.normalizations.append(nn.BatchNorm1d(in_channels))\n",
        "                else:\n",
        "                    self.normalizations.append(nn.Identity())\n",
        "                self.lins.append(nn.Linear(in_channels, out_channels))\n",
        "            else:\n",
        "                if InputNorm:\n",
        "                    self.normalizations.append(nn.BatchNorm1d(in_channels))\n",
        "                else:\n",
        "                    self.normalizations.append(nn.Identity())\n",
        "                self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
        "                self.normalizations.append(nn.BatchNorm1d(hidden_channels))\n",
        "                for _ in range(num_layers - 2):\n",
        "                    self.lins.append(\n",
        "                        nn.Linear(hidden_channels, hidden_channels))\n",
        "                    self.normalizations.append(nn.BatchNorm1d(hidden_channels))\n",
        "                self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
        "        elif Normalization == 'ln':\n",
        "            if num_layers == 1:\n",
        "                # just linear layer i.e. logistic regression\n",
        "                if InputNorm:\n",
        "                    self.normalizations.append(nn.LayerNorm(in_channels))\n",
        "                else:\n",
        "                    self.normalizations.append(nn.Identity())\n",
        "                self.lins.append(nn.Linear(in_channels, out_channels))\n",
        "            else:\n",
        "                if InputNorm:\n",
        "                    self.normalizations.append(nn.LayerNorm(in_channels))\n",
        "                else:\n",
        "                    self.normalizations.append(nn.Identity())\n",
        "                self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
        "                self.normalizations.append(nn.LayerNorm(hidden_channels))\n",
        "                for _ in range(num_layers - 2):\n",
        "                    self.lins.append(\n",
        "                        nn.Linear(hidden_channels, hidden_channels))\n",
        "                    self.normalizations.append(nn.LayerNorm(hidden_channels))\n",
        "                self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
        "        else:\n",
        "            if num_layers == 1:\n",
        "                # just linear layer i.e. logistic regression\n",
        "                self.normalizations.append(nn.Identity())\n",
        "                self.lins.append(nn.Linear(in_channels, out_channels))\n",
        "            else:\n",
        "                self.normalizations.append(nn.Identity())\n",
        "                self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
        "                self.normalizations.append(nn.Identity())\n",
        "                for _ in range(num_layers - 2):\n",
        "                    self.lins.append(\n",
        "                        nn.Linear(hidden_channels, hidden_channels))\n",
        "                    self.normalizations.append(nn.Identity())\n",
        "                self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "        for normalization in self.normalizations:\n",
        "            if normalization.__class__.__name__ != 'Identity':\n",
        "                normalization.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.normalizations[0](x)\n",
        "        for i, lin in enumerate(self.lins[:-1]):\n",
        "            x = lin(x)\n",
        "            x = F.relu(x, inplace=True)\n",
        "            x = self.normalizations[i+1](x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PMA(MessagePassing):\n",
        "    \"\"\"\n",
        "        PMA part:\n",
        "        Note that in original PMA, we need to compute the inner product of the seed and neighbor nodes.\n",
        "        i.e. e_ij = a(Wh_i,Wh_j), where a should be the inner product, h_i is the seed and h_j are neightbor nodes.\n",
        "        In GAT, a(x,y) = a^T[x||y]. We use the same logic.\n",
        "    \"\"\"\n",
        "    _alpha: OptTensor\n",
        "\n",
        "    def __init__(self, in_channels, hid_dim,\n",
        "                 out_channels, num_layers, heads=1, concat=True,\n",
        "                 negative_slope=0.2, dropout=0.0, bias=False, **kwargs):\n",
        "        #         kwargs.setdefault('aggr', 'add')\n",
        "        super(PMA, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden = hid_dim // heads\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.concat = concat\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = 0.\n",
        "        self.aggr = 'add'\n",
        "#         self.input_seed = input_seed\n",
        "\n",
        "#         This is the encoder part. Where we use 1 layer NN (Theta*x_i in the GATConv description)\n",
        "#         Now, no seed as input. Directly learn the importance weights alpha_ij.\n",
        "#         self.lin_O = Linear(heads*self.hidden, self.hidden) # For heads combining\n",
        "        # For neighbor nodes (source side, key)\n",
        "        self.lin_K = Linear(in_channels, self.heads*self.hidden)\n",
        "        # For neighbor nodes (source side, value)\n",
        "        self.lin_V = Linear(in_channels, self.heads*self.hidden)\n",
        "        self.att_r = Parameter(torch.Tensor(\n",
        "            1, heads, self.hidden))  # Seed vector\n",
        "        self.rFF = MLP(in_channels=self.heads*self.hidden,\n",
        "                       hidden_channels=self.heads*self.hidden,\n",
        "                       out_channels=out_channels,\n",
        "                       num_layers=num_layers,\n",
        "                       dropout=.0, Normalization='None',)\n",
        "        self.ln0 = nn.LayerNorm(self.heads*self.hidden)\n",
        "        self.ln1 = nn.LayerNorm(self.heads*self.hidden)\n",
        "#         if bias and concat:\n",
        "#             self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
        "#         elif bias and not concat:\n",
        "#             self.bias = Parameter(torch.Tensor(out_channels))\n",
        "#         else:\n",
        "\n",
        "#         Always no bias! (For now)\n",
        "        self.register_parameter('bias', None)\n",
        "\n",
        "        self._alpha = None\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #         glorot(self.lin_l.weight)\n",
        "        glorot(self.lin_K.weight)\n",
        "        glorot(self.lin_V.weight)\n",
        "        self.rFF.reset_parameters()\n",
        "        self.ln0.reset_parameters()\n",
        "        self.ln1.reset_parameters()\n",
        "#         glorot(self.att_l)\n",
        "        nn.init.xavier_uniform_(self.att_r)\n",
        "#         zeros(self.bias)\n",
        "\n",
        "    def forward(self, x, edge_index: Adj,\n",
        "                size: Size = None, return_attention_weights=None):\n",
        "        # type: (Union[Tensor, OptPairTensor], Tensor, Size, NoneType) -> Tensor  # noqa\n",
        "        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, NoneType) -> Tensor  # noqa\n",
        "        # type: (Union[Tensor, OptPairTensor], Tensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
        "        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
        "                will additionally return the tuple\n",
        "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
        "                attention weights for each edge. (default: :obj:`None`)\n",
        "        \"\"\"\n",
        "        H, C = self.heads, self.hidden\n",
        "\n",
        "        x_l: OptTensor = None\n",
        "        x_r: OptTensor = None\n",
        "        alpha_l: OptTensor = None\n",
        "        alpha_r: OptTensor = None\n",
        "        if isinstance(x, Tensor):\n",
        "            assert x.dim() == 2, 'Static graphs not supported in `GATConv`.'\n",
        "            x_K = self.lin_K(x).view(-1, H, C)\n",
        "            x_V = self.lin_V(x).view(-1, H, C)\n",
        "            alpha_r = (x_K * self.att_r).sum(dim=-1)\n",
        "\n",
        "        out = self.propagate(edge_index, x=x_V,\n",
        "                             alpha=alpha_r, aggr=self.aggr)\n",
        "\n",
        "        alpha = self._alpha\n",
        "        self._alpha = None\n",
        "\n",
        "#         Note that in the original code of GMT paper, they do not use additional W^O to combine heads.\n",
        "#         This is because O = softmax(QK^T)V and V = V_in*W^V. So W^O can be effectively taken care by W^V!!!\n",
        "        out += self.att_r  # This is Seed + Multihead\n",
        "        # concat heads then LayerNorm. Z (rhs of Eq(7)) in GMT paper.\n",
        "        out = self.ln0(out.view(-1, self.heads * self.hidden))\n",
        "        # rFF and skip connection. Lhs of eq(7) in GMT paper.\n",
        "        out = self.ln1(out+F.relu(self.rFF(out)))\n",
        "\n",
        "        if isinstance(return_attention_weights, bool):\n",
        "            assert alpha is not None\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                return out, (edge_index, alpha)\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                return out, edge_index.set_value(alpha, layout='coo')\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def message(self, x_j, alpha_j,\n",
        "                index, ptr,\n",
        "                size_j):\n",
        "        #         ipdb.set_trace()\n",
        "        alpha = alpha_j\n",
        "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
        "        alpha = softmax(alpha, index, ptr, index.max()+1)\n",
        "        self._alpha = alpha\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "        return x_j * alpha.unsqueeze(-1)\n",
        "\n",
        "    def aggregate(self, inputs, index,\n",
        "                  dim_size=None):\n",
        "        r\"\"\"Aggregates messages from neighbors as\n",
        "        :math:`\\square_{j \\in \\mathcal{N}(i)}`.\n",
        "\n",
        "        Takes in the output of message computation as first argument and any\n",
        "        argument which was initially passed to :meth:`propagate`.\n",
        "\n",
        "        By default, this function will delegate its call to scatter functions\n",
        "        that support \"add\", \"mean\" and \"max\" operations as specified in\n",
        "        :meth:`__init__` by the :obj:`aggr` argument.\n",
        "        \"\"\"\n",
        "#         ipdb.set_trace()\n",
        "        if self.aggr is None:\n",
        "            raise ValueError(\"aggr was not passed!\")\n",
        "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
        "                                             self.in_channels,\n",
        "                                             self.out_channels, self.heads)\n",
        "\n",
        "\n",
        "class HalfNLHconv(MessagePassing):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 hid_dim,\n",
        "                 out_dim,\n",
        "                 num_layers,\n",
        "                 dropout,\n",
        "                 Normalization='bn',\n",
        "                 InputNorm=False,\n",
        "                 heads=1,\n",
        "                 attention=True\n",
        "                 ):\n",
        "        super(HalfNLHconv, self).__init__()\n",
        "\n",
        "        self.attention = attention\n",
        "        self.dropout = dropout\n",
        "\n",
        "        if self.attention:\n",
        "            self.prop = PMA(in_dim, hid_dim, out_dim, num_layers, heads=heads)\n",
        "        else:\n",
        "            if num_layers > 0:\n",
        "                self.f_enc = MLP(in_dim, hid_dim, hid_dim, num_layers, dropout, Normalization, InputNorm)\n",
        "                self.f_dec = MLP(hid_dim, hid_dim, out_dim, num_layers, dropout, Normalization, InputNorm)\n",
        "            else:\n",
        "                self.f_enc = nn.Identity()\n",
        "                self.f_dec = nn.Identity()\n",
        "#         self.bn = nn.BatchNorm1d(dec_hid_dim)\n",
        "#         self.dropout = dropout\n",
        "#         self.Prop = S2SProp()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "\n",
        "        if self.attention:\n",
        "            self.prop.reset_parameters()\n",
        "        else:\n",
        "            if self.f_enc.__class__.__name__ != 'Identity':\n",
        "                self.f_enc.reset_parameters()\n",
        "            if self.f_dec.__class__.__name__ != 'Identity':\n",
        "                self.f_dec.reset_parameters()\n",
        "#         self.bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, size):\n",
        "        \"\"\"\n",
        "        input -> MLP -> Prop\n",
        "        \"\"\"\n",
        "\n",
        "        if self.attention:\n",
        "            x = self.prop(x, edge_index)\n",
        "        else:\n",
        "            x = F.relu(self.f_enc(x))\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = self.propagate(edge_index, x=x, size=size)\n",
        "            x = F.relu(self.f_dec(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def message(self, x_j):\n",
        "        out = x_j\n",
        "        return out\n",
        "\n",
        "    def aggregate(self, inputs, index,\n",
        "                  dim_size=None):\n",
        "        r\"\"\"Aggregates messages from neighbors as\n",
        "        :math:`\\square_{j \\in \\mathcal{N}(i)}`.\n",
        "\n",
        "        Takes in the output of message computation as first argument and any\n",
        "        argument which was initially passed to :meth:`propagate`.\n",
        "\n",
        "        By default, this function will delegate its call to scatter functions\n",
        "        that support \"add\", \"mean\" and \"max\" operations as specified in\n",
        "        :meth:`__init__` by the :obj:`aggr` argument.\n",
        "        \"\"\"\n",
        "#         ipdb.set_trace()\n",
        "        if self.aggr is None:\n",
        "            raise ValueError(\"aggr was not passed!\")\n",
        "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bb98929f",
      "metadata": {
        "id": "bb98929f"
      },
      "outputs": [],
      "source": [
        "class SetGNN(nn.Module):\n",
        "    def __init__(self, num_features,Classifier_hidden, num_classes,\n",
        "                 Classifier_num_layers, MLP_hidden, MLP_num_layers,\n",
        "                 All_num_layers, dropout, aggregate, normalization,\n",
        "                 deepset_input_norm, GPR, use_PMA, heads, LearnMask, norm=None):\n",
        "        super(SetGNN, self).__init__()\n",
        "        \"\"\"\n",
        "        args should contain the following:\n",
        "        V_in_dim, V_enc_hid_dim, V_dec_hid_dim, V_out_dim, V_enc_num_layers, V_dec_num_layers\n",
        "        E_in_dim, E_enc_hid_dim, E_dec_hid_dim, E_out_dim, E_enc_num_layers, E_dec_num_layers\n",
        "        All_num_layers,dropout\n",
        "        !!! V_in_dim should be the dimension of node features\n",
        "        !!! E_out_dim should be the number of classes (for classification)\n",
        "        \"\"\"\n",
        "\n",
        "#         Now set all dropout the same, but can be different\n",
        "        self.All_num_layers = All_num_layers\n",
        "        self.dropout = dropout\n",
        "        self.aggr = aggregate\n",
        "        self.NormLayer = normalization\n",
        "        self.InputNorm = deepset_input_norm\n",
        "        self.GPR = GPR\n",
        "        self.LearnMask =LearnMask\n",
        "#         Now define V2EConvs[i], V2EConvs[i] for ith layers\n",
        "#         Currently we assume there's no hyperedge features, which means V_out_dim = E_in_dim\n",
        "#         If there's hyperedge features, concat with Vpart decoder output features [V_feat||E_feat]\n",
        "        self.V2EConvs = nn.ModuleList()\n",
        "        self.E2VConvs = nn.ModuleList()\n",
        "        self.bnV2Es = nn.ModuleList()\n",
        "        self.bnE2Vs = nn.ModuleList()\n",
        "\n",
        "        if self.LearnMask:\n",
        "            self.Importance = Parameter(torch.ones(norm.size()))\n",
        "\n",
        "        if self.All_num_layers == 0:\n",
        "            self.classifier = MLP(in_channels=num_features,\n",
        "                                  hidden_channels=Classifier_hidden,\n",
        "                                  out_channels=num_classes,\n",
        "                                  num_layers=Classifier_num_layers,\n",
        "                                  dropout=self.dropout,\n",
        "                                  Normalization=self.NormLayer,\n",
        "                                  InputNorm=False)\n",
        "        else:\n",
        "            self.V2EConvs.append(HalfNLHconv(in_dim=num_features,\n",
        "                                             hid_dim=MLP_hidden,\n",
        "                                             out_dim=MLP_hidden,\n",
        "                                             num_layers=MLP_num_layers,\n",
        "                                             dropout=self.dropout,\n",
        "                                             Normalization=self.NormLayer,\n",
        "                                             InputNorm=self.InputNorm,\n",
        "                                             heads=heads,\n",
        "                                             attention=use_PMA))\n",
        "            self.bnV2Es.append(nn.BatchNorm1d(MLP_hidden))\n",
        "            self.E2VConvs.append(HalfNLHconv(in_dim=MLP_hidden,\n",
        "                                             hid_dim=MLP_hidden,\n",
        "                                             out_dim=MLP_hidden,\n",
        "                                             num_layers=MLP_num_layers,\n",
        "                                             dropout=self.dropout,\n",
        "                                             Normalization=self.NormLayer,\n",
        "                                             InputNorm=self.InputNorm,\n",
        "                                             heads=heads,\n",
        "                                             attention=use_PMA))\n",
        "            self.bnE2Vs.append(nn.BatchNorm1d(MLP_hidden))\n",
        "            for _ in range(self.All_num_layers-1):\n",
        "                self.V2EConvs.append(HalfNLHconv(in_dim=MLP_hidden,\n",
        "                                                 hid_dim=MLP_hidden,\n",
        "                                                 out_dim=MLP_hidden,\n",
        "                                                 num_layers=MLP_num_layers,\n",
        "                                                 dropout=self.dropout,\n",
        "                                                 Normalization=self.NormLayer,\n",
        "                                                 InputNorm=self.InputNorm,\n",
        "                                                 heads=heads,\n",
        "                                                 attention=use_PMA))\n",
        "                self.bnV2Es.append(nn.BatchNorm1d(MLP_hidden))\n",
        "                self.E2VConvs.append(HalfNLHconv(in_dim=MLP_hidden,\n",
        "                                                 hid_dim=MLP_hidden,\n",
        "                                                 out_dim=MLP_hidden,\n",
        "                                                 num_layers=MLP_num_layers,\n",
        "                                                 dropout=self.dropout,\n",
        "                                                 Normalization=self.NormLayer,\n",
        "                                                 InputNorm=self.InputNorm,\n",
        "                                                 heads=heads,\n",
        "                                                 attention=use_PMA))\n",
        "                self.bnE2Vs.append(nn.BatchNorm1d(MLP_hidden))\n",
        "            if self.GPR:\n",
        "                self.MLP = MLP(in_channels=num_features,\n",
        "                               hidden_channels=MLP_hidden,\n",
        "                               out_channels=MLP_hidden,\n",
        "                               num_layers=MLP_num_layers,\n",
        "                               dropout=self.dropout,\n",
        "                               Normalization=self.NormLayer,\n",
        "                               InputNorm=False)\n",
        "                self.GPRweights = Linear(self.All_num_layers+1, 1, bias=False)\n",
        "                self.classifier = MLP(in_channels=MLP_hidden,\n",
        "                                      hidden_channels=Classifier_hidden,\n",
        "                                      out_channels=num_classes,\n",
        "                                      num_layers=Classifier_num_layers,\n",
        "                                      dropout=self.dropout,\n",
        "                                      Normalization=self.NormLayer,\n",
        "                                      InputNorm=False)\n",
        "            else:\n",
        "                self.classifier = MLP(in_channels=MLP_hidden,\n",
        "                                      hidden_channels=Classifier_hidden,\n",
        "                                      out_channels=num_classes,\n",
        "                                      num_layers=Classifier_num_layers,\n",
        "                                      dropout=self.dropout,\n",
        "                                      Normalization=self.NormLayer,\n",
        "                                      InputNorm=False)\n",
        "\n",
        "\n",
        "#         Now we simply use V_enc_hid=V_dec_hid=E_enc_hid=E_dec_hid\n",
        "#         However, in general this can be arbitrary.\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.V2EConvs:\n",
        "            layer.reset_parameters()\n",
        "        for layer in self.E2VConvs:\n",
        "            layer.reset_parameters()\n",
        "        for layer in self.bnV2Es:\n",
        "            layer.reset_parameters()\n",
        "        for layer in self.bnE2Vs:\n",
        "            layer.reset_parameters()\n",
        "        self.classifier.reset_parameters()\n",
        "        if self.GPR:\n",
        "            self.MLP.reset_parameters()\n",
        "            self.GPRweights.reset_parameters()\n",
        "        if self.LearnMask:\n",
        "            nn.init.ones_(self.Importance)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        \"\"\"\n",
        "        The data should contain the follows\n",
        "        data.x: node features\n",
        "        data.edge_index: edge list (of size (2,|E|)) where data.edge_index[0] contains nodes and data.edge_index[1] contains hyperedges\n",
        "        !!! Note that self loop should be assigned to a new (hyper)edge id!!!\n",
        "        !!! Also note that the (hyper)edge id should start at 0 (akin to node id)\n",
        "        data.norm: The weight for edges in bipartite graphs, correspond to data.edge_index\n",
        "        !!! Note that we output final node representation. Loss should be defined outside.\n",
        "        \"\"\"\n",
        "#             The data should contain the follows\n",
        "#             data.x: node features\n",
        "#             data.V2Eedge_index:  edge list (of size (2,|E|)) where\n",
        "#             data.V2Eedge_index[0] contains nodes and data.V2Eedge_index[1] contains hyperedges\n",
        "\n",
        "        # x, edge_index, norm = data.x, data.edge_index, data.norm\n",
        "\n",
        "        if self.LearnMask:\n",
        "            norm = self.Importance\n",
        "        cidx = edge_index[1].min()\n",
        "        edge_index[1] -= cidx  # make sure we do not waste memory\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        reversed_edge_index = torch.stack(\n",
        "            [edge_index[1], edge_index[0]], dim=0)\n",
        "        if self.GPR:\n",
        "            xs = []\n",
        "            xs.append(F.relu(self.MLP(x)))\n",
        "            for i, _ in enumerate(self.V2EConvs):\n",
        "                x = F.relu(self.V2EConvs[i](x, edge_index, self.aggr))\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = self.E2VConvs[i](x, reversed_edge_index,  self.aggr)\n",
        "                x = F.relu(x)\n",
        "                xs.append(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = torch.stack(xs, dim=-1)\n",
        "            x = self.GPRweights(x).squeeze()\n",
        "            x = global_mean_pool(x, batch)\n",
        "            x = self.classifier(x)\n",
        "        else:\n",
        "            num_nodes = x.shape[0]\n",
        "            num_edges = edge_index[1].max()+1\n",
        "            x = F.dropout(x, p=0.2, training=self.training) # Input dropout\n",
        "            for i, _ in enumerate(self.V2EConvs):\n",
        "                x = F.relu(self.V2EConvs[i](x, edge_index, size=(num_nodes, num_edges)))\n",
        "\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "                x = F.relu(self.E2VConvs[i](\n",
        "                    x, reversed_edge_index, size=(num_edges, num_nodes)))\n",
        "\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            x = global_mean_pool(x, batch)\n",
        "            x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "tp4kQ17m7kXd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp4kQ17m7kXd",
        "outputId": "1d0dda93-c556-436f-a3fe-a3d8373930b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ev1vbXamQhy",
        "outputId": "14a19e31-135c-4ba7-dd3d-4665c2d37d6b"
      },
      "id": "_ev1vbXamQhy",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.4-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.4)\n",
            "Requirement already satisfied: nibabel>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (24.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.11.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel>=4.0.0->nilearn) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "mXOkBPDG66uS",
      "metadata": {
        "id": "mXOkBPDG66uS"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import InMemoryDataset\n",
        "from torch_geometric.utils import from_networkx\n",
        "import networkx as nx\n",
        "from networkx.convert_matrix import from_numpy_array\n",
        "import pickle as pkl\n",
        "from nilearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "g7B2_LQM7n1V",
      "metadata": {
        "id": "g7B2_LQM7n1V"
      },
      "outputs": [],
      "source": [
        "# The corr matrices to build the hypergraphs\n",
        "\n",
        "# Creating a dictionary of lists of paths to the correlation matrices for each method. Each list in the dictionary represents a different method.\n",
        "## This is saved as corr_matrix_paths in functions.py\n",
        "methods = ['pearson', 'spearman', 'kendall', 'partial']\n",
        "full_corr_path_lists = {}\n",
        "for method in methods:\n",
        "    method_dir = f'/content/Bachelor_Thesis_Alzheimers_HGNN/ADNI_full/corr_matrices/corr_matrix_{method}'\n",
        "    full_corr_path_lists[method] = []\n",
        "    for file in os.listdir(method_dir):\n",
        "        full_corr_path_lists[method].append(file)\n",
        "\n",
        "# Generating the diagnostic label from the diagnostic_label.csv file\n",
        "diagnostic_label = np.loadtxt('/content/Bachelor_Thesis_Alzheimers_HGNN/ADNI_full/diagnostic_label.csv', dtype=str, delimiter=',')\n",
        "\n",
        "# Combining the 'EMCI', 'LMCI' and 'MCI' diagnostics into a single 'MCI' label for simplicity, then one-hot encoding the diagnostics\n",
        "for patient in range(len(diagnostic_label)):\n",
        "    if diagnostic_label[patient] == 'CN':\n",
        "        diagnostic_label[patient] = 0\n",
        "    elif diagnostic_label[patient] == 'SMC':\n",
        "        diagnostic_label[patient] = 1\n",
        "    elif diagnostic_label[patient] == 'EMCI' or diagnostic_label[patient] == 'LMCI' or diagnostic_label[patient] == 'MCI':\n",
        "        diagnostic_label[patient] = 2\n",
        "    elif diagnostic_label[patient] == 'AD':\n",
        "        diagnostic_label[patient] = 3\n",
        "    else:\n",
        "        print('Error: Diagnostic label not recognised')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "wngd3EX3eZTP",
      "metadata": {
        "id": "wngd3EX3eZTP"
      },
      "outputs": [],
      "source": [
        "# Useful for loading the hg_dicts\n",
        "def load_hg_dict(root):\n",
        "    dict_list = []\n",
        "    for filename in os.listdir(root):\n",
        "        path = os.path.join(root, filename)\n",
        "        hg_dict = pkl.load(open(path, 'rb'))\n",
        "        dict_list.append(hg_dict)\n",
        "    return dict_list\n",
        "\n",
        "# Creating a dictionary of lists of paths to the correlation matrices for each method. Each list in the dictionary represents a different method.\n",
        "def corr_matrix_paths():\n",
        "    methods = ['pearson', 'spearman', 'kendall', 'partial']\n",
        "    full_corr_path_lists = {}\n",
        "    for method in methods:\n",
        "        method_dir = f'/content/Bachelor_Thesis_Alzheimers_HGNN/ADNI_full/corr_matrices/corr_matrix_{method}/'\n",
        "        full_corr_path_lists[method] = []\n",
        "        for file in os.listdir(method_dir):\n",
        "            full_corr_path_lists[method].append(file)\n",
        "    return full_corr_path_lists\n",
        "\n",
        "# Combining the 'EMCI', 'LMCI' and 'MCI' diagnostics into a single 'MCI' label for simplicity, then one-hot encoding the diagnostics\n",
        "def combine_diag_labels(diagnostic_label):\n",
        "    for patient in range(len(diagnostic_label)):\n",
        "        if diagnostic_label[patient] == 'CN':\n",
        "            diagnostic_label[patient] = 0\n",
        "        elif diagnostic_label[patient] == 'SMC':\n",
        "            diagnostic_label[patient] = 1\n",
        "        elif diagnostic_label[patient] == 'EMCI' or diagnostic_label[patient] == 'LMCI' or diagnostic_label[patient] == 'MCI':\n",
        "            diagnostic_label[patient] = 2\n",
        "        elif diagnostic_label[patient] == 'AD':\n",
        "            diagnostic_label[patient] = 3\n",
        "        else:\n",
        "            print('Error: Diagnostic label not recognised')\n",
        "            break\n",
        "    return diagnostic_label\n",
        "\n",
        "# Generating the brain atlas\n",
        "def gen_atlas_labels():\n",
        "    atlas = datasets.fetch_atlas_aal()\n",
        "    atlas_filename = atlas.maps\n",
        "    atlas_labels = atlas.labels\n",
        "    n_ROIs = len(atlas_labels)\n",
        "    return atlas, atlas_filename, atlas_labels, n_ROIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "o8JUGiy7epGf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o8JUGiy7epGf",
        "outputId": "893340f2-f3d6-434f-c87d-69b369d438dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hypernetx\n",
            "  Downloading hypernetx-2.3.2-py3-none-any.whl (578 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.5/578.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting celluloid>=0.2.0 (from hypernetx)\n",
            "  Downloading celluloid-0.2.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting decorator>=5.1.1 (from hypernetx)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting igraph>=0.11.4 (from hypernetx)\n",
            "  Downloading igraph-0.11.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=3.3 in /usr/local/lib/python3.10/dist-packages (from hypernetx) (3.3)\n",
            "Collecting pandas>=2.2.2 (from hypernetx)\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.4.2 (from hypernetx)\n",
            "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from celluloid>=0.2.0->hypernetx) (3.7.1)\n",
            "Collecting texttable>=1.6.2 (from igraph>=0.11.4->hypernetx)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.2->hypernetx) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.2->hypernetx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.2->hypernetx) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.2->hypernetx) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->hypernetx) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->hypernetx) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->hypernetx) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->hypernetx) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->celluloid>=0.2.0->hypernetx) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->celluloid>=0.2.0->hypernetx) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->celluloid>=0.2.0->hypernetx) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->celluloid>=0.2.0->hypernetx) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->celluloid>=0.2.0->hypernetx) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->celluloid>=0.2.0->hypernetx) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->celluloid>=0.2.0->hypernetx) (3.1.2)\n",
            "Installing collected packages: texttable, igraph, decorator, scikit-learn, pandas, celluloid, hypernetx\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed celluloid-0.2.0 decorator-5.1.1 hypernetx-2.3.2 igraph-0.11.5 pandas-2.2.2 scikit-learn-1.5.0 texttable-1.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator"
                ]
              },
              "id": "d67b0d5ec6d14e6893c69e0808dfbd96"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install hypernetx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "jt6E6Or1ejW_",
      "metadata": {
        "id": "jt6E6Or1ejW_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "bf68f62e-6d0e-4fbe-eeb1-dd876f96bc73"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a14bb80bbc58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhypernetx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hypernetx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhypernetx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhypernetx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhypernetx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhypernetx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhypernetx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hypernetx/algorithms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mGillespie_SIS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 40\u001b[0;31m from hypernetx.algorithms.laplacians_clustering import (\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprob_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mget_pi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hypernetx/algorithms/laplacians_clustering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meigs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhypernetx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHyperNetXError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_affinity_propagation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAffinityPropagation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffinity_propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from ._agglomerative import (\n\u001b[1;32m      8\u001b[0m     \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_affinity_propagation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClusterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvergenceWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairwise_distances_argmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import hypernetx as hnx\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "from torch_geometric.data import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OYAbqM9s6_y_",
      "metadata": {
        "id": "OYAbqM9s6_y_"
      },
      "outputs": [],
      "source": [
        "# Defining a function to display some statistics and features about the dataset.\n",
        "def dataset_features_and_stats(dataset):\n",
        "    print()\n",
        "    print(f'Dataset: {dataset}:')\n",
        "    print('====================')\n",
        "    print(f'Number of graphs: {len(dataset)}')\n",
        "    print(f'Weighted: {dataset.weight}')\n",
        "    print(f'Threshold: {dataset.threshold}')\n",
        "    print(f'Correlation Method: {dataset.method}')\n",
        "    print(f'Number of features: {dataset.num_features}')\n",
        "    print(f'Number of classes: {len(np.unique(diagnostic_label))}')\n",
        "\n",
        "    # Getting the first graph object in the dataset.\n",
        "    data = dataset[0]\n",
        "\n",
        "    print()\n",
        "    print(data)\n",
        "    print('=============================================================')\n",
        "\n",
        "    # Some statistics about the first graph.\n",
        "    print(f'Number of nodes: {data.num_nodes}')\n",
        "    print(f'Number of edges: {data.num_edges}')\n",
        "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "    print(f'Has self-loops: {data.has_self_loops()}')\n",
        "    print(f'Is undirected: {data.is_undirected()}')\n",
        "\n",
        "    # Creating a NetworkX graph manually from the sample graph data\n",
        "    sample_networkx_graph = nx.Graph()\n",
        "\n",
        "    # Adding nodes with their attributes\n",
        "    for i, feature_vector in enumerate(data.x):\n",
        "        sample_networkx_graph.add_node(i, degree=feature_vector[0], betweenness_centrality=feature_vector[1], local_efficiency=feature_vector[2], cluster_coefficient=feature_vector[3], ratio_local_global=feature_vector[4])\n",
        "\n",
        "    # Adding edges with no weights\n",
        "    edge_index = data.edge_index\n",
        "    for j in range(edge_index.shape[1]):\n",
        "        node1 = edge_index[0][j].item()\n",
        "        node2 = edge_index[1][j].item()\n",
        "        sample_networkx_graph.add_edge(node1, node2, weight=1)\n",
        "\n",
        "    # Plotting the sample graph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sample_networkx_graph.remove_edges_from(nx.selfloop_edges(sample_networkx_graph))\n",
        "    pos = nx.spring_layout(sample_networkx_graph, k=0.5)\n",
        "    nx.draw(sample_networkx_graph, pos, with_labels=True)\n",
        "    plt.title('Graph Visualization for patient 0')\n",
        "    plt.show()\n",
        "\n",
        "    print(data.y.detach().long())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HG6DrryY6qOz",
      "metadata": {
        "id": "HG6DrryY6qOz"
      },
      "outputs": [],
      "source": [
        "# Defining functions to simplify the code in the class Raw_to_Graph.\n",
        "\n",
        "# To convert a dictionnary into a numpy array\n",
        "def dict_to_array(dict):\n",
        "    array = np.array(list(dict.values()))\n",
        "    return array\n",
        "\n",
        "# To normalize an array\n",
        "def normalize_array(array):\n",
        "    norm_array = (array - np.mean(array)) / np.std(array)\n",
        "    return norm_array\n",
        "\n",
        "# Defining a class to preprocess raw data into a format suitable for training Graph Neural Networks (GNNs).\n",
        "## With the possibility of assigning weight to edges, adding the age feature, sex feature, and matrixe profiling.\n",
        "\n",
        "class Raw_to_Hypergraph(InMemoryDataset):\n",
        "    def __init__(self, root, hg_data_path, method, weight, threshold, age=False, sex=False, transform=None, pre_transform=None):\n",
        "        self.method = method\n",
        "        self.weight = weight\n",
        "        self.threshold = threshold\n",
        "        self.age = age\n",
        "        self.sex = sex\n",
        "        self.hg_data_path = hg_data_path\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    # This function is used to process the raw data into a format suitable for GNNs, by constructing graphs out of the connectivity matrices.\n",
        "    def process(self):\n",
        "        # Loading the prebuilt hypergraphs and the correlation matrices\n",
        "        hg_dict_list = load_hg_dict(self.hg_data_path)\n",
        "        full_corr_path_lists = corr_matrix_paths()\n",
        "        corr_matrix_list = full_corr_path_lists['pearson']\n",
        "\n",
        "        # Generating the diagnostic file from the diagnostic_label.csv file\n",
        "        diagnostic_label = np.loadtxt('/content/Bachelor_Thesis_Alzheimers_HGNN/ADNI_full/diagnostic_label.csv', dtype=str, delimiter=',')\n",
        "        # Combining the 'EMCI', 'LMCI' and 'MCI' diagnostics into a single 'MCI' label for simplicity, then one-hot encoding the diagnostics\n",
        "        diagnostic_label = combine_diag_labels(diagnostic_label)\n",
        "\n",
        "        graphs=[]\n",
        "        for patient_idx, patient_hg in enumerate(hg_dict_list):\n",
        "            # Create a NetworkX graph from the hypergraph matrix\n",
        "            hypergraph = hnx.Hypergraph(patient_hg)\n",
        "            print(type(patient_hg))\n",
        "            print(patient_hg)\n",
        "            print(len(patient_hg.keys()))\n",
        "\n",
        "            # Adding the matrix profiling features to the feature array\n",
        "            patient_matrix = corr_matrix_list[patient_idx]\n",
        "            path = f'/content/Bachelor_Thesis_Alzheimers_HGNN/ADNI_full/matrix_profiles/matrix_profile_pearson/{patient_matrix}'\n",
        "            if patient_matrix.endswith('.DS_Store'):\n",
        "                continue  # Skip hidden system files like .DS_Store\n",
        "            with open(path, \"rb\") as fl:\n",
        "                patient_dict = pkl.load(fl)\n",
        "            # combine dimensions\n",
        "            features = np.array(patient_dict['mp']).reshape(len(patient_dict['mp']),-1)\n",
        "            features = features.astype(np.float32)\n",
        "\n",
        "            # # Compute the degree of each node\n",
        "            # degree_dict = {}\n",
        "            # for node in hypergraph.nodes():s\n",
        "            #     degree_dict[node] = hypergraph.degree(node)\n",
        "\n",
        "            # # Convert the degree to NumPy array then normalize it\n",
        "            # degree_array = dict_to_array(degree_dict)\n",
        "            # degree_array_norm = normalize_array(degree_array)\n",
        "            # degree_array_norm = degree_array_norm.reshape(-1, 1)\n",
        "\n",
        "            path_corr = f'/content/Bachelor_Thesis_Alzheimers_HGNN/ADNI_full/corr_matrices/corr_matrix_pearson/{corr_matrix_list[patient_idx]}'\n",
        "            corr_array = np.loadtxt(path_corr, delimiter=',')\n",
        "\n",
        "            # # Concatenate the degree, betweenness centrality, local efficiency, and ratio of local to global efficiency arrays to form a single feature vector\n",
        "            # x_array = np.concatenate((degree_array_norm, corr_array), axis=-1)\n",
        "            # x_array = x_array.astype(np.float32)\n",
        "\n",
        "            # Concatenate the matrix profiling features to the feature array\n",
        "            x_array = np.concatenate((corr_array, features), axis=-1)\n",
        "            x_array = x_array.astype(np.float32)\n",
        "            x = torch.tensor(x_array, dtype=torch.float)\n",
        "\n",
        "            # Create a Pytorch Geometric Data object\n",
        "            edge_index0 = []\n",
        "            edge_index1 = []\n",
        "            i = 0\n",
        "            for hyperedge, nodes in hypergraph.incidence_dict.items():\n",
        "                edge_index0 = np.concatenate((edge_index0, nodes), axis=0)\n",
        "                for j in range(len(nodes)):\n",
        "                    edge_index1.append(i)\n",
        "                i += 1\n",
        "            edge_index = np.stack([[int(x) for x in edge_index0], edge_index1], axis=0)\n",
        "            y = torch.tensor(float(diagnostic_label[patient_idx]))\n",
        "            hg_data = Data(x=x, edge_index=torch.tensor(edge_index, dtype=torch.long), y=y)\n",
        "            graphs.append(hg_data)\n",
        "\n",
        "        data, slices = self.collate(graphs)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wc5NGH_qBQnQ",
      "metadata": {
        "id": "wc5NGH_qBQnQ"
      },
      "outputs": [],
      "source": [
        "# Building the graphs\n",
        "threshold = '3neighbors'\n",
        "weight = False\n",
        "age = False\n",
        "sex = False\n",
        "matrixprofile = True\n",
        "method = 'knn'\n",
        "w_decay = 0\n",
        "lr = 0.001\n",
        "\n",
        "root = f'/content/Bachelor_Thesis_Alzheimers_HGNN/Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MP{matrixprofile}'\n",
        "hg_data_path = f'/content/Bachelor_Thesis_Alzheimers_HGNN/Hypergraphs/{method}/thresh_{threshold}'\n",
        "dataset = Raw_to_Hypergraph(root=root, hg_data_path=hg_data_path, method=method, weight=weight, threshold=threshold, age=age, sex=sex)\n",
        "dataset_features_and_stats(dataset)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "\n",
        "num_features = 572 #number of features in the input\n",
        "num_classes = 4 #number of classes in the output\n",
        "Classifier_hidden = 64 #hidden dim in the last classification layer\n",
        "Classifier_num_layers = 2 #number of layers in the last classification layer\n",
        "MLP_hidden = 64 #hidden states in all the used MLPs\n",
        "MLP_num_layers = 2 #number of layers in all the used MLPs\n",
        "All_num_layers = 2 #number of All*Sets layers in the model\n",
        "dropout = 0.0 #dropout\n",
        "aggregate = 'mean' #type of aggregation (might be ignored, not sure)\n",
        "normalization = 'ln' #type og normalisation it can be ln, bn or none\n",
        "deepset_input_norm = False #if you norm the input or not\n",
        "GPR = False #when set to True, the classification is made from all the intermendiate representations concatenated\n",
        "use_PMA = False #when True, the model is AllSetTransformer, when False the model is AllDeepSets\n",
        "heads = 1 #number of heads in Transformer\n",
        "LearnMask = False\n",
        "\n",
        "# num_nodes = dataset.nodes\n",
        "# x = dataset.x\n",
        "edge_index = dataset[0].edge_index\n",
        "#norm = torch.ones_like(edge_index[0]) #would keep this like that all the time. it means we don't normalise the input.\n",
        "\n",
        "model = SetGNN(num_features, Classifier_hidden, num_classes,\n",
        "              Classifier_num_layers, MLP_hidden, MLP_num_layers,\n",
        "              All_num_layers, dropout, aggregate, normalization,\n",
        "              deepset_input_norm, GPR, use_PMA, heads, LearnMask)\n",
        "# out = model(x, edge_index, norm)\n",
        "# print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dtK4DmZn-n38",
      "metadata": {
        "id": "dtK4DmZn-n38"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wTdo4o8OHFsV",
      "metadata": {
        "id": "wTdo4o8OHFsV"
      },
      "outputs": [],
      "source": [
        "# The function we are using to compute the accuracy of our model\n",
        "def quick_accuracy(y_hat, y):\n",
        "  \"\"\"\n",
        "  Args :\n",
        "    y_hat : logits predicted by model [n, num_classes]\n",
        "    y : ground trutch labels [n]\n",
        "  returns :\n",
        "    average accuracy\n",
        "  \"\"\"\n",
        "  n = y.shape[0]\n",
        "  y_hat = torch.argmax(y_hat, dim=-1)\n",
        "  accuracy = (y_hat==y).sum().data.item()\n",
        "  return accuracy/n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2w6sYssa-lnN",
      "metadata": {
        "id": "2w6sYssa-lnN"
      },
      "outputs": [],
      "source": [
        "# Epoch Training\n",
        "\n",
        "def epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0\n",
        "    for data in train_loader:\n",
        "        target = data.y.clone().detach().long()\n",
        "        print(target)\n",
        "        print(type(target))\n",
        "        print(target.size())\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        print(out)\n",
        "        print(type(out))\n",
        "        print(out.size())\n",
        "        print(target.size())\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += criterion(out, target)\n",
        "        train_accuracy += quick_accuracy(out, target)\n",
        "\n",
        "    train_losses.append(train_loss.detach().numpy()/len(train_loader))\n",
        "    train_accuracies.append(train_accuracy/len(train_loader))\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            target = data.y.clone().detach().long()\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            valid_loss += criterion(out, target)\n",
        "            valid_accuracy += quick_accuracy(out, target)\n",
        "\n",
        "        valid_losses.append(valid_loss.detach().numpy()/len(valid_loader))\n",
        "        valid_accuracies.append(valid_accuracy/len(valid_loader))\n",
        "        if valid_accuracies[-1] > max_valid_accuracy:\n",
        "            max_valid_accuracy = valid_accuracies[-1]\n",
        "            if testing:\n",
        "                test_accuracy = 0\n",
        "                for data in test_loader:\n",
        "                    target = data.y.clone().detach().long()\n",
        "                    out = model(data.x, data.edge_index, data.batch)\n",
        "                    test_accuracy += quick_accuracy(out, target)\n",
        "                test_accuracy = test_accuracy/len(test_loader)\n",
        "\n",
        "    if testing:\n",
        "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy\n",
        "    else:\n",
        "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sDW4AlldnP6W",
      "metadata": {
        "id": "sDW4AlldnP6W"
      },
      "outputs": [],
      "source": [
        "stratify = True\n",
        "# Training the model\n",
        "def train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, test_loader=False, testing=False, n_epochs=100):\n",
        "    test_loader = test_loader\n",
        "    testing = testing\n",
        "    n_epochs = n_epochs\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    valid_losses = []\n",
        "    valid_accuracies = []\n",
        "    max_valid_accuracy = 0\n",
        "    test_accuracy = 0\n",
        "\n",
        "    # start a new wandb run to track this script\n",
        "    run = wandb.init(\n",
        "        # set the wandb project where this run will be logged\n",
        "        project = \"Alzheimers_GNN\",\n",
        "        # track hyperparameters and run metadata\n",
        "        config = {\n",
        "        \"architecture\": \"AllDeepSets\",\n",
        "        \"method\": method,\n",
        "        \"strat + w loss\": stratify,\n",
        "        \"weights\": weight,\n",
        "        \"corr_m node feature\": True,\n",
        "        \"weight_decay\": w_decay,\n",
        "        \"threshold\": threshold,\n",
        "        \"matrix profiling\": True,\n",
        "        \"learning_rate\": parameters[0],\n",
        "        \"hidden_channels\": parameters[1],\n",
        "        \"num_layers\": parameters[2],\n",
        "        \"dropout\": parameters[3],\n",
        "        \"heads\": parameters[4],\n",
        "        \"epochs\": n_epochs},)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        if testing:\n",
        "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
        "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy, \"Test Accuracy\": test_accuracy})\n",
        "        else:\n",
        "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
        "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy})\n",
        "        print(f'Epoch {epoch+1}/{n_epochs}')\n",
        "        print(f'Train Loss: {train_losses[-1]:.4f}, Validation Loss: {valid_losses[-1]:.4f}')\n",
        "        print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {valid_accuracies[-1]:.4f}')\n",
        "        print(f'Max Validation Accuracy: {max_valid_accuracy:.4f}')\n",
        "\n",
        "    if testing:\n",
        "        print('Test Accuracy:', test_accuracy)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot Losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label=f'Train Loss')\n",
        "    plt.plot(valid_losses, label=f'Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label=f'Train Accuracy')\n",
        "    plt.plot(valid_accuracies, label=f'Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plot\n",
        "    lr = parameters[0]\n",
        "    hidden_channels = parameters[1]\n",
        "    num_layers = parameters[2]\n",
        "    dropout = parameters[3]\n",
        "    filename = f'HGConv_Models_MP/threshold_{threshold}/method_{method}/lr{lr}_hc{hidden_channels}_nl{num_layers}_d{dropout}_epochs{n_epochs}_wdecay{w_decay}_w{weight}.png'\n",
        "    # plt.savefig(filename)\n",
        "    if testing:\n",
        "        plt.title(f'Test Accuracy: {test_accuracy}')\n",
        "    plt.show()\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    if testing:\n",
        "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy\n",
        "    else:\n",
        "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YmJhCyCdrgrH",
      "metadata": {
        "id": "YmJhCyCdrgrH"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Creating the train, validation and test sets\n",
        "def create_train_test_valid(dataset, stratify=True, batch_size=16):\n",
        "    X = dataset\n",
        "    y = dataset.data.y\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.125, random_state=42, stratify=y_train)\n",
        "    nbr_classes = len(np.unique(y))\n",
        "\n",
        "    print(f'Number of training graphs: {len(X_train)}')\n",
        "    print(f'Number of validation graphs: {len(X_valid)}')\n",
        "    print(f'Number of test graphs: {len(X_test)}')\n",
        "    print(f'Number of classes: {nbr_classes}')\n",
        "\n",
        "    train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(X_valid, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if stratify:\n",
        "        return train_loader, valid_loader, test_loader, nbr_classes, y_train\n",
        "    else:\n",
        "        return train_loader, valid_loader, test_loader, nbr_classes, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y3PuB8kZoPoj",
      "metadata": {
        "id": "y3PuB8kZoPoj"
      },
      "outputs": [],
      "source": [
        "parameters = [lr, Classifier_hidden, Classifier_num_layers, dropout, heads]\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader, valid_loader, test_loader, nbr_classes, y_train = create_train_test_valid(dataset, stratify)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in train_loader:\n",
        "  out = model(data.x, data.edge_index, data.batch)\n",
        "  print(out.size())\n",
        "  print(out)\n"
      ],
      "metadata": {
        "id": "dFviX1-9TxEA"
      },
      "id": "dFviX1-9TxEA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J_Ug4XvkMdPV",
      "metadata": {
        "id": "J_Ug4XvkMdPV"
      },
      "outputs": [],
      "source": [
        "# Running the training\n",
        "train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, n_epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product"
      ],
      "metadata": {
        "id": "k3dqZjfUdHq9"
      },
      "id": "k3dqZjfUdHq9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Perform a Grid Search\n",
        "\n",
        "# Define hyperparameters\n",
        "w_decay_options = [0, 0.1, 0.001]\n",
        "lr_options = [0.001, 0.01]\n",
        "classifier_hidden_options = [32, 64, 128]\n",
        "dropout_options = [0, 0.1, 0.2]\n",
        "heads_options = [2, 3, 4]\n",
        "\n",
        "# Define static parameters\n",
        "threshold = '3neighbors'\n",
        "weight = False\n",
        "age = False\n",
        "sex = False\n",
        "matrixprofile = True\n",
        "method = 'knn'\n",
        "num_features = 572\n",
        "num_classes = 4\n",
        "classifier_num_layers = 2\n",
        "mlp_hidden = 64\n",
        "mlp_num_layers = 2\n",
        "all_num_layers = 2\n",
        "aggregate = 'mean'\n",
        "normalization = 'ln'\n",
        "deepset_input_norm = False\n",
        "gpr = False\n",
        "use_pma = False\n",
        "learn_mask = False\n",
        "\n",
        "# Grid search\n",
        "for w_decay, lr, classifier_hidden, dropout, heads in product(w_decay_options, lr_options, classifier_hidden_options, dropout_options, heads_options):\n",
        "    root = f'/content/Bachelor_Thesis_Alzheimers_HGNN/Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MP{matrixprofile}'\n",
        "    hg_data_path = f'/content/Bachelor_Thesis_Alzheimers_HGNN/Hypergraphs/{method}/thresh_{threshold}'\n",
        "\n",
        "    dataset = Raw_to_Hypergraph(root=root, hg_data_path=hg_data_path, method=method, weight=weight, threshold=threshold, age=age, sex=sex)\n",
        "    dataset_features_and_stats(dataset)\n",
        "\n",
        "    edge_index = dataset[0].edge_index\n",
        "\n",
        "    model = SetGNN(num_features, classifier_hidden, num_classes,\n",
        "                   classifier_num_layers, mlp_hidden, mlp_num_layers,\n",
        "                   all_num_layers, dropout, aggregate, normalization,\n",
        "                   deepset_input_norm, gpr, use_pma, heads, learn_mask)\n",
        "\n",
        "    parameters = [lr, Classifier_hidden, Classifier_num_layers, dropout, heads]\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader, valid_loader, test_loader, nbr_classes, y_train = create_train_test_valid(dataset, stratify)\n",
        "\n",
        "    # Running the training\n",
        "    train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, n_epochs=500)\n",
        "\n",
        "    print(f\"Evaluated: w_decay={w_decay}, lr={lr}, classifier_hidden={classifier_hidden}, dropout={dropout}, heads={heads}\")\n"
      ],
      "metadata": {
        "id": "q47MyaQFcBeE"
      },
      "id": "q47MyaQFcBeE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Sp0wM3pd1Gq"
      },
      "id": "3Sp0wM3pd1Gq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# coding: utf-8",
      "executable": "/usr/bin/env python",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}