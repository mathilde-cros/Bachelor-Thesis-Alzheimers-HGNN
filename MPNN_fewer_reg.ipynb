{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Sequential, ModuleList\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME']=\"MPNN.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MPNNLayer class defines a single message passing layer, inheriting from torch_geometric.nn.MessagePassing. It applies a linear transformation to the input node features and then performs message passing.\n",
    "- forward method of MPNNLayer adds self-loops to the adjacency matrix, linearly transforms the input node features, and then starts propagating messages using the propagate method.\n",
    "- message method of MPNNLayer normalizes node features and multiplies them with the corresponding adjacency weights.\n",
    "- update method of MPNNLayer returns the new node embeddings after aggregation.\n",
    "- MPNN class serves as a container for multiple MPNNLayer instances, similar to the GCN class. It iterates over the layers, applies them sequentially, and then applies a final MLP for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, nbr_classes):\n",
    "        super(MPNN, self).__init__()\n",
    "        self.nbr_classes = nbr_classes\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(MPNNLayer(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(MPNNLayer(hidden_channels, hidden_channels))\n",
    "        # self.double_mlp = Sequential(Linear(hidden_channels * heads, hidden_channels), ReLU(), Linear(hidden_channels, hidden_channels))\n",
    "        self.mlp = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # xs = []\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "                # xs.append(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # x = torch.cat(xs, dim=1)\n",
    "        # x = self.double_mlp(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class MPNNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MPNNLayer, self).__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Start propagating messages.\n",
    "        # return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)))\n",
    "\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Raw_to_Graph(197):\n",
      "====================\n",
      "Number of graphs: 197\n",
      "Weighted: False\n",
      "Threshold: 0.5\n",
      "Correlation Method: pearson\n",
      "Number of features: 157\n",
      "Number of classes: 6\n",
      "\n",
      "Data(edge_index=[2, 168], weight=[168], x=[40, 157], y=[1], num_nodes=40)\n",
      "=============================================================\n",
      "Number of nodes: 40\n",
      "Number of edges: 168\n",
      "Average node degree: 4.20\n",
      "Has isolated nodes: True\n",
      "Has self-loops: True\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# Testing the class Raw_to_Graph with one example and saving it\n",
    "threshold = 0.5\n",
    "weight = False\n",
    "age = False\n",
    "sex = False\n",
    "matrixprofile = True\n",
    "if matrixprofile:\n",
    "    in_channels = 157 + int(age) + int(sex)\n",
    "else:\n",
    "    in_channels = 5 + int(age) + int(sex)\n",
    "method = 'pearson'\n",
    "\n",
    "root = f'Raw_to_graph_reduced_reg/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MP{matrixprofile}'\n",
    "dataset = f.Raw_to_Graph(root=root, threshold=threshold, method=method, weight=weight, sex=sex, age=age, matrixprofile=matrixprofile)\n",
    "f.dataset_features_and_stats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 137\n",
      "Number of validation graphs: 20\n",
      "Number of test graphs: 40\n",
      "Number of classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Creating the train, validation and test sets\n",
    "train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train(model, optimizer, criterion, w_decay, threshold, train_loader, valid_loader, parameters, test_loader=False, testing=False, n_epochs=100):\n",
    "    test_loader = test_loader\n",
    "    testing = testing\n",
    "    n_epochs = n_epochs\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    test_accuracy = 0\n",
    "    max_valid_accuracy = 0\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = \"Alzheimers_GNN\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config = {\n",
    "        \"architecture\": \"MPNN_reduced_reg\",\n",
    "        \"weights\": weight,\n",
    "        \"weight_decay\": w_decay,\n",
    "        \"threshold\": threshold,\n",
    "        \"matrix profiling\": matrixprofile,\n",
    "        \"learning_rate\": parameters[0],\n",
    "        \"hidden_channels\": parameters[1],\n",
    "        \"num_layers\": parameters[2],\n",
    "        \"dropout\": parameters[3],\n",
    "        \"epochs\": n_epochs},)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if testing:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_losses, test_accuracies = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "            print(f'Train Loss: {train_losses[-1]:.4f}, Validation Loss: {valid_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
    "            print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {valid_accuracies[-1]:.4f}, Test Accuracy: {test_accuracies[-1]:.4f}')\n",
    "            print(f'Max Validation Accuracy: {max_valid_accuracy:.4f}')\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy, \"Test Loss\": test_losses[-1], \"Test Accuracy\": test_accuracies[-1]})\n",
    "        else:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "            print(f'Train Loss: {train_losses[-1]:.4f}, Validation Loss: {valid_losses[-1]:.4f}')\n",
    "            print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {valid_accuracies[-1]:.4f}')\n",
    "            print(f'Max Validation Accuracy: {max_valid_accuracy:.4f}')\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy})\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=f'Train Loss')\n",
    "    plt.plot(valid_losses, label=f'Validation Loss')\n",
    "    if testing:\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=f'Train Accuracy')\n",
    "    plt.plot(valid_accuracies, label=f'Validation Accuracy')\n",
    "    if testing:\n",
    "        plt.plot(test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    lr = parameters[0]\n",
    "    hidden_channels = parameters[1]\n",
    "    num_layers = parameters[2]\n",
    "    dropout = parameters[3]\n",
    "    if matrixprofile:\n",
    "        filename = f'MPNN_Models_redreg_MP/threshold_{threshold}/lr{lr}_hc{hidden_channels}_nl{num_layers}_d{dropout}_epochs{n_epochs}_w{weight}.png'\n",
    "    else:\n",
    "        filename = f'MPNN_Models_redreg/threshold_{threshold}/lr{lr}_hc{hidden_channels}_nl{num_layers}_d{dropout}_epochs{n_epochs}_w{weight}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    if testing:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_losses, test_accuracies\n",
    "    else:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Raw_to_Graph(197):\n",
      "====================\n",
      "Number of graphs: 197\n",
      "Weighted: False\n",
      "Threshold: 0.5\n",
      "Correlation Method: pearson\n",
      "Number of features: 157\n",
      "Number of classes: 6\n",
      "\n",
      "Data(edge_index=[2, 168], weight=[168], x=[40, 157], y=[1], num_nodes=40)\n",
      "=============================================================\n",
      "Number of nodes: 40\n",
      "Number of edges: 168\n",
      "Average node degree: 4.20\n",
      "Has isolated nodes: True\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "Number of training graphs: 137\n",
      "Number of validation graphs: 20\n",
      "Number of test graphs: 40\n",
      "Number of classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNN(\n",
      "  (convs): ModuleList(\n",
      "    (0-2): 3 x MPNNLayer()\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:otjbo3nf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-cherry-9713</strong> at: <a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN/runs/otjbo3nf' target=\"_blank\">https://wandb.ai/alzheimers-cl/Alzheimers_GNN/runs/otjbo3nf</a><br/> View job at <a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE4MjA0NjgxNA==/version_details/v0' target=\"_blank\">https://wandb.ai/alzheimers-cl/Alzheimers_GNN/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE4MjA0NjgxNA==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240604_095202-otjbo3nf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:otjbo3nf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mathilde/Documents/Bachelor_Thesis_Alzheimers_HGNN/wandb/run-20240604_095324-37k00fcc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN/runs/37k00fcc' target=\"_blank\">soft-waterfall-9715</a></strong> to <a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN' target=\"_blank\">https://wandb.ai/alzheimers-cl/Alzheimers_GNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alzheimers-cl/Alzheimers_GNN/runs/37k00fcc' target=\"_blank\">https://wandb.ai/alzheimers-cl/Alzheimers_GNN/runs/37k00fcc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Running the training\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2750\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, w_decay, threshold, train_loader, valid_loader, parameters, test_loader, testing, n_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Valid Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_valid_accuracy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]})\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_accuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_valid_accuracy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Bachelor_Thesis_Alzheimers_HGNN/functions.py:798\u001b[0m, in \u001b[0;36mepochs_training\u001b[0;34m(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\u001b[0m\n\u001b[1;32m    796\u001b[0m target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    797\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 798\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, target)\n\u001b[1;32m    800\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mMPNN.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# xs = []\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 16\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 44\u001b[0m, in \u001b[0;36mMPNNLayer.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Step 3: Start propagating messages.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/folders/wh/snjqmpvj7qv64q1ngy8zvqbh0000gn/T/__main___MPNNLayer_propagate_gg3k7pn9.py:131\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage_and_aggregate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutable_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Begin Message Forward Pre Hook #######################################\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m/var/folders/wh/snjqmpvj7qv64q1ngy8zvqbh0000gn/T/__main___MPNNLayer_propagate_gg3k7pn9.py:74\u001b[0m, in \u001b[0;36mcollect\u001b[0;34m(self, edge_index, size)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Collect user-defined arguments:\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# (1) - Collect `x_j`:\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mx\u001b[49m, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     76\u001b[0m     _x_0, _x_1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "age = False\n",
    "sex = False\n",
    "matrixprofile = True\n",
    "weight = False\n",
    "if matrixprofile:\n",
    "    in_channels = 157 + int(age) + int(sex)\n",
    "else:\n",
    "    in_channels = 5 + int(age) + int(sex)\n",
    "method = 'pearson'\n",
    "\n",
    "root = f'Raw_to_graph_reduced_reg/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MP{matrixprofile}'\n",
    "dataset = f.Raw_to_Graph(root=root, threshold=threshold, method=method, weight=weight, sex=sex, age=age, matrixprofile=matrixprofile)\n",
    "f.dataset_features_and_stats(dataset)\n",
    "# Creating the train, validation and test sets\n",
    "train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset)\n",
    "\n",
    "# Defining the model, optimizer and loss function\n",
    "lr=0.00001\n",
    "hidden_channels=32\n",
    "num_layers=3\n",
    "dropout=0.2\n",
    "w_decay=0\n",
    "parameters = [lr, hidden_channels, num_layers, dropout]\n",
    "\n",
    "model = MPNN(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Printing the model architecture\n",
    "print(model)\n",
    "\n",
    "# Running the training\n",
    "train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = train(model, optimizer, criterion, w_decay, threshold, train_loader, valid_loader, parameters, n_epochs=2750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Raw_to_Graph(197):\n",
      "====================\n",
      "Number of graphs: 197\n",
      "Weighted: True\n",
      "Threshold: 0.5\n",
      "Correlation Method: pearson\n",
      "Number of features: 157\n",
      "Number of classes: 6\n",
      "\n",
      "Data(edge_index=[2, 218], weight=[218], x=[40, 157], y=[1], num_nodes=40)\n",
      "=============================================================\n",
      "Number of nodes: 40\n",
      "Number of edges: 218\n",
      "Average node degree: 5.45\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "Number of training graphs: 137\n",
      "Number of validation graphs: 20\n",
      "Number of test graphs: 40\n",
      "Number of classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m     w_decay \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     56\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mparameters[\u001b[38;5;241m0\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mw_decay)\n\u001b[0;32m---> 57\u001b[0m train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(model, optimizer, criterion, w_decay, threshold, train_loader, valid_loader, parameters, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2750\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Doing some parameter gridsearch to find the best hyperparameters\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Building the graphs\n",
    "threshold = 0.5\n",
    "age = False\n",
    "sex = False\n",
    "matrixprofile = True\n",
    "weight = True\n",
    "if matrixprofile:\n",
    "    in_channels = 157 + int(age) + int(sex)\n",
    "else:\n",
    "    in_channels = 5 + int(age) + int(sex)\n",
    "method = 'pearson'\n",
    "\n",
    "root = f'Raw_to_graph_reduced_reg/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MP{matrixprofile}'\n",
    "dataset = f.Raw_to_Graph(root=root, threshold=threshold, method=method, weight=weight, sex=sex, age=age, matrixprofile=matrixprofile)\n",
    "f.dataset_features_and_stats(dataset)\n",
    "# Creating the train, validation and test sets\n",
    "train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset)\n",
    "\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.001, 0.0001, 0.00001],\n",
    "#     'hidden_channels': [128, 64, 32],\n",
    "#     'num_layers': [3, 2, 1],\n",
    "#     'dropout_rate': [0.3, 0.2, 0.1, 0.0],\n",
    "#     'weight_decay': [0.01, 0.001, 0.0001]\n",
    "# }\n",
    "param_grid = {\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'hidden_channels': [64, 128],\n",
    "    'num_layers': [1, 2],\n",
    "    'dropout_rate': [0.0, 0.1, 0.2],\n",
    "    'weight_decay': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "param_combinations = ParameterGrid(param_grid)\n",
    "n_epochs = 2750\n",
    "# Train using each combination\n",
    "for params in param_combinations:\n",
    "    if matrixprofile:\n",
    "        filename = f'MPNN_Models_redreg_MP/threshold_{threshold}/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}_wdecay{params[\"weight_decay\"]}_w{weight}.png'\n",
    "    else:\n",
    "        filename = f'MPNN_Models_redreg/threshold_{threshold}/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}_wdecay{params[\"weight_decay\"]}_w{weight}.png'\n",
    "    if os.path.exists(filename):\n",
    "        pass\n",
    "    else:\n",
    "        parameters = [params['learning_rate'], params['hidden_channels'], params['num_layers'], params['dropout_rate']]\n",
    "        model = MPNN(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        if 'weight_decay' not in params.keys():\n",
    "            w_decay = 0\n",
    "        else:\n",
    "            w_decay = params['weight_decay']\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "        train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = train(model, optimizer, criterion, w_decay, threshold, train_loader, valid_loader, parameters, n_epochs=2750)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alzheimers-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
