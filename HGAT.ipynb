{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82f483a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HypergraphAttentionConv' from 'torch_geometric.nn' (/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/nn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReLU, Linear, Sequential, ModuleList\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HypergraphAttentionConv\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m global_mean_pool, global_max_pool, global_add_pool\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mf\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HypergraphAttentionConv' from 'torch_geometric.nn' (/Users/mathilde/anaconda3/envs/alzheimers-cl/lib/python3.11/site-packages/torch_geometric/nn/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ReLU, Linear, Sequential, ModuleList\n",
    "from torch_geometric.nn import HypergraphAttentionConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d71b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME']=\"HGAT.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805697be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Hypergraph Attention Neural Network\n",
    "class HGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, heads, nbr_classes):\n",
    "        super(HGAT, self).__init__()\n",
    "        self.nbr_classes = nbr_classes\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(HypergraphAttentionConv(in_channels, hidden_channels, heads=heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(HypergraphAttentionConv(hidden_channels * heads, hidden_channels, heads=heads))\n",
    "        self.mlp = Sequential(Linear(hidden_channels * heads, hidden_channels), ReLU(), Linear(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, hyperedge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, hyperedge_index)\n",
    "            x = F.elu(x)  # Using ELU activation for HGAT\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c63a67b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m hg_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHypergraphs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/thresh_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRaw_to_hypergraph/ADNI_T_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_M_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_W\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_A\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_S\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_MPTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241m.\u001b[39mRaw_to_Hypergraph(root\u001b[38;5;241m=\u001b[39mroot, hg_data_path\u001b[38;5;241m=\u001b[39mhg_data_path, method\u001b[38;5;241m=\u001b[39mmethod, weight\u001b[38;5;241m=\u001b[39mweight, threshold\u001b[38;5;241m=\u001b[39mthreshold, age\u001b[38;5;241m=\u001b[39mage, sex\u001b[38;5;241m=\u001b[39msex)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing the class Raw_to_Hypergraph with one example and saving it\n",
    "threshold = 0.5\n",
    "age = False\n",
    "sex = False\n",
    "method = 'fourier_cluster'\n",
    "weight = False\n",
    "\n",
    "hg_data_path = f'Hypergraphs/{method}/thresh_{threshold}'\n",
    "\n",
    "root = f'Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MPTrue'\n",
    "dataset = f.Raw_to_Hypergraph(root=root, hg_data_path=hg_data_path, method=method, weight=weight, threshold=threshold, age=age, sex=sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train, validation and test sets\n",
    "train_loader, valid_loader, test_loader, nbr_classes = f.create_train_test_valid(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065f29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratify = True\n",
    "# Training the model\n",
    "def train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, test_loader=False, testing=False, n_epochs=100):\n",
    "    test_loader = test_loader\n",
    "    testing = testing\n",
    "    n_epochs = n_epochs\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    max_valid_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    weight = 'NA'\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = \"Alzheimers_GNN\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config = {\n",
    "        \"architecture\": \"HGAT\",\n",
    "        \"method\": method,\n",
    "        \"strat + w loss\": stratify,\n",
    "        \"weights\": weight,\n",
    "        \"corr_m node feature\": True,\n",
    "        \"weight_decay\": w_decay,\n",
    "        \"threshold\": threshold,\n",
    "        \"matrix profiling\": True,\n",
    "        \"learning_rate\": parameters[0],\n",
    "        \"hidden_channels\": parameters[1],\n",
    "        \"num_layers\": parameters[2],\n",
    "        \"dropout\": parameters[3],\n",
    "        \"epochs\": n_epochs},)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if testing:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy, \"Test Accuracy\": test_accuracy})\n",
    "        else:\n",
    "            train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = f.epochs_training(model, optimizer, criterion, train_loader, valid_loader, test_loader, testing, test_accuracy, train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy)\n",
    "            wandb.log({\"Train Loss\": train_losses[-1], \"Train Accuracy\": train_accuracies[-1], \"Validation Loss\": valid_losses[-1], \"Validation Accuracy\": valid_accuracies[-1], \"Max Valid Accuracy\": max_valid_accuracy})\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "        print(f'Train Loss: {train_losses[-1]:.4f}, Validation Loss: {valid_losses[-1]:.4f}')\n",
    "        print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {valid_accuracies[-1]:.4f}')\n",
    "        print(f'Max Validation Accuracy: {max_valid_accuracy:.4f}')\n",
    "\n",
    "    if testing:\n",
    "        print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=f'Train Loss')\n",
    "    plt.plot(valid_losses, label=f'Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=f'Train Accuracy')\n",
    "    plt.plot(valid_accuracies, label=f'Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    lr = parameters[0]\n",
    "    hidden_channels = parameters[1]\n",
    "    num_layers = parameters[2]\n",
    "    dropout = parameters[3]\n",
    "    filename = f'HGAT_Models_MP/threshold_{threshold}/method_{method}/lr{lr}_hc{hidden_channels}_nl{num_layers}_d{dropout}_epochs{n_epochs}_wdecay{w_decay}_w{weight}.png'\n",
    "    plt.savefig(filename)\n",
    "    if testing:\n",
    "        plt.title(f'Test Accuracy: {test_accuracy}')\n",
    "    plt.show()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    if testing:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy\n",
    "    else:\n",
    "        return train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e0bb4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Example model\n",
    "\n",
    "threshold = 0.5\n",
    "age = False\n",
    "sex = False\n",
    "method = 'maximal_clique'\n",
    "weight = False\n",
    "\n",
    "hg_data_path = f'Hypergraphs/{method}/thresh_{threshold}'\n",
    "root = f'Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MPTrue'\n",
    "dataset = f.Raw_to_Hypergraph(root=root, hg_data_path=hg_data_path, method=method, weight=weight, threshold=threshold, age=age, sex=sex)\n",
    "\n",
    "train_loader, valid_loader, test_loader, nbr_classes = f.create_train_test_valid(dataset)\n",
    "\n",
    "# Defining the model, optimizer and loss function\n",
    "lr=0.0001\n",
    "hidden_channels=64\n",
    "num_layers=2\n",
    "dropout=0.1\n",
    "w_decay = 0\n",
    "parameters = [lr, hidden_channels, num_layers, dropout]\n",
    "in_channels = dataset.num_features\n",
    "\n",
    "model = HGAT(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Printing the model architecture\n",
    "print(model)\n",
    "\n",
    "# Running the training\n",
    "train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy = train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, n_epochs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bb180",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Doing some parameter gridsearch to find the best hyperparameters\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# # Building the graphs\n",
    "# threshold = 0.5\n",
    "# age = False\n",
    "# sex = False\n",
    "# method = 'fourier_cluster'\n",
    "# weight = False\n",
    "\n",
    "# hg_data_path = f'Hypergraphs/{method}/thresh_{threshold}'\n",
    "# root = f'Raw_to_hypergraph/ADNI_T_{threshold}_M_{method}_W{weight}_A{age}_S{sex}_MPTrue'\n",
    "# dataset = f.Raw_to_Hypergraph(root=root, hg_data_path=hg_data_path, method=method, weight=weight, threshold=threshold, age=age, sex=sex)\n",
    "\n",
    "# # Creating the train, validation and test sets\n",
    "# train_loader, valid_loader, test_loader, nbr_classes, y_train = f.create_train_test_valid(dataset, stratify)\n",
    "\n",
    "# # param_grid = {\n",
    "# #     'learning_rate': [0.001, 0.0001],\n",
    "# #     'hidden_channels': [128, 64, 32],\n",
    "# #     'num_layers': [3, 2, 1],\n",
    "# #     'dropout_rate': [0.2, 0.1, 0.0],\n",
    "# #     'weight_decay': [0.001, 0.0001]\n",
    "# # }\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.0001, 0.001],\n",
    "#     'hidden_channels': [64, 128],\n",
    "#     'num_layers': [1, 2, 3],\n",
    "#     'dropout_rate': [0.0, 0.1, 0.2],\n",
    "#     'weight_decay': [0.0001, 0.001]\n",
    "# }\n",
    "\n",
    "# # Create combinations of hyperparameters\n",
    "# param_combinations = ParameterGrid(param_grid)\n",
    "# n_epochs = 800\n",
    "# in_channels = dataset.num_features\n",
    "# # Train using each combination\n",
    "# for params in param_combinations:\n",
    "#     filename = f'HGAT_Models_MP/threshold_{threshold}/method_{method}/lr{params[\"learning_rate\"]}_hc{params[\"hidden_channels\"]}_nl{params[\"num_layers\"]}_d{params[\"dropout_rate\"]}_epochs{n_epochs}_wdecay{params[\"weight_decay\"]}_w{weight}.png'\n",
    "#     if os.path.exists(filename):\n",
    "#         pass\n",
    "#     else:\n",
    "#         parameters = [params['learning_rate'], params['hidden_channels'], params['num_layers'], params['dropout_rate']]\n",
    "#         model = HGAT(in_channels=in_channels, hidden_channels=parameters[1], out_channels=nbr_classes, num_layers=parameters[2], dropout=parameters[3], nbr_classes=nbr_classes)\n",
    "#         if stratify:\n",
    "#             diag_lab = [0 , 1 , 2, 3]\n",
    "#             class_freq = []\n",
    "#             for i in diag_lab:\n",
    "#                 class_freq.append(np.count_nonzero(torch.Tensor(y_train) == i))\n",
    "#             class_freq = torch.FloatTensor(class_freq)\n",
    "#             class_weights = 1 / class_freq\n",
    "#             class_weights /= class_weights.sum()\n",
    "#             criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "#         else:\n",
    "#             criterion = torch.nn.CrossEntropyLoss() \n",
    "#         if 'weight_decay' not in params.keys():\n",
    "#             w_decay = 0\n",
    "#         else:\n",
    "#             w_decay = params['weight_decay']\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=parameters[0], weight_decay=w_decay)\n",
    "#         train_losses, train_accuracies, valid_losses, valid_accuracies, max_valid_accuracy, test_accuracy = train(model, optimizer, criterion, w_decay, threshold, method, train_loader, valid_loader, parameters, test_loader, testing=True, n_epochs=800)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "alzheimers-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
