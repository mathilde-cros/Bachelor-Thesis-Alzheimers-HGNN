{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49622b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "## FUTURE WORK (NOT PART OF THESIS)\n",
    "\n",
    "import math\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_scatter import scatter_add, scatter,  scatter_mean\n",
    "from torch_geometric.typing import Adj, Size, OptTensor\n",
    "from typing import Optional\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e70cf4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d4cd1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout=.5, Normalization='bn', InputNorm=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.normalizations = nn.ModuleList()\n",
    "        self.InputNorm = InputNorm\n",
    "\n",
    "        assert Normalization in ['bn', 'ln', 'None']\n",
    "        if Normalization == 'bn':\n",
    "            if num_layers == 1:\n",
    "                # just linear layer i.e. logistic regression\n",
    "                if InputNorm:\n",
    "                    self.normalizations.append(nn.BatchNorm1d(in_channels))\n",
    "                else:\n",
    "                    self.normalizations.append(nn.Identity())\n",
    "                self.lins.append(nn.Linear(in_channels, out_channels))\n",
    "            else:\n",
    "                if InputNorm:\n",
    "                    self.normalizations.append(nn.BatchNorm1d(in_channels))\n",
    "                else:\n",
    "                    self.normalizations.append(nn.Identity())\n",
    "                self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "                self.normalizations.append(nn.BatchNorm1d(hidden_channels))\n",
    "                for _ in range(num_layers - 2):\n",
    "                    self.lins.append(\n",
    "                        nn.Linear(hidden_channels, hidden_channels))\n",
    "                    self.normalizations.append(nn.BatchNorm1d(hidden_channels))\n",
    "                self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
    "        elif Normalization == 'ln':\n",
    "            if num_layers == 1:\n",
    "                # just linear layer i.e. logistic regression\n",
    "                if InputNorm:\n",
    "                    self.normalizations.append(nn.LayerNorm(in_channels))\n",
    "                else:\n",
    "                    self.normalizations.append(nn.Identity())\n",
    "                self.lins.append(nn.Linear(in_channels, out_channels))\n",
    "            else:\n",
    "                if InputNorm:\n",
    "                    self.normalizations.append(nn.LayerNorm(in_channels))\n",
    "                else:\n",
    "                    self.normalizations.append(nn.Identity())\n",
    "                self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "                self.normalizations.append(nn.LayerNorm(hidden_channels))\n",
    "                for _ in range(num_layers - 2):\n",
    "                    self.lins.append(\n",
    "                        nn.Linear(hidden_channels, hidden_channels))\n",
    "                    self.normalizations.append(nn.LayerNorm(hidden_channels))\n",
    "                self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
    "        else:\n",
    "            if num_layers == 1:\n",
    "                # just linear layer i.e. logistic regression\n",
    "                self.normalizations.append(nn.Identity())\n",
    "                self.lins.append(nn.Linear(in_channels, out_channels))\n",
    "            else:\n",
    "                self.normalizations.append(nn.Identity())\n",
    "                self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "                self.normalizations.append(nn.Identity())\n",
    "                for _ in range(num_layers - 2):\n",
    "                    self.lins.append(\n",
    "                        nn.Linear(hidden_channels, hidden_channels))\n",
    "                    self.normalizations.append(nn.Identity())\n",
    "                self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for normalization in self.normalizations:\n",
    "            if normalization.__class__.__name__ != 'Identity':\n",
    "                normalization.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalizations[0](x)\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = F.relu(x, inplace=True)\n",
    "            x = self.normalizations[i+1](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PMA(MessagePassing):\n",
    "    \"\"\"\n",
    "        PMA part:\n",
    "        Note that in original PMA, we need to compute the inner product of the seed and neighbor nodes.\n",
    "        i.e. e_ij = a(Wh_i,Wh_j), where a should be the inner product, h_i is the seed and h_j are neightbor nodes.\n",
    "        In GAT, a(x,y) = a^T[x||y]. We use the same logic.\n",
    "    \"\"\"\n",
    "    _alpha: OptTensor\n",
    "\n",
    "    def __init__(self, in_channels, hid_dim,\n",
    "                 out_channels, num_layers, heads=1, concat=True,\n",
    "                 negative_slope=0.2, dropout=0.0, bias=False, **kwargs):\n",
    "        #         kwargs.setdefault('aggr', 'add')\n",
    "        super(PMA, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden = hid_dim // heads\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = 0.\n",
    "        self.aggr = 'add'\n",
    "#         self.input_seed = input_seed\n",
    "\n",
    "#         This is the encoder part. Where we use 1 layer NN (Theta*x_i in the GATConv description)\n",
    "#         Now, no seed as input. Directly learn the importance weights alpha_ij.\n",
    "#         self.lin_O = Linear(heads*self.hidden, self.hidden) # For heads combining\n",
    "        # For neighbor nodes (source side, key)\n",
    "        self.lin_K = Linear(in_channels, self.heads*self.hidden)\n",
    "        # For neighbor nodes (source side, value)\n",
    "        self.lin_V = Linear(in_channels, self.heads*self.hidden)\n",
    "        self.att_r = Parameter(torch.Tensor(\n",
    "            1, heads, self.hidden))  # Seed vector\n",
    "        self.rFF = MLP(in_channels=self.heads*self.hidden,\n",
    "                       hidden_channels=self.heads*self.hidden,\n",
    "                       out_channels=out_channels,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=.0, Normalization='None',)\n",
    "        self.ln0 = nn.LayerNorm(self.heads*self.hidden)\n",
    "        self.ln1 = nn.LayerNorm(self.heads*self.hidden)\n",
    "#         if bias and concat:\n",
    "#             self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "#         elif bias and not concat:\n",
    "#             self.bias = Parameter(torch.Tensor(out_channels))\n",
    "#         else:\n",
    "\n",
    "#         Always no bias! (For now)\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #         glorot(self.lin_l.weight)\n",
    "        glorot(self.lin_K.weight)\n",
    "        glorot(self.lin_V.weight)\n",
    "        self.rFF.reset_parameters()\n",
    "        self.ln0.reset_parameters()\n",
    "        self.ln1.reset_parameters()\n",
    "#         glorot(self.att_l)\n",
    "        nn.init.xavier_uniform_(self.att_r)\n",
    "#         zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index: Adj,\n",
    "                size: Size = None, return_attention_weights=None):\n",
    "        # type: (Union[Tensor, OptPairTensor], Tensor, Size, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, OptPairTensor], Tensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
    "                will additionally return the tuple\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
    "                attention weights for each edge. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.hidden\n",
    "\n",
    "        x_l: OptTensor = None\n",
    "        x_r: OptTensor = None\n",
    "        alpha_l: OptTensor = None\n",
    "        alpha_r: OptTensor = None\n",
    "        if isinstance(x, Tensor):\n",
    "            assert x.dim() == 2, 'Static graphs not supported in `GATConv`.'\n",
    "            x_K = self.lin_K(x).view(-1, H, C)\n",
    "            x_V = self.lin_V(x).view(-1, H, C)\n",
    "            alpha_r = (x_K * self.att_r).sum(dim=-1)\n",
    "\n",
    "        out = self.propagate(edge_index, x=x_V,\n",
    "                             alpha=alpha_r, aggr=self.aggr)\n",
    "\n",
    "        alpha = self._alpha\n",
    "        self._alpha = None\n",
    "\n",
    "#         Note that in the original code of GMT paper, they do not use additional W^O to combine heads.\n",
    "#         This is because O = softmax(QK^T)V and V = V_in*W^V. So W^O can be effectively taken care by W^V!!!\n",
    "        out += self.att_r  # This is Seed + Multihead\n",
    "        # concat heads then LayerNorm. Z (rhs of Eq(7)) in GMT paper.\n",
    "        out = self.ln0(out.view(-1, self.heads * self.hidden))\n",
    "        # rFF and skip connection. Lhs of eq(7) in GMT paper.\n",
    "        out = self.ln1(out+F.relu(self.rFF(out)))\n",
    "\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_j, alpha_j,\n",
    "                index, ptr,\n",
    "                size_j):\n",
    "        #         ipdb.set_trace()\n",
    "        alpha = alpha_j\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, index, ptr, index.max()+1)\n",
    "        self._alpha = alpha\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return x_j * alpha.unsqueeze(-1)\n",
    "\n",
    "    def aggregate(self, inputs, index,\n",
    "                  dim_size=None):\n",
    "        r\"\"\"Aggregates messages from neighbors as\n",
    "        :math:`\\square_{j \\in \\mathcal{N}(i)}`.\n",
    "\n",
    "        Takes in the output of message computation as first argument and any\n",
    "        argument which was initially passed to :meth:`propagate`.\n",
    "\n",
    "        By default, this function will delegate its call to scatter functions\n",
    "        that support \"add\", \"mean\" and \"max\" operations as specified in\n",
    "        :meth:`__init__` by the :obj:`aggr` argument.\n",
    "        \"\"\"\n",
    "#         ipdb.set_trace()\n",
    "        if self.aggr is None:\n",
    "            raise ValueError(\"aggr was not passed!\")\n",
    "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)\n",
    "\n",
    "\n",
    "class HalfNLHconv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 hid_dim,\n",
    "                 out_dim,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 Normalization='bn',\n",
    "                 InputNorm=False,\n",
    "                 heads=1,\n",
    "                 attention=True\n",
    "                 ):\n",
    "        super(HalfNLHconv, self).__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if self.attention:\n",
    "            self.prop = PMA(in_dim, hid_dim, out_dim, num_layers, heads=heads)\n",
    "        else:\n",
    "            if num_layers > 0:\n",
    "                self.f_enc = MLP(in_dim, hid_dim, hid_dim, num_layers, dropout, Normalization, InputNorm)\n",
    "                self.f_dec = MLP(hid_dim, hid_dim, out_dim, num_layers, dropout, Normalization, InputNorm)\n",
    "            else:\n",
    "                self.f_enc = nn.Identity()\n",
    "                self.f_dec = nn.Identity()\n",
    "#         self.bn = nn.BatchNorm1d(dec_hid_dim)\n",
    "#         self.dropout = dropout\n",
    "#         self.Prop = S2SProp()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        if self.attention:\n",
    "            self.prop.reset_parameters()\n",
    "        else:\n",
    "            if self.f_enc.__class__.__name__ != 'Identity':\n",
    "                self.f_enc.reset_parameters()\n",
    "            if self.f_dec.__class__.__name__ != 'Identity':\n",
    "                self.f_dec.reset_parameters()\n",
    "#         self.bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, norm, aggr='add'):\n",
    "        \"\"\"\n",
    "        input -> MLP -> Prop\n",
    "        \"\"\"\n",
    "\n",
    "        if self.attention:\n",
    "            x = self.prop(x, edge_index)\n",
    "        else:\n",
    "            x = F.relu(self.f_enc(x))\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.propagate(edge_index, x=x, norm=norm, aggr=aggr)\n",
    "            x = F.relu(self.f_dec(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def aggregate(self, inputs, index,\n",
    "                  dim_size=None):\n",
    "        r\"\"\"Aggregates messages from neighbors as\n",
    "        :math:`\\square_{j \\in \\mathcal{N}(i)}`.\n",
    "\n",
    "        Takes in the output of message computation as first argument and any\n",
    "        argument which was initially passed to :meth:`propagate`.\n",
    "\n",
    "        By default, this function will delegate its call to scatter functions\n",
    "        that support \"add\", \"mean\" and \"max\" operations as specified in\n",
    "        :meth:`__init__` by the :obj:`aggr` argument.\n",
    "        \"\"\"\n",
    "#         ipdb.set_trace()\n",
    "        if self.aggr is None:\n",
    "            raise ValueError(\"aggr was not passed!\")\n",
    "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98929f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SetGNN(nn.Module):\n",
    "    def __init__(self, num_features,Classifier_hidden, num_classes,\n",
    "                 Classifier_num_layers, MLP_hidden, MLP_num_layers,\n",
    "                 All_num_layers, dropout, aggregate, normalization,\n",
    "                 deepset_input_norm, GPR, use_PMA, heads, LearnMask, norm=None):\n",
    "        super(SetGNN, self).__init__()\n",
    "        \"\"\"\n",
    "        args should contain the following:\n",
    "        V_in_dim, V_enc_hid_dim, V_dec_hid_dim, V_out_dim, V_enc_num_layers, V_dec_num_layers\n",
    "        E_in_dim, E_enc_hid_dim, E_dec_hid_dim, E_out_dim, E_enc_num_layers, E_dec_num_layers\n",
    "        All_num_layers,dropout\n",
    "        !!! V_in_dim should be the dimension of node features\n",
    "        !!! E_out_dim should be the number of classes (for classification)\n",
    "        \"\"\"\n",
    "\n",
    "#         Now set all dropout the same, but can be different\n",
    "        self.All_num_layers = All_num_layers\n",
    "        self.dropout = dropout\n",
    "        self.aggr = aggregate\n",
    "        self.NormLayer = normalization\n",
    "        self.InputNorm = deepset_input_norm\n",
    "        self.GPR = GPR\n",
    "        self.LearnMask =LearnMask\n",
    "#         Now define V2EConvs[i], V2EConvs[i] for ith layers\n",
    "#         Currently we assume there's no hyperedge features, which means V_out_dim = E_in_dim\n",
    "#         If there's hyperedge features, concat with Vpart decoder output features [V_feat||E_feat]\n",
    "        self.V2EConvs = nn.ModuleList()\n",
    "        self.E2VConvs = nn.ModuleList()\n",
    "        self.bnV2Es = nn.ModuleList()\n",
    "        self.bnE2Vs = nn.ModuleList()\n",
    "\n",
    "        if self.LearnMask:\n",
    "            self.Importance = Parameter(torch.ones(norm.size()))\n",
    "\n",
    "        if self.All_num_layers == 0:\n",
    "            self.classifier = MLP(in_channels=num_features,\n",
    "                                  hidden_channels=Classifier_hidden,\n",
    "                                  out_channels=num_classes,\n",
    "                                  num_layers=Classifier_num_layers,\n",
    "                                  dropout=self.dropout,\n",
    "                                  Normalization=self.NormLayer,\n",
    "                                  InputNorm=False)\n",
    "        else:\n",
    "            self.V2EConvs.append(HalfNLHconv(in_dim=num_features,\n",
    "                                             hid_dim=MLP_hidden,\n",
    "                                             out_dim=MLP_hidden,\n",
    "                                             num_layers=MLP_num_layers,\n",
    "                                             dropout=self.dropout,\n",
    "                                             Normalization=self.NormLayer,\n",
    "                                             InputNorm=self.InputNorm,\n",
    "                                             heads=heads,\n",
    "                                             attention=use_PMA))\n",
    "            self.bnV2Es.append(nn.BatchNorm1d(MLP_hidden))\n",
    "            self.E2VConvs.append(HalfNLHconv(in_dim=MLP_hidden,\n",
    "                                             hid_dim=MLP_hidden,\n",
    "                                             out_dim=MLP_hidden,\n",
    "                                             num_layers=MLP_num_layers,\n",
    "                                             dropout=self.dropout,\n",
    "                                             Normalization=self.NormLayer,\n",
    "                                             InputNorm=self.InputNorm,\n",
    "                                             heads=heads,\n",
    "                                             attention=use_PMA))\n",
    "            self.bnE2Vs.append(nn.BatchNorm1d(MLP_hidden))\n",
    "            for _ in range(self.All_num_layers-1):\n",
    "                self.V2EConvs.append(HalfNLHconv(in_dim=MLP_hidden,\n",
    "                                                 hid_dim=MLP_hidden,\n",
    "                                                 out_dim=MLP_hidden,\n",
    "                                                 num_layers=MLP_num_layers,\n",
    "                                                 dropout=self.dropout,\n",
    "                                                 Normalization=self.NormLayer,\n",
    "                                                 InputNorm=self.InputNorm,\n",
    "                                                 heads=heads,\n",
    "                                                 attention=use_PMA))\n",
    "                self.bnV2Es.append(nn.BatchNorm1d(MLP_hidden))\n",
    "                self.E2VConvs.append(HalfNLHconv(in_dim=MLP_hidden,\n",
    "                                                 hid_dim=MLP_hidden,\n",
    "                                                 out_dim=MLP_hidden,\n",
    "                                                 num_layers=MLP_num_layers,\n",
    "                                                 dropout=self.dropout,\n",
    "                                                 Normalization=self.NormLayer,\n",
    "                                                 InputNorm=self.InputNorm,\n",
    "                                                 heads=heads,\n",
    "                                                 attention=use_PMA))\n",
    "                self.bnE2Vs.append(nn.BatchNorm1d(MLP_hidden))\n",
    "            if self.GPR:\n",
    "                self.MLP = MLP(in_channels=num_features,\n",
    "                               hidden_channels=MLP_hidden,\n",
    "                               out_channels=MLP_hidden,\n",
    "                               num_layers=MLP_num_layers,\n",
    "                               dropout=self.dropout,\n",
    "                               Normalization=self.NormLayer,\n",
    "                               InputNorm=False)\n",
    "                self.GPRweights = Linear(self.All_num_layers+1, 1, bias=False)\n",
    "                self.classifier = MLP(in_channels=MLP_hidden,\n",
    "                                      hidden_channels=Classifier_hidden,\n",
    "                                      out_channels=num_classes,\n",
    "                                      num_layers=Classifier_num_layers,\n",
    "                                      dropout=self.dropout,\n",
    "                                      Normalization=self.NormLayer,\n",
    "                                      InputNorm=False)\n",
    "            else:\n",
    "                self.classifier = MLP(in_channels=MLP_hidden,\n",
    "                                      hidden_channels=Classifier_hidden,\n",
    "                                      out_channels=num_classes,\n",
    "                                      num_layers=Classifier_num_layers,\n",
    "                                      dropout=self.dropout,\n",
    "                                      Normalization=self.NormLayer,\n",
    "                                      InputNorm=False)\n",
    "\n",
    "\n",
    "#         Now we simply use V_enc_hid=V_dec_hid=E_enc_hid=E_dec_hid\n",
    "#         However, in general this can be arbitrary.\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.V2EConvs:\n",
    "            layer.reset_parameters()\n",
    "        for layer in self.E2VConvs:\n",
    "            layer.reset_parameters()\n",
    "        for layer in self.bnV2Es:\n",
    "            layer.reset_parameters()\n",
    "        for layer in self.bnE2Vs:\n",
    "            layer.reset_parameters()\n",
    "        self.classifier.reset_parameters()\n",
    "        if self.GPR:\n",
    "            self.MLP.reset_parameters()\n",
    "            self.GPRweights.reset_parameters()\n",
    "        if self.LearnMask:\n",
    "            nn.init.ones_(self.Importance)\n",
    "\n",
    "    def forward(self, x, edge_index, norm):\n",
    "        \"\"\"\n",
    "        The data should contain the follows\n",
    "        data.x: node features\n",
    "        data.edge_index: edge list (of size (2,|E|)) where data.edge_index[0] contains nodes and data.edge_index[1] contains hyperedges\n",
    "        !!! Note that self loop should be assigned to a new (hyper)edge id!!!\n",
    "        !!! Also note that the (hyper)edge id should start at 0 (akin to node id)\n",
    "        data.norm: The weight for edges in bipartite graphs, correspond to data.edge_index\n",
    "        !!! Note that we output final node representation. Loss should be defined outside.\n",
    "        \"\"\"\n",
    "#             The data should contain the follows\n",
    "#             data.x: node features\n",
    "#             data.V2Eedge_index:  edge list (of size (2,|E|)) where\n",
    "#             data.V2Eedge_index[0] contains nodes and data.V2Eedge_index[1] contains hyperedges\n",
    "\n",
    "        # x, edge_index, norm = data.x, data.edge_index, data.norm\n",
    "        if self.LearnMask:\n",
    "            norm = self.Importance*norm\n",
    "        cidx = edge_index[1].min()\n",
    "        edge_index[1] -= cidx  # make sure we do not waste memory\n",
    "        reversed_edge_index = torch.stack(\n",
    "            [edge_index[1], edge_index[0]], dim=0)\n",
    "        if self.GPR:\n",
    "            xs = []\n",
    "            xs.append(F.relu(self.MLP(x)))\n",
    "            for i, _ in enumerate(self.V2EConvs):\n",
    "                x = F.relu(self.V2EConvs[i](x, edge_index, norm, self.aggr))\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "                x = self.E2VConvs[i](x, reversed_edge_index, norm, self.aggr)\n",
    "                x = F.relu(x)\n",
    "                xs.append(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = torch.stack(xs, dim=-1)\n",
    "            x = self.GPRweights(x).squeeze()\n",
    "            x = self.classifier(x)\n",
    "        else:\n",
    "            x = F.dropout(x, p=0.2, training=self.training) # Input dropout\n",
    "            for i, _ in enumerate(self.V2EConvs):\n",
    "                x = F.relu(self.V2EConvs[i](x, edge_index, norm, self.aggr))\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "                x = F.relu(self.E2VConvs[i](\n",
    "                    x, reversed_edge_index, norm, self.aggr))\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be331a59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_features = 461 #number of features in the input\n",
    "num_classes = 4 #number of classes in the output\n",
    "Classifier_hidden = 64 #hidden dim in the last classification layer\n",
    "Classifier_num_layers = 2 #number of layers in the last classification layer\n",
    "MLP_hidden = 64 #hidden states in all the used MLPs\n",
    "MLP_num_layers = 2 #number of layers in all the used MLPs\n",
    "All_num_layers = 2 #number of All*Sets layers in the model\n",
    "dropout = 0.0 #dropout\n",
    "aggregate = 'mean' #type of aggregation (might be ignored, not sure)\n",
    "normalization = 'ln' #type og normalisation it can be ln, bn or none\n",
    "deepset_input_norm = False #if you norm the input or not\n",
    "GPR = True #when set to True, the classification is made from all the intermendiate representations concatenated\n",
    "use_PMA = False #when True, the model is AllSetTransformer, when False the model is AllDeepSets\n",
    "heads = 1 #number of heads in Transformer\n",
    "LearnMask = False\n",
    "\n",
    "num_nodes = 5\n",
    "x = torch.rand(num_nodes, num_features)\n",
    "edge_index = torch.tensor([[0,1,2,0,1,3,4,1,2,4],[0,0,0,1,1,1,1,2,2,2]])\n",
    "norm = torch.ones_like(edge_index[0]) #would keep this like that all the time. it means we don't normalise the input.\n",
    "\n",
    "model = SetGNN(num_features,Classifier_hidden, num_classes,\n",
    "                 Classifier_num_layers, MLP_hidden, MLP_num_layers,\n",
    "                 All_num_layers, dropout, aggregate, normalization,\n",
    "                 deepset_input_norm, GPR, use_PMA, heads, LearnMask, norm)\n",
    "out = model(x, edge_index, norm)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "alzheimers-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
