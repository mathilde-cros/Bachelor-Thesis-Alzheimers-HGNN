{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.convert_matrix import from_numpy_array\n",
    "\n",
    "# Additional imports to deal with Matrix Profiling\n",
    "import matrixprofile as mp\n",
    "import stumpy\n",
    "from stumpy import stumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To retrieve variables from Graph_Building.ipynb\n",
    "%store -r full_corr_path_lists\n",
    "%store -r diagnostic_label\n",
    "%store -r ages\n",
    "%store -r min_age\n",
    "%store -r max_age\n",
    "%store -r sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to simplify the code in the class Raw_to_Graph_Time_Series_Features, like in Graph_Building.ipynb\n",
    "\n",
    "# To convert a dictionnary into a numpy array\n",
    "def dict_to_array(dict):\n",
    "    array = np.array(list(dict.values()))\n",
    "    return array\n",
    "\n",
    "# To normalize an array\n",
    "def normalize_array(array):\n",
    "    norm_array = (array - np.mean(array)) / np.std(array)\n",
    "    return norm_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_features_and_stats(dataset):\n",
    "    print()\n",
    "    print(f'Dataset: {dataset}:')\n",
    "    print('====================')\n",
    "    print(f'Number of graphs: {len(dataset)}')\n",
    "    print(f'Weighted: {dataset.weight}')\n",
    "    print(f'Threshold: {dataset.threshold}')\n",
    "    print(f'Correlation Method: {dataset.method}')\n",
    "    print(f'Number of features: {dataset.num_features}')\n",
    "    print(f'Number of classes: {len(np.unique(diagnostic_label))}')\n",
    "\n",
    "    # Getting the first graph object in the dataset.\n",
    "    data = dataset[0]\n",
    "\n",
    "    print()\n",
    "    print(data)\n",
    "    print('=============================================================')\n",
    "\n",
    "    # Some statistics about the first graph.\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "    print(f'Has self-loops: {data.has_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class to preprocess raw data into a format suitable for training Graph Neural Networks (GNNs).\n",
    "## With time series features and with the possibility of assigning weight to edges.\n",
    "\n",
    "class Raw_to_Graph_MatrixProfile(InMemoryDataset):\n",
    "    def __init__(self, root, threshold, method, weight, transform=None, pre_transform=None):\n",
    "        self.threshold = threshold\n",
    "        self.method = method\n",
    "        self.weight = weight\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "    \n",
    "    # Computing the matrix profile using STUMPY++\n",
    "    def compute_matrix_profile(self, time_series):\n",
    "        m, idx = stumped(time_series, m=3)\n",
    "        return m, idx\n",
    "\n",
    "    # Finding the motif and discord in the matrix profile\n",
    "    def find_motif_discord(self, matrix_profile, idx):\n",
    "        motif_idx = np.argmin(matrix_profile)\n",
    "        discord_idx = np.argmax(matrix_profile)\n",
    "        motif = idx[motif_idx]\n",
    "        discord = idx[discord_idx]\n",
    "        return motif, discord\n",
    "\n",
    "    # This function is used to process the raw data into a format suitable for GNNs, by constructing graphs out of the connectivity matrices.\n",
    "    def process(self):\n",
    "        graphs=[]\n",
    "        corr_matrices = full_corr_path_lists[self.method]\n",
    "        for patient_idx, patient_matrix in enumerate(corr_matrices):\n",
    "            path = f'ADNI_full/corr_matrices/corr_matrix_{self.method}/{patient_matrix}'\n",
    "            corr_matrix = pd.read_csv(path, header=None).values\n",
    "\n",
    "            # Here ROIs stands for Regions of Interest and we are building the edge_matrix from the correlation matrix\n",
    "            nbr_ROIs = corr_matrix.shape[0]\n",
    "            edge_matrix = np.zeros((nbr_ROIs,nbr_ROIs))\n",
    "            for j in range(nbr_ROIs):\n",
    "                for k in range(nbr_ROIs):\n",
    "                    # Here we are using the absolute value of each element of the correlation matrix, as the corr coeff is in the range [-1,1].\n",
    "                    if np.abs(corr_matrix[ j , k ]) < self.threshold:\n",
    "                        edge_matrix[ j , k ] = 0\n",
    "                    else:\n",
    "                        if self.weight:\n",
    "                            # Here we assign the absolute value of the correlation coefficient as the edge weight.\n",
    "                            edge_matrix[ j , k ] = corr_matrix[ j , k]\n",
    "                        else:\n",
    "                            # Here we assign 1 as the edge weight, i.e. regardless of the the absolute value of the correlation coefficient.\n",
    "                            edge_matrix[ j , k ] = 1\n",
    "\n",
    "            # Computing the matrix profile for each row (time series) of the correlation matrix\n",
    "            matrix_profiles = []\n",
    "            motifs = []\n",
    "            discords = []\n",
    "            for row in corr_matrix:\n",
    "                m, idx = self.compute_matrix_profile(row)\n",
    "                matrix_profiles.append(m)\n",
    "                motif, discord = self.find_motif_discord(m, idx)\n",
    "                motifs.append(motif)\n",
    "                discords.append(discord)\n",
    "            motifs_array = np.array(motifs)\n",
    "            discords_array = np.array(discords)\n",
    "\n",
    "            # Create a NetworkX graph from the edge matrix\n",
    "            NetworkX_graph = from_numpy_array(edge_matrix)\n",
    "\n",
    "            # Compute the degree, betweenness centrality, clustering coefficient, local efficiency for each node of the graph and the global efficiency of the graph\n",
    "            degree_dict = dict(NetworkX_graph.degree())\n",
    "            between_central_dict = nx.betweenness_centrality(NetworkX_graph)\n",
    "            cluster_coeff_dict = nx.clustering(NetworkX_graph)\n",
    "            global_eff = nx.global_efficiency(NetworkX_graph)\n",
    "            local_eff_dict = {}\n",
    "            for node in NetworkX_graph.nodes():\n",
    "                subgraph_neighb = NetworkX_graph.subgraph(NetworkX_graph.neighbors(node))\n",
    "                if subgraph_neighb.number_of_nodes() > 1:\n",
    "                    efficiency = nx.global_efficiency(subgraph_neighb)\n",
    "                else:\n",
    "                    efficiency = 0.0\n",
    "                local_eff_dict[node] = efficiency\n",
    "\n",
    "            # Convert the degree, betweenness centrality, local efficiency, clustering coefficient and ratio of local to global efficiency dictionaries to NumPy arrays then normalize them\n",
    "            degree_array = dict_to_array(degree_dict)\n",
    "            degree_array_norm = normalize_array(degree_array)\n",
    "\n",
    "            between_central_array = dict_to_array(between_central_dict)\n",
    "            between_central_array_norm = normalize_array(between_central_array)\n",
    "\n",
    "            local_efficiency_array = dict_to_array(local_eff_dict)\n",
    "            local_eff_array_norm = normalize_array(local_efficiency_array)\n",
    "\n",
    "            ratio_local_global_array = dict_to_array(local_eff_dict) / global_eff\n",
    "            ratio_local_global_array_norm = normalize_array(ratio_local_global_array)\n",
    "\n",
    "            cluster_coeff_array = dict_to_array(cluster_coeff_dict)\n",
    "            cluster_coeff_array_norm = normalize_array(cluster_coeff_array)\n",
    "            \n",
    "            # Extracting the age and sex features of the patient\n",
    "            patient_age = ages[patient_idx]\n",
    "            age_norm = (patient_age - min_age) / (max_age - min_age)\n",
    "            patient_sex = int(sex[patient_idx])\n",
    "            # Making the age and sex arrays the same size as the other arrays\n",
    "            age_array = np.full((nbr_ROIs,), age_norm)\n",
    "            sex_array = np.full((nbr_ROIs,), patient_sex)\n",
    "\n",
    "            # Concatenate the degree, participation coefficient, betweenness centrality, local efficiency, and ratio of local to global efficiency arrays to form a single feature vector\n",
    "            x_conc = torch.tensor(np.concatenate((degree_array_norm, between_central_array_norm, local_eff_array_norm, cluster_coeff_array_norm, ratio_local_global_array_norm, motifs_array, discords_array, age_array, sex_array)), dtype=torch.float)\n",
    "            # Determining the number of features concatenated to reshape with the correct dimensions\n",
    "            x = torch.reshape(x_conc, (9, nbr_ROIs)).T\n",
    "\n",
    "            # Create a Pytorch Geometric Data object from the NetworkX \n",
    "            graph_data = from_networkx(NetworkX_graph)\n",
    "            ## The feature matrix of the graph is the degree, betweenness centrality, local efficiency, clustering coefficient and ratio of local to global efficiency of each node\n",
    "            graph_data.x = x\n",
    "            ## The target/output variable that we want to predict is the diagnostic label of the patient\n",
    "            graph_data.y = diagnostic_label[patient_idx]\n",
    "            graphs.append(graph_data)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "weight = False\n",
    "method = 'pearson'\n",
    "\n",
    "root = f'Raw_to_graph_MatrixProfile/ADNI_T_{threshold}_W_{weight}_M_{method}'\n",
    "dataset = Raw_to_Graph_MatrixProfile(root=root, threshold=threshold, method=method, weight=weight)\n",
    "dataset_features_and_stats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "weights = [False, True]\n",
    "methods = ['pearson', 'spearman', 'kendall', 'partial']\n",
    "\n",
    "for weight in weights:\n",
    "    for method in methods:\n",
    "        for threshold in thresholds:\n",
    "            root = f'Raw_to_graph_MatrixProfile/ADNI_T_{threshold}_W_{weight}_M_{method}'\n",
    "            dataset = Raw_to_Graph_MatrixProfile(root=root, threshold=threshold, method=method, weight=weight)\n",
    "            dataset_features_and_stats(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alzheimers-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
