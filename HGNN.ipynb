{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From HGNN_conv.ipynb\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, scatter\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "class HypergraphConv(MessagePassing):\n",
    "    r\"\"\"The hypergraph convolutional operator from the `\"Hypergraph Convolution\n",
    "    and Hypergraph Attention\" <https://arxiv.org/abs/1901.08150>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{D}^{-1} \\mathbf{H} \\mathbf{W}\n",
    "        \\mathbf{B}^{-1} \\mathbf{H}^{\\top} \\mathbf{X} \\mathbf{\\Theta}\n",
    "\n",
    "    where :math:`\\mathbf{H} \\in {\\{ 0, 1 \\}}^{N \\times M}` is the incidence\n",
    "    matrix, :math:`\\mathbf{W} \\in \\mathbb{R}^M` is the diagonal hyperedge\n",
    "    weight matrix, and\n",
    "    :math:`\\mathbf{D}` and :math:`\\mathbf{B}` are the corresponding degree\n",
    "    matrices.\n",
    "\n",
    "    For example, in the hypergraph scenario\n",
    "    :math:`\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})` with\n",
    "    :math:`\\mathcal{V} = \\{ 0, 1, 2, 3 \\}` and\n",
    "    :math:`\\mathcal{E} = \\{ \\{ 0, 1, 2 \\}, \\{ 1, 2, 3 \\} \\}`, the\n",
    "    :obj:`hyperedge_index` is represented as:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        hyperedge_index = torch.tensor([\n",
    "            [0, 1, 2, 1, 2, 3],\n",
    "            [0, 0, 0, 1, 1, 1],\n",
    "        ])\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
    "            the size from the first input(s) to the forward method.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        use_attention (bool, optional): If set to :obj:`True`, attention\n",
    "            will be added to this layer. (default: :obj:`False`)\n",
    "        heads (int, optional): Number of multi-head-attentions.\n",
    "            (default: :obj:`1`)\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
    "            attentions are averaged instead of concatenated.\n",
    "            (default: :obj:`True`)\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
    "            slope. (default: :obj:`0.2`)\n",
    "        dropout (float, optional): Dropout probability of the normalized\n",
    "            attention coefficients which exposes each node to a stochastically\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:\n",
    "        - **input:**\n",
    "          node features :math:`(|\\mathcal{V}|, F_{in})`,\n",
    "          hyperedge indices :math:`(|\\mathcal{V}|, |\\mathcal{E}|)`,\n",
    "          hyperedge weights :math:`(|\\mathcal{E}|)` *(optional)*\n",
    "          hyperedge features :math:`(|\\mathcal{E}|, D)` *(optional)*\n",
    "        - **output:** node features :math:`(|\\mathcal{V}|, F_{out})`\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_attention=False, heads=1,\n",
    "                 concat=True, negative_slope=0.2, dropout=0, bias=True,\n",
    "                 **kwargs):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(flow='source_to_target', node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        if self.use_attention:\n",
    "            self.heads = heads\n",
    "            self.concat = concat\n",
    "            self.negative_slope = negative_slope\n",
    "            self.dropout = dropout\n",
    "            self.lin = Linear(in_channels, heads * out_channels, bias=False,\n",
    "                              weight_initializer='glorot')\n",
    "            self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels))\n",
    "        else:\n",
    "            self.heads = 1\n",
    "            self.concat = True\n",
    "            self.lin = Linear(in_channels, out_channels, bias=False,\n",
    "                              weight_initializer='glorot')\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "        if self.use_attention:\n",
    "            glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, hyperedge_index: Tensor,\n",
    "                hyperedge_weight: Optional[Tensor] = None,\n",
    "                hyperedge_attr: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Runs the forward pass of the module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Node feature matrix\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{N \\times F}`.\n",
    "            hyperedge_index (torch.Tensor): The hyperedge indices, *i.e.*\n",
    "                the sparse incidence matrix\n",
    "                :math:`\\mathbf{H} \\in {\\{ 0, 1 \\}}^{N \\times M}` mapping from\n",
    "                nodes to edges.\n",
    "            hyperedge_weight (torch.Tensor, optional): Hyperedge weights\n",
    "                :math:`\\mathbf{W} \\in \\mathbb{R}^M`. (default: :obj:`None`)\n",
    "            hyperedge_attr (torch.Tensor, optional): Hyperedge feature matrix\n",
    "                in :math:`\\mathbb{R}^{M \\times F}`.\n",
    "                These features only need to get passed in case\n",
    "                :obj:`use_attention=True`. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        num_nodes, num_edges = x.size(0), 0\n",
    "        if hyperedge_index.numel() > 0:\n",
    "            num_edges = int(hyperedge_index[1].max()) + 1\n",
    "\n",
    "        if hyperedge_weight is None:\n",
    "            hyperedge_weight = x.new_ones(num_edges)\n",
    "\n",
    "        x = self.lin(x)\n",
    "\n",
    "        alpha = None\n",
    "        if self.use_attention:\n",
    "            assert hyperedge_attr is not None\n",
    "            x = x.view(-1, self.heads, self.out_channels)\n",
    "            hyperedge_attr = self.lin(hyperedge_attr)\n",
    "            hyperedge_attr = hyperedge_attr.view(-1, self.heads,\n",
    "                                                 self.out_channels)\n",
    "            x_i = x[hyperedge_index[0]]\n",
    "            x_j = hyperedge_attr[hyperedge_index[1]]\n",
    "            alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n",
    "            alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "            alpha = softmax(alpha, hyperedge_index[0], num_nodes=x.size(0))\n",
    "            alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        D = scatter(hyperedge_weight[hyperedge_index[1]], hyperedge_index[0],\n",
    "                    dim=0, dim_size=num_nodes, reduce='sum')\n",
    "        D = 1.0 / D\n",
    "        D[D == float(\"inf\")] = 0\n",
    "\n",
    "        B = scatter(x.new_ones(hyperedge_index.size(1)), hyperedge_index[1],\n",
    "                    dim=0, dim_size=num_edges, reduce='sum')\n",
    "        B = 1.0 / B\n",
    "        B[B == float(\"inf\")] = 0\n",
    "\n",
    "        out = self.propagate(hyperedge_index, x=x, norm=B, alpha=alpha,\n",
    "                             size=(num_nodes, num_edges))\n",
    "        out = self.propagate(hyperedge_index.flip([0]), x=out, norm=D,\n",
    "                             alpha=alpha, size=(num_edges, num_nodes))\n",
    "\n",
    "        if self.concat is True:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, norm_i: Tensor, alpha: Tensor) -> Tensor:\n",
    "        H, F = self.heads, self.out_channels\n",
    "\n",
    "        out = norm_i.view(-1, 1, 1) * x_j.view(-1, H, F)\n",
    "\n",
    "        if alpha is not None:\n",
    "            out = alpha.view(-1, self.heads, 1) * out\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From HGNN_dataset.ipynb\n",
    "\n",
    "from typing import Optional\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, scatter\n",
    "from torch.nn import Parameter\n",
    "from csv import writer\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "from networkx.convert_matrix import from_numpy_array\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load matrix and correlation\n",
    "def matrix_loader(root):\n",
    "    ts_list = sorted(os.listdir(root))\n",
    "    ts_path_list = []\n",
    "    for i in range(0, len(ts_list)):\n",
    "            ts_path_list.append(os.path.join(root, ts_list[i]))\n",
    "    return ts_path_list\n",
    "\n",
    "def filter_SMC_patient_info():\n",
    "    df          = pd.read_csv('/Users/georgepulickal/Documents/ADNI_FULL/patient_info.csv')\n",
    "    labels      = df['Research Group']\n",
    "    label_idx_list = [i for i in range(len(labels)) if labels[i] != 'SMC']\n",
    "    return label_idx_list\n",
    "\n",
    "def store_results(List):\n",
    "    with open('results.csv', 'a') as f_object:\n",
    "        writer_object = writer(f_object)\n",
    "        writer_object.writerow(List)\n",
    "        f_object.close()\n",
    "\n",
    "corr_list = matrix_loader('ADNI_gsr_172/corr_matrices')\n",
    "hg_list = matrix_loader('ADNI_gsr_full/hypergraphs/cluster/thresh_0.6')\n",
    "corr_test = np.loadtxt(corr_list[0], delimiter=',')\n",
    "hg_test = np.loadtxt(hg_list[0], delimiter=',')\n",
    "hg_nx = from_numpy_array(hg_test)\n",
    "hg_matrix_data = from_networkx(hg_nx)\n",
    "hg_matrix_data.x = torch.tensor(corr_test).float()\n",
    "\n",
    "class HGNN_ADNI_dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, hg_data_path = 'hypergraphs/cluster/thresh_0.6'):\n",
    "        self.hg_data_path = hg_data_path\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Converts raw data into GNN-readable format by constructing\n",
    "        graphs out of connectivity matrices.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Paths of connectivity matrices\n",
    "        full_corr_list = matrix_loader('ADNI_gsr_full/corr_matrices')\n",
    "        idx = filter_SMC_patient_info()\n",
    "        corr_list = [full_corr_list[i] for i in idx]\n",
    "\n",
    "        hg_list   = matrix_loader(self.hg_data_path)\n",
    "        idx = filter_SMC_patient_info()\n",
    "        new_hg_list = [hg_list[i] for i in idx]\n",
    "        labels = torch.from_numpy(np.loadtxt('ADNI_gsr_172/labels.csv', delimiter=','))\n",
    "        assert len(corr_list) == len(new_hg_list)\n",
    "        assert len(labels) == len(corr_list)\n",
    "\n",
    "        graphs = []\n",
    "        for i in range(0, len(corr_list)):\n",
    "            corr_array = np.loadtxt(corr_list[i], delimiter=',')\n",
    "            hg_array = np.loadtxt(new_hg_list[i], delimiter=',')\n",
    "\n",
    "            #Pushing partial correlation matrices through pipeline to get final Data object\n",
    "            hg_nx = from_numpy_array(hg_array)\n",
    "            hg_matrix_data = from_networkx(hg_nx)\n",
    "            hg_matrix_data.x = torch.tensor(corr_array).float()\n",
    "            hg_matrix_data.y = labels[i].type(torch.LongTensor)\n",
    "            #hg_matrix_data.pos = coordinates\n",
    "\n",
    "            # Add to running list of all dataset items\n",
    "            graphs.append(hg_matrix_data)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "dataset = HGNN_ADNI_dataset('ADNI_gsr_pyg_test_hypergraph_cluster')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of hypergraphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "\n",
    "#hypergraph convolution\n",
    "class HyperGraph1(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(HyperGraph1, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.hconv1 = HypergraphConv(dataset.num_node_features, hidden_channels, use_attention=False, heads=1)\n",
    "        self.hconv2 = HypergraphConv(hidden_channels, hidden_channels)\n",
    "        #self.conv3 = HypergraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.hconv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.hconv2(x, edge_index)\n",
    "        #x = x.relu()\n",
    "        #x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class HyperGraph2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(HyperGraph2, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.hconv1 = HypergraphConv(dataset.num_node_features, hidden_channels, use_attention=True, heads=8, concat=True,bias=True,dropout=0.6)\n",
    "        self.hconv2 = HypergraphConv(hidden_channels * 8, hidden_channels,  use_attention=False, heads=1, concat=True,bias=True,dropout=0.6)\n",
    "        #self.conv3 = HypergraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.hconv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.hconv2(x, edge_index)\n",
    "        #x = x.relu()\n",
    "        #x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def train():\n",
    "    model.train()\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.#\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "tot_test_acc = []\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "for train_val_idx, test_idx in kf.split(dataset, dataset.data.y):\n",
    "    X_train_val = [dataset[i] for i in train_val_idx]\n",
    "    X_test      = [dataset[i] for i in test_idx]\n",
    "    Y_train_val = [dataset.data.y[i] for i in train_val_idx]\n",
    "    Y_test      = [dataset.data.y[i] for i in test_idx]\n",
    "\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_val, Y_train_val , test_size=0.125,\n",
    "                                                    random_state=42, stratify=Y_train_val)\n",
    "\n",
    "    print(f'Number of training graphs: {len(X_train)}')\n",
    "    print(f'Number of validation graphs: {len(X_valid)}')\n",
    "    print(f'Number of test graphs: {len(X_test)}')\n",
    "\n",
    "    train_loader = DataLoader(X_train, batch_size=64, shuffle=True)\n",
    "    valid_loader = DataLoader(X_valid, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(X_test, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = HyperGraph1(hidden_channels=8)\n",
    "    #model = HyperGraph2(hidden_channels=8)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, 30):\n",
    "        train()\n",
    "        train_acc = test(train_loader)\n",
    "        valid_acc = test(valid_loader)\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Valid Acc: {valid_acc:.4f}')\n",
    "        #wandb.log({\"val_acc\": valid_acc , \"train_acc\": train_acc})\n",
    "\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Test Acc: {test_acc: .4f}')\n",
    "    tot_test_acc.append(test_acc)\n",
    "\n",
    "print(f'Average Test Accuracy: {sum(tot_test_acc) / len(tot_test_acc)}')\n",
    "print(f'Max test accuracy: {max(tot_test_acc)}')\n",
    "print(f'Standard Deviation: {np.std(tot_test_acc)}')\n",
    "results = [dataset.hg_data_path, (sum(tot_test_acc) / len(tot_test_acc)) , np.std(tot_test_acc)]\n",
    "store_results(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
